Name:         linux-1-6lwms
Namespace:    default
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:10 -0400
Labels:       deployment=linux-1
              deploymentconfig=linux
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.98"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift.io/deployment-config.latest-version: 1
              openshift.io/deployment-config.name: linux
              openshift.io/deployment.name: linux-1
              openshift.io/generated-by: OpenShiftNewApp
Status:       Running
IP:           10.241.0.98
IPs:
  IP:           10.241.0.98
Controlled By:  ReplicationController/linux-1
Containers:
  linux:
    Container ID:   cri-o://3088f712ff5018fbcb07e15b8b068ef989de0e8909104e881b449731f09ac6ee
    Image:          cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Image ID:       docker.io/library/cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 14 May 2020 00:23:43 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      linux-1-skqs5
Namespace:                 default
Priority:                  0
Node:                      ibmworker02.os4.ringen.us/192.168.93.53
Start Time:                Wed, 13 May 2020 17:13:28 -0400
Labels:                    deployment=linux-1
                           deploymentconfig=linux
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.2.6"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.2.1"
                                 ]
                             }]
                           openshift.io/deployment-config.latest-version: 1
                           openshift.io/deployment-config.name: linux
                           openshift.io/deployment.name: linux-1
                           openshift.io/generated-by: OpenShiftNewApp
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.2.6
IPs:
  IP:           10.241.2.6
Controlled By:  ReplicationController/linux-1
Containers:
  linux:
    Container ID:   cri-o://ac7160db98c3aafe4664738c4fb76147e7dba56707a1ae79545dffa34feb9f32
    Image:          cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Image ID:       docker.io/library/cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 13 May 2020 17:13:33 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      linux2-1-ggjcj
Namespace:                 default
Priority:                  0
Node:                      ibmworker02.os4.ringen.us/192.168.93.53
Start Time:                Wed, 13 May 2020 17:31:41 -0400
Labels:                    deployment=linux2-1
                           deploymentconfig=linux2
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.2.7"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.2.1"
                                 ]
                             }]
                           openshift.io/deployment-config.latest-version: 1
                           openshift.io/deployment-config.name: linux2
                           openshift.io/deployment.name: linux2-1
                           openshift.io/generated-by: OpenShiftNewApp
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.2.7
IPs:
  IP:           10.241.2.7
Controlled By:  ReplicationController/linux2-1
Containers:
  linux2:
    Container ID:   cri-o://4dd0869b1e07065a4f5bd9fb617e5a50167bceb9a3b14d426a6c2e81156a3ed8
    Image:          cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Image ID:       docker.io/library/cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 13 May 2020 17:31:45 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         linux2-1-j7c6p
Namespace:    default
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:10 -0400
Labels:       deployment=linux2-1
              deploymentconfig=linux2
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.96"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift.io/deployment-config.latest-version: 1
              openshift.io/deployment-config.name: linux2
              openshift.io/deployment.name: linux2-1
              openshift.io/generated-by: OpenShiftNewApp
Status:       Running
IP:           10.241.0.96
IPs:
  IP:           10.241.0.96
Controlled By:  ReplicationController/linux2-1
Containers:
  linux2:
    Container ID:   cri-o://e26beba2789d2cad63890adbb7f6e4a0e71c1fccdcee7f594d05dd863ed2081e
    Image:          cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Image ID:       docker.io/library/cirros@sha256:c7d58d6d463247a2540b8c10ff012c34fd443426462e891b13119a9c66dfd28a
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 14 May 2020 00:23:49 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      web-1-1-6gxmm
Namespace:                 default
Priority:                  0
Node:                      ibmworker02.os4.ringen.us/192.168.93.53
Start Time:                Wed, 13 May 2020 15:33:14 -0400
Labels:                    deployment=web-1-1
                           deploymentconfig=web-1
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.2.5"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.2.1"
                                 ]
                             }]
                           openshift.io/deployment-config.latest-version: 1
                           openshift.io/deployment-config.name: web-1
                           openshift.io/deployment.name: web-1-1
                           openshift.io/generated-by: OpenShiftNewApp
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.2.5
IPs:
  IP:           10.241.2.5
Controlled By:  ReplicationController/web-1-1
Containers:
  web-1:
    Container ID:   cri-o://531bb13fc0b7db839640eb0f5c469130983cdb748f6ef5bf151955f4f678a91d
    Image:          ringend/web-server2@sha256:54419d3ded7c6c4232461d3546a075f7f67b89e48d1b46e09857bb9c6d534679
    Image ID:       docker.io/ringend/web-server2@sha256:54419d3ded7c6c4232461d3546a075f7f67b89e48d1b46e09857bb9c6d534679
    Ports:          8080/TCP, 8443/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Wed, 13 May 2020 15:34:10 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         web-1-1-rt9j4
Namespace:    default
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:10 -0400
Labels:       deployment=web-1-1
              deploymentconfig=web-1
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.97"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift.io/deployment-config.latest-version: 1
              openshift.io/deployment-config.name: web-1
              openshift.io/deployment.name: web-1-1
              openshift.io/generated-by: OpenShiftNewApp
Status:       Running
IP:           10.241.0.97
IPs:
  IP:           10.241.0.97
Controlled By:  ReplicationController/web-1-1
Containers:
  web-1:
    Container ID:   cri-o://696f2eb37fb8cf08388b93ec1f26e8049e64c38627734e35bbb27af06d255348
    Image:          ringend/web-server2@sha256:54419d3ded7c6c4232461d3546a075f7f67b89e48d1b46e09857bb9c6d534679
    Image ID:       docker.io/ringend/web-server2@sha256:54419d3ded7c6c4232461d3546a075f7f67b89e48d1b46e09857bb9c6d534679
    Ports:          8080/TCP, 8443/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Thu, 14 May 2020 00:23:52 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62cr8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-62cr8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62cr8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 openshift-apiserver-operator-5dbcb99bff-dmclb
Namespace:            openshift-apiserver-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=openshift-apiserver-operator
                      pod-template-hash=5dbcb99bff
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.5"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.5
IPs:
  IP:           10.241.0.5
Controlled By:  ReplicaSet/openshift-apiserver-operator-5dbcb99bff
Containers:
  openshift-apiserver-operator:
    Container ID:  cri-o://02399ded86768cd2f7561dae0b221a918ae828f32af26edbee6db17bc532936d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fdf4f72a63cc98ee5f4e440ca2effa89ce096025779466e2a465233455a3755a
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fdf4f72a63cc98ee5f4e440ca2effa89ce096025779466e2a465233455a3755a
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-openshift-apiserver-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
    State:       Running
      Started:   Wed, 13 May 2020 12:38:34 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   t https://172.30.0.1:443/apis/operator.openshift.io/v1/openshiftapiservers?allowWatchBookmarks=true&resourceVersion=8164&timeoutSeconds=415&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.395051       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: Get https://172.30.0.1:443/api/v1/namespaces/openshift-apiserver/secrets?allowWatchBookmarks=true&resourceVersion=7994&timeout=6m5s&timeoutSeconds=365&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.396261       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ServiceAccount: Get https://172.30.0.1:443/api/v1/namespaces/openshift-apiserver/serviceaccounts?allowWatchBookmarks=true&resourceVersion=7997&timeout=8m4s&timeoutSeconds=484&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.420761       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: Get https://172.30.0.1:443/api/v1/namespaces/kube-system/secrets?allowWatchBookmarks=true&resourceVersion=8148&timeout=8m28s&timeoutSeconds=508&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.421816       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-apiserver/configmaps?allowWatchBookmarks=true&resourceVersion=5580&timeout=7m58s&timeoutSeconds=478&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.793513       1 migration_controller.go:347] key failed with: Put https://172.30.0.1:443/apis/operator.openshift.io/v1/openshiftapiservers/cluster/status: dial tcp 172.30.0.1:443: connect: connection refused
I0513 16:38:17.802582       1 leaderelection.go:287] failed to renew lease openshift-apiserver-operator/openshift-apiserver-operator-lock: failed to tryAcquireOrRenew context deadline exceeded
F0513 16:38:17.803207       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:33:19 -0400
      Finished:     Wed, 13 May 2020 12:38:18 -0400
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:645065486a2acdc70332ea166f1802ae577cf1ca6ad6815bd9ab8b4efe0e64ed
      OPERATOR_IMAGE:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fdf4f72a63cc98ee5f4e440ca2effa89ce096025779466e2a465233455a3755a
      OPERATOR_IMAGE_VERSION:  4.3.13
      OPERAND_IMAGE_VERSION:   4.3.13
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-apiserver-operator-token-d2v2p (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-apiserver-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      openshift-apiserver-operator-config
    Optional:  false
  openshift-apiserver-operator-token-d2v2p:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-apiserver-operator-token-d2v2p
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 apiserver-x4gnl
Namespace:            openshift-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:36:24 -0400
Labels:               apiserver=true
                      app=openshift-apiserver
                      controller-revision-hash=844c99859c
                      pod-template-generation=3
                      revision=0
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.32"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      operator.openshift.io/force: 29e8401d-151b-41c6-b435-c2f379d7aa20
Status:               Running
IP:                   10.241.0.32
IPs:
  IP:           10.241.0.32
Controlled By:  DaemonSet/apiserver
Init Containers:
  fix-audit-permissions:
    Container ID:  cri-o://edce4cd72a3aa0a9599dab2e14b7811f9a977421ef7e988f97568ae30744f641
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:645065486a2acdc70332ea166f1802ae577cf1ca6ad6815bd9ab8b4efe0e64ed
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:645065486a2acdc70332ea166f1802ae577cf1ca6ad6815bd9ab8b4efe0e64ed
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      chmod 0700 /var/log/openshift-apiserver
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:36:26 -0400
      Finished:     Wed, 13 May 2020 12:36:26 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/openshift-apiserver from audit-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-apiserver-sa-token-kmlsh (ro)
Containers:
  openshift-apiserver:
    Container ID:  cri-o://1078ac86e9ec9e47ee05e3bcb9d18af6d1954ffcf41e2767f954a7dd3aa63d35
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:645065486a2acdc70332ea166f1802ae577cf1ca6ad6815bd9ab8b4efe0e64ed
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:645065486a2acdc70332ea166f1802ae577cf1ca6ad6815bd9ab8b4efe0e64ed
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      /bin/bash
      -ec
    Args:
      if [ -s /var/run/configmaps/trusted-ca-bundle/tls-ca-bundle.pem ]; then
        echo "Copying system trust bundle"
        cp -f /var/run/configmaps/trusted-ca-bundle/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec openshift-apiserver start --config=/var/run/configmaps/config/config.yaml -v=2
    State:          Running
      Started:      Wed, 13 May 2020 12:36:27 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        150m
      memory:     200Mi
    Liveness:     http-get https://:8443/healthz delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:8443/healthz delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:  <none>
    Mounts:
      /var/log/openshift-apiserver from audit-dir (rw)
      /var/run/configmaps/config from config (rw)
      /var/run/configmaps/etcd-serving-ca from etcd-serving-ca (rw)
      /var/run/configmaps/image-import-ca from image-import-ca (rw)
      /var/run/configmaps/trusted-ca-bundle from trusted-ca-bundle (rw)
      /var/run/secrets/encryption-config from encryption-config (rw)
      /var/run/secrets/etcd-client from etcd-client (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-apiserver-sa-token-kmlsh (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      config
    Optional:  false
  etcd-client:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  etcd-client
    Optional:    false
  etcd-serving-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      etcd-serving-ca
    Optional:  false
  image-import-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      image-import-ca
    Optional:  true
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  serving-cert
    Optional:    false
  trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca-bundle
    Optional:  true
  encryption-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  encryption-config-0
    Optional:    true
  audit-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/openshift-apiserver
    HostPathType:  
  openshift-apiserver-sa-token-kmlsh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-apiserver-sa-token-kmlsh
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.32:8443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.32:8443/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.32:8443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                 authentication-operator-6d865c4957-4qmr7
Namespace:            openshift-authentication-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:12 -0400
Labels:               app=authentication-operator
                      pod-template-hash=6d865c4957
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.45"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.45
IPs:
  IP:           10.241.0.45
Controlled By:  ReplicaSet/authentication-operator-6d865c4957
Containers:
  operator:
    Container ID:  cri-o://82ffbb1e82988771f1bff404caf414dee4af7a94768878b4947933a6a7f66d76
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b08f006c5b1e969fd8ec27fb40cd1edf3648fc5959b1b0dbeb92f68a4613d45e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b08f006c5b1e969fd8ec27fb40cd1edf3648fc5959b1b0dbeb92f68a4613d45e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -ec
    Args:
      if [ -s /var/run/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then
          echo "Copying system trust bundle"
          cp -f /var/run/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec authentication-operator operator --config=/var/run/configmaps/config/operator-config.yaml --v=2 --terminate-on-files=/var/run/configmaps/trusted-ca-bundle/ca-bundle.crt
      
    State:          Running
      Started:      Wed, 13 May 2020 12:38:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
      OPERATOR_IMAGE_VERSION:  4.3.13
      OPERAND_IMAGE_VERSION:   4.3.13_openshift
      POD_NAME:                authentication-operator-6d865c4957-4qmr7 (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/configmaps/trusted-ca-bundle from trusted-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from authentication-operator-token-4qp9x (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      authentication-operator-config
    Optional:  false
  trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca-bundle
    Optional:  true
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  serving-cert
    Optional:    true
  authentication-operator-token-4qp9x:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  authentication-operator-token-4qp9x
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 oauth-openshift-75bbc7bc6b-b488v
Namespace:            openshift-authentication
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:47 -0400
Labels:               app=oauth-openshift
                      pod-template-hash=75bbc7bc6b
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.72"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
                      operator.openshift.io/pull-spec:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
                      operator.openshift.io/rvs-hash: qiNQ2OxWeo3ms16ZJFTVM5V4SAQtXDNR0t_n9Xt-WTnTf0csOGo-_v93pz1ogk4wUgMeoU235JHqKWS0BoAWGg
Status:               Running
IP:                   10.241.0.72
IPs:
  IP:           10.241.0.72
Controlled By:  ReplicaSet/oauth-openshift-75bbc7bc6b
Containers:
  oauth-openshift:
    Container ID:  cri-o://954f5074f78d91f5d9d718cbce16e4467b645b0f76457e37c559e164fb9b243b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
    Port:          6443/TCP
    Host Port:     0/TCP
    Command:
      /bin/bash
      -ec
    Args:
      
      if [ -s /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle/ca-bundle.crt ]; then
          echo "Copying system trust bundle"
          cp -f /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec oauth-server osinserver --config=/var/config/system/configmaps/v4-0-config-system-cliconfig/v4-0-config-system-cliconfig --v=2
      
    State:          Running
      Started:      Wed, 13 May 2020 12:43:23 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get https://:6443/healthz delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:6443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/config/system/configmaps/v4-0-config-system-cliconfig from v4-0-config-system-cliconfig (ro)
      /var/config/system/configmaps/v4-0-config-system-service-ca from v4-0-config-system-service-ca (ro)
      /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle from v4-0-config-system-trusted-ca-bundle (ro)
      /var/config/system/secrets/v4-0-config-system-ocp-branding-template from v4-0-config-system-ocp-branding-template (ro)
      /var/config/system/secrets/v4-0-config-system-router-certs from v4-0-config-system-router-certs (ro)
      /var/config/system/secrets/v4-0-config-system-serving-cert from v4-0-config-system-serving-cert (ro)
      /var/config/system/secrets/v4-0-config-system-session from v4-0-config-system-session (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from oauth-openshift-token-rphhl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  v4-0-config-system-session:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-session
    Optional:    true
  v4-0-config-system-cliconfig:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-cliconfig
    Optional:  true
  v4-0-config-system-serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-serving-cert
    Optional:    true
  v4-0-config-system-service-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-service-ca
    Optional:  true
  v4-0-config-system-router-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-router-certs
    Optional:    true
  v4-0-config-system-ocp-branding-template:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-ocp-branding-template
    Optional:    true
  v4-0-config-system-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-trusted-ca-bundle
    Optional:  true
  oauth-openshift-token-rphhl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  oauth-openshift-token-rphhl
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.72:6443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x4 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.72:6443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                 oauth-openshift-75bbc7bc6b-zlh5q
Namespace:            openshift-authentication
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:47 -0400
Labels:               app=oauth-openshift
                      pod-template-hash=75bbc7bc6b
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.69"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
                      operator.openshift.io/pull-spec:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
                      operator.openshift.io/rvs-hash: qiNQ2OxWeo3ms16ZJFTVM5V4SAQtXDNR0t_n9Xt-WTnTf0csOGo-_v93pz1ogk4wUgMeoU235JHqKWS0BoAWGg
Status:               Running
IP:                   10.241.0.69
IPs:
  IP:           10.241.0.69
Controlled By:  ReplicaSet/oauth-openshift-75bbc7bc6b
Containers:
  oauth-openshift:
    Container ID:  cri-o://21c761cd55a2db6b593092a6aa7a9d7541a9180ccd6fcf0e4f9e58cd48258a7a
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1a9046e61ba0374bac4e69fe09780ff53340444dd46308020ec500d2fee85c6
    Port:          6443/TCP
    Host Port:     0/TCP
    Command:
      /bin/bash
      -ec
    Args:
      
      if [ -s /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle/ca-bundle.crt ]; then
          echo "Copying system trust bundle"
          cp -f /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec oauth-server osinserver --config=/var/config/system/configmaps/v4-0-config-system-cliconfig/v4-0-config-system-cliconfig --v=2
      
    State:          Running
      Started:      Wed, 13 May 2020 17:52:27 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:43:26 -0400
      Finished:     Wed, 13 May 2020 17:52:25 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get https://:6443/healthz delay=30s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:6443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/config/system/configmaps/v4-0-config-system-cliconfig from v4-0-config-system-cliconfig (ro)
      /var/config/system/configmaps/v4-0-config-system-service-ca from v4-0-config-system-service-ca (ro)
      /var/config/system/configmaps/v4-0-config-system-trusted-ca-bundle from v4-0-config-system-trusted-ca-bundle (ro)
      /var/config/system/secrets/v4-0-config-system-ocp-branding-template from v4-0-config-system-ocp-branding-template (ro)
      /var/config/system/secrets/v4-0-config-system-router-certs from v4-0-config-system-router-certs (ro)
      /var/config/system/secrets/v4-0-config-system-serving-cert from v4-0-config-system-serving-cert (ro)
      /var/config/system/secrets/v4-0-config-system-session from v4-0-config-system-session (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from oauth-openshift-token-rphhl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  v4-0-config-system-session:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-session
    Optional:    true
  v4-0-config-system-cliconfig:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-cliconfig
    Optional:  true
  v4-0-config-system-serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-serving-cert
    Optional:    true
  v4-0-config-system-service-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-service-ca
    Optional:  true
  v4-0-config-system-router-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-router-certs
    Optional:    true
  v4-0-config-system-ocp-branding-template:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v4-0-config-system-ocp-branding-template
    Optional:    true
  v4-0-config-system-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      v4-0-config-system-trusted-ca-bundle
    Optional:  true
  oauth-openshift-token-rphhl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  oauth-openshift-token-rphhl
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x5 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.69:6443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x4 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.69:6443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                 cloud-credential-operator-868c5f9f7f-lqc54
Namespace:            openshift-cloud-credential-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=cloud-credential-operator
                      control-plane=controller-manager
                      controller-tools.k8s.io=1.0
                      pod-template-hash=868c5f9f7f
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.4"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.4
IPs:
  IP:           10.241.0.4
Controlled By:  ReplicaSet/cloud-credential-operator-868c5f9f7f
Containers:
  manager:
    Container ID:  cri-o://d3f0a957e6d220ec467cb03c797387d732449724d2e8452d4e239f7b15036440
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9eb44c375a4dbcdc8cf96ca144ed3d1277f551d0941869c2f066e4d10ee8ca1
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9eb44c375a4dbcdc8cf96ca144ed3d1277f551d0941869c2f066e4d10ee8ca1
    Port:          9876/TCP
    Host Port:     0/TCP
    Command:
      /bin/bash
      -ec
    Args:
      if [ -s /var/run/configmaps/trusted-ca-bundle/tls-ca-bundle.pem ]; then
          echo "Copying system trust bundle"
          cp -f /var/run/configmaps/trusted-ca-bundle/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec /root/manager --log-level=debug
      
    State:       Running
      Started:   Thu, 14 May 2020 07:11:24 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   974&timeoutSeconds=384&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0514 11:11:02.042115       1 reflector.go:270] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers_map.go:126: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/configmaps?resourceVersion=259106&timeoutSeconds=387&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0514 11:11:02.042838       1 reflector.go:126] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers_map.go:204: Failed to list *v1.ClusterOperator: Get https://172.30.0.1:443/apis/config.openshift.io/v1/clusteroperators?limit=500&resourceVersion=0: dial tcp 172.30.0.1:443: connect: connection refused
E0514 11:11:02.129105       1 leaderelection.go:306] error retrieving resource lock openshift-cloud-credential-operator/cloud-credential-operator-leader: Get https://172.30.0.1:443/api/v1/namespaces/openshift-cloud-credential-operator/configmaps/cloud-credential-operator-leader: dial tcp 172.30.0.1:443: connect: connection refused
E0514 11:11:12.129161       1 event.go:247] Could not construct reference to: '&v1.ConfigMap{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Data:map[string]string(nil), BinaryData:map[string][]uint8(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'cloud-credential-operator-868c5f9f7f-lqc54_65b703d3-95d2-11ea-ab42-0a580af10004 stopped leading'
time="2020-05-14T11:11:12Z" level=error msg="leader election lostunable to run the manager"

      Exit Code:    1
      Started:      Thu, 14 May 2020 07:02:23 -0400
      Finished:     Thu, 14 May 2020 07:11:12 -0400
    Ready:          True
    Restart Count:  4
    Requests:
      cpu:     10m
      memory:  150Mi
    Environment:
      RELEASE_VERSION:  4.3.13
    Mounts:
      /var/run/configmaps/trusted-ca-bundle from cco-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5rqqf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cco-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cco-trusted-ca
    Optional:  true
  default-token-5rqqf:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-5rqqf
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason   Age                From                                Message
  ----     ------   ----               ----                                -------
  Warning  BackOff  19m (x2 over 18h)  kubelet, dc1master01.os4.ringen.us  Back-off restarting failed container
  Normal   Created  18m (x5 over 18h)  kubelet, dc1master01.os4.ringen.us  Created container manager
  Normal   Started  18m (x5 over 18h)  kubelet, dc1master01.os4.ringen.us  Started container manager
  Normal   Pulled   18m (x4 over 18h)  kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b9eb44c375a4dbcdc8cf96ca144ed3d1277f551d0941869c2f066e4d10ee8ca1" already present on machine


Name:                 machine-approver-56d779879d-4g5mg
Namespace:            openshift-cluster-machine-approver
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=machine-approver
                      pod-template-hash=56d779879d
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  ReplicaSet/machine-approver-56d779879d
Containers:
  kube-rbac-proxy:
    Container ID:  cri-o://5bec5f9234eaf07b5186413a23285a9525384cfab0f715be9b02772b95f9a854
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9192/TCP
    Host Port:     9192/TCP
    Args:
      --secure-listen-address=0.0.0.0:9192
      --upstream=http://127.0.0.1:9191/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --config-file=/etc/kube-rbac-proxy/config-file.yaml
      --logtostderr=true
      --v=10
    State:          Running
      Started:      Wed, 13 May 2020 12:32:24 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kube-rbac-proxy from auth-proxy-config (rw)
      /etc/tls/private from machine-approver-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-approver-sa-token-bn9gk (ro)
  machine-approver-controller:
    Container ID:  cri-o://8c54ab15e55db378f66d14f4fda660d1040d0d0e981cd998897ca12ff60ca974
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:61c7877553402049c75b5539e12cef7d8c8fb3634ce1d5a8c3f1835c726a5c17
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:61c7877553402049c75b5539e12cef7d8c8fb3634ce1d5a8c3f1835c726a5c17
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-approver
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=2
      --logtostderr
    State:          Running
      Started:      Wed, 13 May 2020 12:32:29 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  127.0.0.1
      METRICS_PORT:             9191
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-approver-sa-token-bn9gk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  auth-proxy-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-rbac-proxy
    Optional:  false
  machine-approver-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-approver-tls
    Optional:    false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      machine-approver-config
    Optional:  true
  machine-approver-sa-token-bn9gk:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-approver-sa-token-bn9gk
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 cluster-node-tuning-operator-8478dbbbd5-v88qb
Namespace:            openshift-cluster-node-tuning-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:14 -0400
Labels:               name=cluster-node-tuning-operator
                      pod-template-hash=8478dbbbd5
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.43"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.43
IPs:
  IP:           10.241.0.43
Controlled By:  ReplicaSet/cluster-node-tuning-operator-8478dbbbd5
Containers:
  cluster-node-tuning-operator:
    Container ID:  cri-o://530986bd8b41c70124f45f509da2d862a7e89fb3a7e354f2a7d14f9ca7290b72
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d3f208fe2519bd17720310a587203fc44272c360b4d96f32b77305be975075c4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d3f208fe2519bd17720310a587203fc44272c360b4d96f32b77305be975075c4
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-node-tuning-operator
    Args:
      -v=1
    State:          Running
      Started:      Wed, 13 May 2020 12:38:11 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      RELEASE_VERSION:           4.3.13
      WATCH_NAMESPACE:           openshift-cluster-node-tuning-operator (v1:metadata.namespace)
      POD_NAME:                  cluster-node-tuning-operator-8478dbbbd5-v88qb (v1:metadata.name)
      OPERATOR_NAME:             node-tuning
      RESYNC_PERIOD:             600
      CLUSTER_NODE_TUNED_IMAGE:  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-node-tuning-operator-token-58jhm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cluster-node-tuning-operator-token-58jhm:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-node-tuning-operator-token-58jhm
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 tuned-9tb7k
Namespace:            openshift-cluster-node-tuning-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:00:39 -0400
Labels:               controller-revision-hash=7c6d57c44c
                      openshift-app=tuned
                      pod-template-generation=1
Annotations:          openshift.io/scc: privileged
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/tuned
Containers:
  tuned:
    Container ID:  cri-o://87e4afc60dca76cef6d9d728bf2965425d5941d48d5c43b00bdffed9148c69c7
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Port:          <none>
    Host Port:     <none>
    Command:
      /var/lib/tuned/bin/run
      start
    State:          Running
      Started:      Wed, 13 May 2020 13:01:01 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      OCP_NODE_NAME:     (v1:spec.nodeName)
      RESYNC_PERIOD:    60
      RELEASE_VERSION:  4.3.13
    Mounts:
      /etc/tuned/recommend.d from etc-tuned-recommend (rw)
      /lib/modules from lib-modules (ro)
      /run/systemd/system from run-systemd-system (ro)
      /sys from sys (rw)
      /var/lib/tuned/profiles-data from var-lib-tuned-profiles-data (rw)
      /var/run/dbus from var-run-dbus (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from tuned-token-222gp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  run-systemd-system:
    Type:          HostPath (bare host directory volume)
    Path:          /run/systemd/system
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  etc-tuned-recommend:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-recommend
    Optional:  true
  var-lib-tuned-profiles-data:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-profiles
    Optional:  true
  tuned-token-222gp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tuned-token-222gp
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 tuned-wcgcz
Namespace:            openshift-cluster-node-tuning-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:02 -0400
Labels:               controller-revision-hash=7c6d57c44c
                      openshift-app=tuned
                      pod-template-generation=1
Annotations:          openshift.io/scc: privileged
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/tuned
Containers:
  tuned:
    Container ID:  cri-o://7c11fa9c323c47fc39fcfc83675181c88662efd4f9c1e4067564a252618b3905
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Port:          <none>
    Host Port:     <none>
    Command:
      /var/lib/tuned/bin/run
      start
    State:          Running
      Started:      Wed, 13 May 2020 12:45:09 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      OCP_NODE_NAME:     (v1:spec.nodeName)
      RESYNC_PERIOD:    60
      RELEASE_VERSION:  4.3.13
    Mounts:
      /etc/tuned/recommend.d from etc-tuned-recommend (rw)
      /lib/modules from lib-modules (ro)
      /run/systemd/system from run-systemd-system (ro)
      /sys from sys (rw)
      /var/lib/tuned/profiles-data from var-lib-tuned-profiles-data (rw)
      /var/run/dbus from var-run-dbus (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from tuned-token-222gp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  run-systemd-system:
    Type:          HostPath (bare host directory volume)
    Path:          /run/systemd/system
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  etc-tuned-recommend:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-recommend
    Optional:  true
  var-lib-tuned-profiles-data:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-profiles
    Optional:  true
  tuned-token-222gp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tuned-token-222gp
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 tuned-xn2tn
Namespace:            openshift-cluster-node-tuning-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:00:49 -0400
Labels:               controller-revision-hash=7c6d57c44c
                      openshift-app=tuned
                      pod-template-generation=1
Annotations:          openshift.io/scc: privileged
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/tuned
Containers:
  tuned:
    Container ID:  cri-o://214dc886e295e891a5241ca98c6a97e399852a09e8795e9ce0a150c92266485b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0746289b47025239b157f306b7c791aed8badfd8d8faccc34e7c2c79376cf66
    Port:          <none>
    Host Port:     <none>
    Command:
      /var/lib/tuned/bin/run
      start
    State:          Running
      Started:      Wed, 13 May 2020 13:01:09 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      OCP_NODE_NAME:     (v1:spec.nodeName)
      RESYNC_PERIOD:    60
      RELEASE_VERSION:  4.3.13
    Mounts:
      /etc/tuned/recommend.d from etc-tuned-recommend (rw)
      /lib/modules from lib-modules (ro)
      /run/systemd/system from run-systemd-system (ro)
      /sys from sys (rw)
      /var/lib/tuned/profiles-data from var-lib-tuned-profiles-data (rw)
      /var/run/dbus from var-run-dbus (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from tuned-token-222gp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  run-systemd-system:
    Type:          HostPath (bare host directory volume)
    Path:          /run/systemd/system
    HostPathType:  
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  etc-tuned-recommend:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-recommend
    Optional:  true
  var-lib-tuned-profiles-data:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      tuned-profiles
    Optional:  true
  tuned-token-222gp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  tuned-token-222gp
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 cluster-samples-operator-7647685db9-xgfgn
Namespace:            openshift-cluster-samples-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:47 -0400
Labels:               name=cluster-samples-operator
                      pod-template-hash=7647685db9
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.73"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.73
IPs:
  IP:           10.241.0.73
Controlled By:  ReplicaSet/cluster-samples-operator-7647685db9
Containers:
  cluster-samples-operator:
    Container ID:  cri-o://c5db8fefc09c7d37e8534dd22979becb81926cb3907895124565567942db8aa8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fa10d15fe82452fef76ae50e1676f64b1207ccc028eeb689d2cb14d70b24c33b
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fa10d15fe82452fef76ae50e1676f64b1207ccc028eeb689d2cb14d70b24c33b
    Port:          60000/TCP
    Host Port:     0/TCP
    Command:
      cluster-samples-operator
    State:          Running
      Started:      Wed, 13 May 2020 12:43:25 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      WATCH_NAMESPACE:     openshift-cluster-samples-operator (v1:metadata.namespace)
      OPERATOR_NAME:       cluster-samples-operator
      RELEASE_VERSION:     4.3.13
      IMAGE_JENKINS:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c454944825be5359c85728e1ba2d4bc30c73976698ef33f7f72cfb4d4ae2d9fc
      IMAGE_AGENT_NODEJS:  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dd104213164b7b5f99339c8aadd38c3ec6cb1e6d5927c1624de9592329d67c6
      IMAGE_AGENT_MAVEN:   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6dfa24227dbc945a2385db81f6b50860d9f6a48d266c1443373197d145ec81ff
    Mounts:
      /etc/secrets from samples-operator-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-samples-operator-token-j9d5d (ro)
  cluster-samples-operator-watch:
    Container ID:  cri-o://2a5dfa7d72a400b62ccfd17ea89a249992a65c18e14291bf650d2565f5e44f5b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fa10d15fe82452fef76ae50e1676f64b1207ccc028eeb689d2cb14d70b24c33b
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fa10d15fe82452fef76ae50e1676f64b1207ccc028eeb689d2cb14d70b24c33b
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-samples-operator-watch
      file-watcher-watchdog
    Args:
      --namespace=openshift-cluster-samples-operator
      --process-name=cluster-samples-operator
      --termination-grace-period=30s
      --files=/etc/secrets/tls.crt,/etc/secrets/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:43:27 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-samples-operator-token-j9d5d (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  samples-operator-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  samples-operator-tls
    Optional:    false
  cluster-samples-operator-token-j9d5d:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-samples-operator-token-j9d5d
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 cluster-storage-operator-7648b88945-7hh28
Namespace:            openshift-cluster-storage-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:13 -0400
Labels:               name=cluster-storage-operator
                      pod-template-hash=7648b88945
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.38"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.38
IPs:
  IP:           10.241.0.38
Controlled By:  ReplicaSet/cluster-storage-operator-7648b88945
Containers:
  cluster-storage-operator:
    Container ID:  cri-o://b1e4894647392e1e3e12a1056ba48200ecbe7a7a65d54782ef0772b74f756a26
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4a857a6abe958ce50a04594b38d39e41ec660261204148ba4ab298a367a8bc27
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4a857a6abe958ce50a04594b38d39e41ec660261204148ba4ab298a367a8bc27
    Port:          60000/TCP
    Host Port:     0/TCP
    Command:
      cluster-storage-operator
    State:          Running
      Started:      Wed, 13 May 2020 12:38:12 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      RELEASE_VERSION:  4.3.13
      WATCH_NAMESPACE:  openshift-cluster-storage-operator (v1:metadata.namespace)
      POD_NAME:         cluster-storage-operator-7648b88945-7hh28 (v1:metadata.name)
      OPERATOR_NAME:    storage
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-storage-operator-token-pqznj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cluster-storage-operator-token-pqznj:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-storage-operator-token-pqznj
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 cluster-version-operator-76bbd48b9-nt9dt
Namespace:            openshift-cluster-version
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               k8s-app=cluster-version-operator
                      pod-template-hash=76bbd48b9
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  ReplicaSet/cluster-version-operator-76bbd48b9
Containers:
  cluster-version-operator:
    Container ID:  cri-o://5383af4a31837abe0ad96e2483ced29c933de4153196c448499ef9b5586fb10a
    Image:         quay.io/openshift-release-dev/ocp-release@sha256:e1ebc7295248a8394afb8d8d918060a7cc3de12c491283b317b80b26deedfe61
    Image ID:      quay.io/openshift-release-dev/ocp-release@sha256:e1ebc7295248a8394afb8d8d918060a7cc3de12c491283b317b80b26deedfe61
    Port:          <none>
    Host Port:     <none>
    Args:
      start
      --release-image=quay.io/openshift-release-dev/ocp-release@sha256:e1ebc7295248a8394afb8d8d918060a7cc3de12c491283b317b80b26deedfe61
      --enable-auto-update=false
      --enable-default-cluster-version=true
      --v=4
    State:          Running
      Started:      Wed, 13 May 2020 12:38:32 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:31:26 -0400
      Finished:     Wed, 13 May 2020 12:38:30 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  127.0.0.1
      NODE_NAME:                 (v1:spec.nodeName)
    Mounts:
      /etc/cvo/updatepayloads from etc-cvo-updatepayloads (ro)
      /etc/ssl/certs from etc-ssl-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8457g (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etc-ssl-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  
  etc-cvo-updatepayloads:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cvo/updatepayloads
    HostPathType:  
  default-token-8457g:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-8457g
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 console-operator-84c7ddd788-qsz5c
Namespace:            openshift-console-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:05 -0400
Labels:               name=console-operator
                      pod-template-hash=84c7ddd788
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.35"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.35
IPs:
  IP:           10.241.0.35
Controlled By:  ReplicaSet/console-operator-84c7ddd788
Containers:
  console-operator:
    Container ID:  cri-o://f62320db17891f22517594ab6257a2de6b7e7d702b9bf9e1328cb812711b586d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d2b04d4f01ecf4842f1fad42de0d098306e81ad919a73c4b2466bfe1452948f6
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d2b04d4f01ecf4842f1fad42de0d098306e81ad919a73c4b2466bfe1452948f6
    Port:          60000/TCP
    Host Port:     0/TCP
    Command:
      console
      operator
    Args:
      -v=2
      --config=/var/run/configmaps/config/controller-config.yaml
    State:       Running
      Started:   Thu, 14 May 2020 05:13:22 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   Operator ended with: too old resource version: 151379 (151452)
W0514 02:38:33.997229       1 reflector.go:299] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterOperator ended with: too old resource version: 151452 (151475)
W0514 02:45:43.172062       1 reflector.go:299] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterOperator ended with: too old resource version: 153118 (153289)
W0514 02:45:49.020458       1 reflector.go:299] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: watch of *v1.ClusterOperator ended with: too old resource version: 153289 (153320)
E0514 09:13:21.290304       1 status.go:73] DeploymentAvailable FailedUpdate 1 replicas ready at version 4.3.13
I0514 09:13:21.313686       1 status_controller.go:165] clusteroperator/console diff {"status":{"conditions":[{"lastTransitionTime":"2020-05-13T16:37:25Z","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2020-05-13T16:45:34Z","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2020-05-14T09:13:21Z","message":"DeploymentAvailable: 1 replicas ready at version 4.3.13","reason":"DeploymentAvailableFailedUpdate","status":"False","type":"Available"},{"lastTransitionTime":"2020-05-13T16:37:25Z","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
I0514 09:13:21.347382       1 event.go:255] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-console-operator", Name:"console-operator", UID:"554651d1-c8b9-4e95-a973-3a923b8ba67d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/console changed: Available changed from True to False ("DeploymentAvailable: 1 replicas ready at version 4.3.13")
I0514 09:13:21.550954       1 cmd.go:79] Received SIGTERM or SIGINT signal, shutting down controller.
F0514 09:13:21.551135       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:38:42 -0400
      Finished:     Thu, 14 May 2020 05:13:21 -0400
    Ready:          True
    Restart Count:  3
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   http-get https://:8443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get https://:8443/readyz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      IMAGE:            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
      RELEASE_VERSION:  4.3.13
      OPERATOR_NAME:    console-operator
      POD_NAME:         console-operator-84c7ddd788-qsz5c (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from console-operator-token-jldc6 (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      console-operator-config
    Optional:  false
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  serving-cert
    Optional:    true
  console-operator-token-jldc6:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  console-operator-token-jldc6
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x5 over 18h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.35:8443/readyz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Normal   Killing    137m (x3 over 18h)  kubelet, dc1master01.os4.ringen.us  Container console-operator failed liveness probe, will be restarted
  Warning  Unhealthy  137m (x5 over 18h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.35:8443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Normal   Created    137m (x4 over 18h)  kubelet, dc1master01.os4.ringen.us  Created container console-operator
  Normal   Started    137m (x4 over 18h)  kubelet, dc1master01.os4.ringen.us  Started container console-operator
  Normal   Pulled     137m (x3 over 18h)  kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d2b04d4f01ecf4842f1fad42de0d098306e81ad919a73c4b2466bfe1452948f6" already present on machine
  Warning  Unhealthy  28m (x3 over 18h)   kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.35:8443/readyz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  19m (x4 over 18h)   kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.35:8443/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)


Name:                 console-8476b9b455-7fq44
Namespace:            openshift-console
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:16 -0400
Labels:               app=console
                      component=ui
                      pod-template-hash=8476b9b455
Annotations:          console.openshift.io/console-config-version: 9706
                      console.openshift.io/image:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
                      console.openshift.io/oauth-secret-version: 9723
                      console.openshift.io/proxy-config-version: 431
                      console.openshift.io/service-ca-config-version: 9711
                      console.openshift.io/trusted-ca-config-version: 9749
                      k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.87"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
                      operator.openshift.io/pull-spec:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
Status:               Running
IP:                   10.241.0.87
IPs:
  IP:           10.241.0.87
Controlled By:  ReplicaSet/console-8476b9b455
Containers:
  console:
    Container ID:  cri-o://89a0119bc4f6cf32713b1c57eca1fba724c65095a357c73f8d9c9e364c1e4cd3
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
    Port:          443/TCP
    Host Port:     0/TCP
    Command:
      /opt/bridge/bin/bridge
      --public-dir=/opt/bridge/static
      --config=/var/console-config/console-config.yaml
      --service-ca-file=/var/service-ca/service-ca.crt
    State:          Running
      Started:      Wed, 13 May 2020 12:45:28 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     100Mi
    Liveness:     http-get https://:8443/health delay=150s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:8443/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/pki/ca-trust/extracted/pem from trusted-ca-bundle (ro)
      /var/console-config from console-config (ro)
      /var/oauth-config from console-oauth-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8g84 (ro)
      /var/service-ca from service-ca (ro)
      /var/serving-cert from console-serving-cert (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  console-serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  console-serving-cert
    Optional:    false
  console-oauth-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  console-oauth-config
    Optional:    false
  console-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      console-config
    Optional:  false
  service-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      service-ca
    Optional:  false
  trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca-bundle
    Optional:  false
  default-token-f8g84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8g84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-reachable:NoExecute for 120s
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.87:8443/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.87:8443/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.87:8443/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                 console-8476b9b455-tcfmp
Namespace:            openshift-console
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:48 -0400
Labels:               app=console
                      component=ui
                      pod-template-hash=8476b9b455
Annotations:          console.openshift.io/console-config-version: 9706
                      console.openshift.io/image:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
                      console.openshift.io/oauth-secret-version: 9723
                      console.openshift.io/proxy-config-version: 431
                      console.openshift.io/service-ca-config-version: 9711
                      console.openshift.io/trusted-ca-config-version: 9749
                      k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.74"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
                      operator.openshift.io/pull-spec:
                        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
Status:               Running
IP:                   10.241.0.74
IPs:
  IP:           10.241.0.74
Controlled By:  ReplicaSet/console-8476b9b455
Containers:
  console:
    Container ID:  cri-o://0607dcb0f276bd6191025e2fe8b7751ead0fc2c4342afbd25d74720f4ccea802
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:70ea1979fc44eb46f1016c14fb67441144b740bec3e53ef5235a6a0302a4b1f4
    Port:          443/TCP
    Host Port:     0/TCP
    Command:
      /opt/bridge/bin/bridge
      --public-dir=/opt/bridge/static
      --config=/var/console-config/console-config.yaml
      --service-ca-file=/var/service-ca/service-ca.crt
    State:          Running
      Started:      Wed, 13 May 2020 12:43:01 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     100Mi
    Liveness:     http-get https://:8443/health delay=150s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:8443/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/pki/ca-trust/extracted/pem from trusted-ca-bundle (ro)
      /var/console-config from console-config (ro)
      /var/oauth-config from console-oauth-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8g84 (ro)
      /var/service-ca from service-ca (ro)
      /var/serving-cert from console-serving-cert (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  console-serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  console-serving-cert
    Optional:    false
  console-oauth-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  console-oauth-config
    Optional:    false
  console-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      console-config
    Optional:  false
  service-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      service-ca
    Optional:  false
  trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca-bundle
    Optional:  false
  default-token-f8g84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8g84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-reachable:NoExecute for 120s
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.74:8443/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.74:8443/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get https://10.241.0.74:8443/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:         downloads-78d964646c-2jbp7
Namespace:    openshift-console
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 12:36:59 -0400
Labels:       app=console
              component=downloads
              pod-template-hash=78d964646c
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.33"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift.io/scc: restricted
Status:       Running
IP:           10.241.0.33
IPs:
  IP:           10.241.0.33
Controlled By:  ReplicaSet/downloads-78d964646c
Containers:
  download-server:
    Container ID:  cri-o://a5c5d1e39765fe501f64a83c7c4844c59dff9723283190c0aafae9f4d71f7d64
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:908f80952617b0f7324ac5f0fc053a83f1cda049b4229101adcf2e321ffd6c38
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:908f80952617b0f7324ac5f0fc053a83f1cda049b4229101adcf2e321ffd6c38
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /bin/sh
    Args:
      -c
      cat <<EOF >>/tmp/serve.py
      import BaseHTTPServer, os, re, signal, SimpleHTTPServer, socket, sys, tarfile, tempfile, threading, time, zipfile
      
      signal.signal(signal.SIGTERM, lambda signum, frame: sys.exit(0))
      
      # Launch multiple listeners as threads
      class Thread(threading.Thread):
          def __init__(self, i, socket):
              threading.Thread.__init__(self)
              self.i = i
              self.socket = socket
              self.daemon = True
              self.start()
      
          def run(self):
              httpd = BaseHTTPServer.HTTPServer(addr, SimpleHTTPServer.SimpleHTTPRequestHandler, False)
      
              # Prevent the HTTP server from re-binding every handler.
              # https://stackoverflow.com/questions/46210672/
              httpd.socket = self.socket
              httpd.server_bind = self.server_close = lambda self: None
      
              httpd.serve_forever()
      
      temp_dir = tempfile.mkdtemp()
      print('serving from {}'.format(temp_dir))
      os.chdir(temp_dir)
      for arch in ['amd64']:
          os.mkdir(arch)
          for operating_system in ['linux', 'mac', 'windows']:
              os.mkdir(os.path.join(arch, operating_system))
      for arch in ['arm64', 'ppc64le', 's390x']:
          os.mkdir(arch)
          for operating_system in ['linux']:
              os.mkdir(os.path.join(arch, operating_system))
      
      for arch, operating_system, path in [
              ('amd64', 'linux', '/usr/share/openshift/linux_amd64/oc'),
              ('amd64', 'mac', '/usr/share/openshift/mac/oc'),
              ('amd64', 'windows', '/usr/share/openshift/windows/oc.exe'),
              ('arm64', 'linux', '/usr/share/openshift/linux_arm64/oc'),
              ('ppc64le', 'linux', '/usr/share/openshift/linux_ppc64le/oc'),
              ('s390x', 'linux', '/usr/share/openshift/linux_s390x/oc'),
              ]:
          basename = os.path.basename(path)
          target_path = os.path.join(arch, operating_system, basename)
          os.symlink(path, target_path)
          base_root, _ = os.path.splitext(basename)
          archive_path_root = os.path.join(arch, operating_system, base_root)
          with tarfile.open('{}.tar'.format(archive_path_root), 'w') as tar:
              tar.add(path, basename)
          with zipfile.ZipFile('{}.zip'.format(archive_path_root), 'w') as zip:
              zip.write(path, basename)
      
      # Create socket
      # IPv6 should handle IPv4 passively so long as it is not bound to a
      # specific address or set to IPv6_ONLY
      # https://stackoverflow.com/questions/25817848/python-3-does-http-server-support-ipv6
      addr = ('::', 8080)
      sock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
      sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
      sock.bind(addr)
      sock.listen(5)
      
      [Thread(i, socket=sock) for i in range(100)]
      time.sleep(9e9)
      EOF
      exec python2 /tmp/serve.py  # the cli image only has Python 2.7
      
    State:          Running
      Started:      Wed, 13 May 2020 12:38:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8g84 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-f8g84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8g84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.33:8080/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.33:8080/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.33:8080/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)


Name:         downloads-78d964646c-v6x88
Namespace:    openshift-console
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 12:36:59 -0400
Labels:       app=console
              component=downloads
              pod-template-hash=78d964646c
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.34"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift.io/scc: restricted
Status:       Running
IP:           10.241.0.34
IPs:
  IP:           10.241.0.34
Controlled By:  ReplicaSet/downloads-78d964646c
Containers:
  download-server:
    Container ID:  cri-o://5d68b0050f3ca22f78ec9dc92a022965d40a9960e5de0ddc7cc181c4495f48fd
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:908f80952617b0f7324ac5f0fc053a83f1cda049b4229101adcf2e321ffd6c38
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:908f80952617b0f7324ac5f0fc053a83f1cda049b4229101adcf2e321ffd6c38
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /bin/sh
    Args:
      -c
      cat <<EOF >>/tmp/serve.py
      import BaseHTTPServer, os, re, signal, SimpleHTTPServer, socket, sys, tarfile, tempfile, threading, time, zipfile
      
      signal.signal(signal.SIGTERM, lambda signum, frame: sys.exit(0))
      
      # Launch multiple listeners as threads
      class Thread(threading.Thread):
          def __init__(self, i, socket):
              threading.Thread.__init__(self)
              self.i = i
              self.socket = socket
              self.daemon = True
              self.start()
      
          def run(self):
              httpd = BaseHTTPServer.HTTPServer(addr, SimpleHTTPServer.SimpleHTTPRequestHandler, False)
      
              # Prevent the HTTP server from re-binding every handler.
              # https://stackoverflow.com/questions/46210672/
              httpd.socket = self.socket
              httpd.server_bind = self.server_close = lambda self: None
      
              httpd.serve_forever()
      
      temp_dir = tempfile.mkdtemp()
      print('serving from {}'.format(temp_dir))
      os.chdir(temp_dir)
      for arch in ['amd64']:
          os.mkdir(arch)
          for operating_system in ['linux', 'mac', 'windows']:
              os.mkdir(os.path.join(arch, operating_system))
      for arch in ['arm64', 'ppc64le', 's390x']:
          os.mkdir(arch)
          for operating_system in ['linux']:
              os.mkdir(os.path.join(arch, operating_system))
      
      for arch, operating_system, path in [
              ('amd64', 'linux', '/usr/share/openshift/linux_amd64/oc'),
              ('amd64', 'mac', '/usr/share/openshift/mac/oc'),
              ('amd64', 'windows', '/usr/share/openshift/windows/oc.exe'),
              ('arm64', 'linux', '/usr/share/openshift/linux_arm64/oc'),
              ('ppc64le', 'linux', '/usr/share/openshift/linux_ppc64le/oc'),
              ('s390x', 'linux', '/usr/share/openshift/linux_s390x/oc'),
              ]:
          basename = os.path.basename(path)
          target_path = os.path.join(arch, operating_system, basename)
          os.symlink(path, target_path)
          base_root, _ = os.path.splitext(basename)
          archive_path_root = os.path.join(arch, operating_system, base_root)
          with tarfile.open('{}.tar'.format(archive_path_root), 'w') as tar:
              tar.add(path, basename)
          with zipfile.ZipFile('{}.zip'.format(archive_path_root), 'w') as zip:
              zip.write(path, basename)
      
      # Create socket
      # IPv6 should handle IPv4 passively so long as it is not bound to a
      # specific address or set to IPv6_ONLY
      # https://stackoverflow.com/questions/25817848/python-3-does-http-server-support-ipv6
      addr = ('::', 8080)
      sock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
      sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
      sock.bind(addr)
      sock.listen(5)
      
      [Thread(i, socket=sock) for i in range(100)]
      time.sleep(9e9)
      EOF
      exec python2 /tmp/serve.py  # the cli image only has Python 2.7
      
    State:          Running
      Started:      Wed, 13 May 2020 12:38:14 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8g84 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-f8g84:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8g84
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.34:8080/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.34:8080/: net/http: request canceled (Client.Timeout exceeded while awaiting headers)


Name:                 openshift-controller-manager-operator-75cd97bd87-jcmpz
Namespace:            openshift-controller-manager-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=openshift-controller-manager-operator
                      pod-template-hash=75cd97bd87
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.8"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.8
IPs:
  IP:           10.241.0.8
Controlled By:  ReplicaSet/openshift-controller-manager-operator-75cd97bd87
Containers:
  operator:
    Container ID:  cri-o://1e567de89d546b2d392445ca6051b381b6f4754319978bd3f99c83f33cc50743
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:798d94baf4901748bef3699f355398690526c8a179f9e461c4d9aa1007ef7b5b
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:798d94baf4901748bef3699f355398690526c8a179f9e461c4d9aa1007ef7b5b
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-openshift-controller-manager-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=4
    State:       Running
      Started:   Wed, 13 May 2020 12:38:37 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   :443: connect: connection refused
E0513 16:38:10.520782       1 reflector.go:280] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Failed to watch *v1.Build: Get https://172.30.0.1:443/apis/config.openshift.io/v1/builds?allowWatchBookmarks=true&resourceVersion=5002&timeout=7m22s&timeoutSeconds=442&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.521125       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-controller-manager/configmaps?allowWatchBookmarks=true&resourceVersion=8270&timeout=5m10s&timeoutSeconds=310&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.521227       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: Get https://172.30.0.1:443/api/v1/namespaces/openshift-controller-manager/services?allowWatchBookmarks=true&resourceVersion=4267&timeout=8m56s&timeoutSeconds=536&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.530658       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-controller-manager-operator/configmaps?allowWatchBookmarks=true&resourceVersion=8158&timeout=8m34s&timeoutSeconds=514&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.552547       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: Get https://172.30.0.1:443/api/v1/namespaces?allowWatchBookmarks=true&resourceVersion=7120&timeout=5m16s&timeoutSeconds=316&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
I0513 16:38:18.862682       1 leaderelection.go:287] failed to renew lease openshift-controller-manager-operator/openshift-controller-manager-operator-lock: failed to tryAcquireOrRenew context deadline exceeded
F0513 16:38:18.863053       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:32:44 -0400
      Finished:     Wed, 13 May 2020 12:38:18 -0400
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:  4.3.13
      IMAGE:            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7e4f524b9bb9930b0a7f5276c4de21f9a811edec2eeba943b901702b13e9f9cd
      POD_NAME:         openshift-controller-manager-operator-75cd97bd87-jcmpz (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-controller-manager-operator-token-z5ffb (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-controller-manager-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      openshift-controller-manager-operator-config
    Optional:  false
  openshift-controller-manager-operator-token-z5ffb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-controller-manager-operator-token-z5ffb
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 controller-manager-6scqd
Namespace:            openshift-controller-manager
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:02:44 -0400
Labels:               app=openshift-controller-manager
                      controller-manager=true
                      controller-revision-hash=6fd95ddfd5
                      pod-template-generation=11
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.101"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.101
IPs:
  IP:           10.241.0.101
Controlled By:  DaemonSet/controller-manager
Containers:
  controller-manager:
    Container ID:  cri-o://80e5f6523c336abebfe898c145a696e5b03b1da72bd4385f9ba3d1bdedc58ea8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7e4f524b9bb9930b0a7f5276c4de21f9a811edec2eeba943b901702b13e9f9cd
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7e4f524b9bb9930b0a7f5276c4de21f9a811edec2eeba943b901702b13e9f9cd
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      openshift-controller-manager
      start
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=2
    State:          Running
      Started:      Thu, 14 May 2020 07:02:48 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  <none>
    Mounts:
      /var/run/configmaps/client-ca from client-ca (rw)
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-controller-manager-sa-token-75mqx (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      config
    Optional:  false
  client-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      client-ca
    Optional:  false
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  serving-cert
    Optional:    false
  openshift-controller-manager-sa-token-75mqx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-controller-manager-sa-token-75mqx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     
Events:
  Type    Reason     Age        From                                Message
  ----    ------     ----       ----                                -------
  Normal  Scheduled  <unknown>  default-scheduler                   Successfully assigned openshift-controller-manager/controller-manager-6scqd to dc1master01.os4.ringen.us
  Normal  Pulled     27m        kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7e4f524b9bb9930b0a7f5276c4de21f9a811edec2eeba943b901702b13e9f9cd" already present on machine
  Normal  Created    27m        kubelet, dc1master01.os4.ringen.us  Created container controller-manager
  Normal  Started    27m        kubelet, dc1master01.os4.ringen.us  Started container controller-manager


Name:                 dns-operator-86d8bcc788-n6pkl
Namespace:            openshift-dns-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               name=dns-operator
                      pod-template-hash=86d8bcc788
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.13"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.13
IPs:
  IP:           10.241.0.13
Controlled By:  ReplicaSet/dns-operator-86d8bcc788
Containers:
  dns-operator:
    Container ID:  cri-o://fedfa252e3690161331d5b5f816b0a1dec376397b1416426460fac8d8cb87d68
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:385c17b2080e4d8fc2ee0fc5958c7f4aef50c363a47b98f36a5aad9d3d5f4fc0
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:385c17b2080e4d8fc2ee0fc5958c7f4aef50c363a47b98f36a5aad9d3d5f4fc0
    Port:          <none>
    Host Port:     <none>
    Command:
      dns-operator
    State:          Running
      Started:      Wed, 13 May 2020 12:31:58 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      RELEASE_VERSION:      4.3.13
      IMAGE:                quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
      OPENSHIFT_CLI_IMAGE:  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from dns-operator-token-6w8qh (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://c43816de8953780f252372c34add90a39bb916e36857170a7913ba662e12047c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9393/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:9393
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:60000/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:31:59 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from metrics-tls (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-operator-token-6w8qh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  metrics-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  metrics-tls
    Optional:    false
  dns-operator-token-6w8qh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  dns-operator-token-6w8qh
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 dns-default-fnn7t
Namespace:            openshift-dns
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:01:30 -0400
Labels:               controller-revision-hash=864646c597
                      dns.operator.openshift.io/daemonset-dns=default
                      pod-template-generation=1
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.2.2"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.2.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.2.2
IPs:
  IP:           10.241.2.2
Controlled By:  DaemonSet/dns-default
Containers:
  dns:
    Container ID:  cri-o://686c8613240897ad13209ca09374dbd59e262d82b7dbaf2b1c7bd9228d9259ad
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Ports:         5353/UDP, 5353/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Command:
      coredns
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Wed, 13 May 2020 13:01:36 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8080/health delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
  dns-node-resolver:
    Container ID:  cri-o://2fdf8647b2ea6d4c48ee3b2fbfe8c1b8afef203458057d29a8ac00727bb4ec7b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -uo pipefail
      
      trap 'jobs -p | xargs kill || true; wait; exit 0' TERM
      
      OPENSHIFT_MARKER="openshift-generated-node-resolver"
      HOSTS_FILE="/etc/hosts"
      TEMP_FILE="/etc/hosts.tmp"
      
      IFS=', ' read -r -a services <<< "${SERVICES}"
      
      # Make a temporary file with the old hosts file's attributes.
      cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"
      
      while true; do
        declare -A svc_ips
        for svc in "${services[@]}"; do
          # Fetch service IP from cluster dns if present. We make several tries
          # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
          # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
          # support UDP loadbalancers and require reaching DNS through TCP.
          cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t A +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"')
          for i in ${!cmds[*]}
          do
            ips=($(eval "${cmds[i]}"))
            if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
              svc_ips["${svc}"]="${ips[@]}"
              break
            fi
          done
        done
      
        # Update /etc/hosts only if we get valid service IPs
        # We will not update /etc/hosts when there is coredns service outage or api unavailability
        # Stale entries could exist in /etc/hosts if the service is deleted
        if [[ "${#svc_ips[@]}" -ne 0 ]]; then
          # Build a new hosts file from /etc/hosts with our custom entries filtered out
          grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"
      
          # Append resolver entries for services
          for svc in "${!svc_ips[@]}"; do
            for ip in ${svc_ips[${svc}]}; do
              echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
            done
          done
      
          # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
          # Replace /etc/hosts with our modified version if needed
          cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
          # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
        fi
        sleep 60 & wait
        unset svc_ips
      done
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:41 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      SERVICES:        image-registry.openshift-image-registry.svc
      NAMESERVER:      172.30.0.10
      CLUSTER_DOMAIN:  cluster.local
    Mounts:
      /etc/hosts from hosts-file (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      dns-default
    Optional:  false
  hosts-file:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/hosts
    HostPathType:  File
  dns-token-2q2qd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  dns-token-2q2qd
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/master
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 dns-default-vgv8j
Namespace:            openshift-dns
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:01 -0400
Labels:               controller-revision-hash=864646c597
                      dns.operator.openshift.io/daemonset-dns=default
                      pod-template-generation=1
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.17"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.17
IPs:
  IP:           10.241.0.17
Controlled By:  DaemonSet/dns-default
Containers:
  dns:
    Container ID:  cri-o://1ebb4eb6e3b51aca78fe42867ce2be5fdd634daeea80b565bf5fb9b551dbecfc
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Ports:         5353/UDP, 5353/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Command:
      coredns
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Wed, 13 May 2020 12:32:11 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8080/health delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
  dns-node-resolver:
    Container ID:  cri-o://f89f94776c0a18e72d2408ccd6d6125083bb6a947219c8697f25b792768ebbf8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -uo pipefail
      
      trap 'jobs -p | xargs kill || true; wait; exit 0' TERM
      
      OPENSHIFT_MARKER="openshift-generated-node-resolver"
      HOSTS_FILE="/etc/hosts"
      TEMP_FILE="/etc/hosts.tmp"
      
      IFS=', ' read -r -a services <<< "${SERVICES}"
      
      # Make a temporary file with the old hosts file's attributes.
      cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"
      
      while true; do
        declare -A svc_ips
        for svc in "${services[@]}"; do
          # Fetch service IP from cluster dns if present. We make several tries
          # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
          # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
          # support UDP loadbalancers and require reaching DNS through TCP.
          cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t A +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"')
          for i in ${!cmds[*]}
          do
            ips=($(eval "${cmds[i]}"))
            if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
              svc_ips["${svc}"]="${ips[@]}"
              break
            fi
          done
        done
      
        # Update /etc/hosts only if we get valid service IPs
        # We will not update /etc/hosts when there is coredns service outage or api unavailability
        # Stale entries could exist in /etc/hosts if the service is deleted
        if [[ "${#svc_ips[@]}" -ne 0 ]]; then
          # Build a new hosts file from /etc/hosts with our custom entries filtered out
          grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"
      
          # Append resolver entries for services
          for svc in "${!svc_ips[@]}"; do
            for ip in ${svc_ips[${svc}]}; do
              echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
            done
          done
      
          # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
          # Replace /etc/hosts with our modified version if needed
          cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
          # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
        fi
        sleep 60 & wait
        unset svc_ips
      done
      
    State:          Running
      Started:      Wed, 13 May 2020 12:32:17 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      SERVICES:        image-registry.openshift-image-registry.svc
      NAMESERVER:      172.30.0.10
      CLUSTER_DOMAIN:  cluster.local
    Mounts:
      /etc/hosts from hosts-file (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      dns-default
    Optional:  false
  hosts-file:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/hosts
    HostPathType:  File
  dns-token-2q2qd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  dns-token-2q2qd
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/master
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 dns-default-znr7z
Namespace:            openshift-dns
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:01:20 -0400
Labels:               controller-revision-hash=864646c597
                      dns.operator.openshift.io/daemonset-dns=default
                      pod-template-generation=1
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.1.3"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.1.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.1.3
IPs:
  IP:           10.241.1.3
Controlled By:  DaemonSet/dns-default
Containers:
  dns:
    Container ID:  cri-o://c362abfb67240c5e242666be1a8b47ff39d92042091d825c9fa4aed1ca4355a0
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:56a630ea3bf343be622bc6376dcad3f092260b1f5c76ea61fe310689f55409ae
    Ports:         5353/UDP, 5353/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Command:
      coredns
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Wed, 13 May 2020 13:01:30 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8080/health delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
  dns-node-resolver:
    Container ID:  cri-o://e18c03be015f49d43b0bbab841303007bfb53a91daa7343db3bd9f248657660e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -uo pipefail
      
      trap 'jobs -p | xargs kill || true; wait; exit 0' TERM
      
      OPENSHIFT_MARKER="openshift-generated-node-resolver"
      HOSTS_FILE="/etc/hosts"
      TEMP_FILE="/etc/hosts.tmp"
      
      IFS=', ' read -r -a services <<< "${SERVICES}"
      
      # Make a temporary file with the old hosts file's attributes.
      cp -f --attributes-only "${HOSTS_FILE}" "${TEMP_FILE}"
      
      while true; do
        declare -A svc_ips
        for svc in "${services[@]}"; do
          # Fetch service IP from cluster dns if present. We make several tries
          # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones
          # are for deployments with Kuryr on older OpenStack (OSP13) - those do not
          # support UDP loadbalancers and require reaching DNS through TCP.
          cmds=('dig -t A @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t A +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"'
                'dig -t AAAA +tcp @"${NAMESERVER}" +short "${svc}.${CLUSTER_DOMAIN}"')
          for i in ${!cmds[*]}
          do
            ips=($(eval "${cmds[i]}"))
            if [[ "$?" -eq 0 && "${#ips[@]}" -ne 0 ]]; then
              svc_ips["${svc}"]="${ips[@]}"
              break
            fi
          done
        done
      
        # Update /etc/hosts only if we get valid service IPs
        # We will not update /etc/hosts when there is coredns service outage or api unavailability
        # Stale entries could exist in /etc/hosts if the service is deleted
        if [[ "${#svc_ips[@]}" -ne 0 ]]; then
          # Build a new hosts file from /etc/hosts with our custom entries filtered out
          grep -v "# ${OPENSHIFT_MARKER}" "${HOSTS_FILE}" > "${TEMP_FILE}"
      
          # Append resolver entries for services
          for svc in "${!svc_ips[@]}"; do
            for ip in ${svc_ips[${svc}]}; do
              echo "${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}" >> "${TEMP_FILE}"
            done
          done
      
          # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior
          # Replace /etc/hosts with our modified version if needed
          cmp "${TEMP_FILE}" "${HOSTS_FILE}" || cp -f "${TEMP_FILE}" "${HOSTS_FILE}"
          # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn
        fi
        sleep 60 & wait
        unset svc_ips
      done
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:35 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      SERVICES:        image-registry.openshift-image-registry.svc
      NAMESERVER:      172.30.0.10
      CLUSTER_DOMAIN:  cluster.local
    Mounts:
      /etc/hosts from hosts-file (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from dns-token-2q2qd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      dns-default
    Optional:  false
  hosts-file:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/hosts
    HostPathType:  File
  dns-token-2q2qd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  dns-token-2q2qd
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/master
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 etcd-member-dc1master01.os4.ringen.us
Namespace:            openshift-etcd
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:24:34 -0400
Labels:               k8s-app=etcd
Annotations:          kubernetes.io/config.hash: 665e7b0c8259fd4048560e91a7e1ce78
                      kubernetes.io/config.mirror: 665e7b0c8259fd4048560e91a7e1ce78
                      kubernetes.io/config.seen: 2020-05-13T16:24:11.503815844Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:  192.168.99.41
Init Containers:
  discovery:
    Container ID:  cri-o://d1ee9c984e82d57b62d979f1559ef3fc4f4eb89627352310a9fca2fc44ba5fa1
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/setup-etcd-environment
    Args:
      run
      --discovery-srv=os4.ringen.us
      --output-file=/run/etcd/environment
      --v=4
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:24:49 -0400
      Finished:     Wed, 13 May 2020 12:24:49 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /run/etcd/ from discovery (rw)
  certs:
    Container ID:  cri-o://5cffae8ab986036c9f628b3e588659a87e111ab7290ceb65617bbce03a2907c1
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bba6fb42ff43c74977eff803b78b860b7d3399873b88651988c5edeccadcf689
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bba6fb42ff43c74977eff803b78b860b7d3399873b88651988c5edeccadcf689
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      #!/bin/sh
      set -euxo pipefail
      
      source /run/etcd/environment
      
      [ -e /etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-servers \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=localhost,etcd.kube-system.svc,etcd.kube-system.svc.cluster.local,etcd.openshift-etcd.svc,etcd.openshift-etcd.svc.cluster.local,${ETCD_WILDCARD_DNS_NAME} \
            --commonname=system:etcd-server:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS},127.0.0.1 \
      
      [ -e /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-peers \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=${ETCD_DNS_NAME},os4.ringen.us \
            --commonname=system:etcd-peer:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS} \
      
      [ -e /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-metrics \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=localhost,etcd.kube-system.svc,etcd.kube-system.svc.cluster.local,etcd.openshift-etcd.svc,etcd.openshift-etcd.svc.cluster.local,${ETCD_WILDCARD_DNS_NAME} \
            --commonname=system:etcd-metric:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS} \
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:24:52 -0400
      Finished:     Wed, 13 May 2020 12:25:23 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/kubeconfig from kubeconfig (rw)
      /etc/ssl/etcd/ from certs (rw)
      /run/etcd/ from discovery (rw)
Containers:
  etcd-member:
    Container ID:  cri-o://2ef711dc417369ef9a48304c02a9c25221e7fa194db08bef44c9754629a923c4
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7eb3c86d8af5711245d43234689a6877fc89712d3edd04399c49fe39b421b860
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7eb3c86d8af5711245d43234689a6877fc89712d3edd04399c49fe39b421b860
    Ports:         2380/TCP, 2379/TCP
    Host Ports:    2380/TCP, 2379/TCP
    Command:
      /bin/sh
      -c
      #!/bin/sh
      set -euo pipefail
      
      source /run/etcd/environment
      
      set -a
      source /etc/etcd/etcd.conf
      set +a
      
      exec etcd \
        --initial-advertise-peer-urls=https://${ETCD_ESCAPED_IP_ADDRESS}:2380 \
        --cert-file=/etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.crt \
        --key-file=/etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.key \
        --trusted-ca-file=/etc/ssl/etcd/ca.crt \
        --client-cert-auth=true \
        --peer-cert-file=/etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt \
        --peer-key-file=/etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key \
        --peer-trusted-ca-file=/etc/ssl/etcd/ca.crt \
        --peer-client-cert-auth=true \
        --advertise-client-urls=https://${ETCD_ESCAPED_IP_ADDRESS}:2379 \
        --listen-client-urls=https://${ETCD_ESCAPED_ALL_IPS}:2379 \
        --listen-peer-urls=https://${ETCD_ESCAPED_ALL_IPS}:2380 \
        --listen-metrics-urls=https://${ETCD_ESCAPED_ALL_IPS}:9978 \
      
    State:          Running
      Started:      Wed, 13 May 2020 12:25:27 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     300m
      memory:  600Mi
    Environment:
      ETCDCTL_API:    3
      ETCD_DATA_DIR:  /var/lib/etcd
      ETCD_NAME:      etcd-member-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/etcd/ from conf (rw)
      /etc/ssl/etcd/ from certs (rw)
      /run/etcd/ from discovery (rw)
      /var/lib/etcd/ from data-dir (rw)
  etcd-metrics:
    Container ID:  cri-o://27a58715d33b396a3c797103767263a19b6b80a8b2e57980e3ed1641ef58b6a2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7eb3c86d8af5711245d43234689a6877fc89712d3edd04399c49fe39b421b860
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7eb3c86d8af5711245d43234689a6877fc89712d3edd04399c49fe39b421b860
    Port:          9979/TCP
    Host Port:     9979/TCP
    Command:
      /bin/sh
      -c
      #!/bin/sh
      set -euo pipefail
      
      source /run/etcd/environment
      
      exec etcd grpc-proxy start \
        --endpoints https://${ETCD_DNS_NAME}:9978 \
        --metrics-addr https://${ETCD_ESCAPED_ALL_IPS}:9979 \
        --listen-addr ${ETCD_ESCAPED_LOCALHOST_IP}:9977 \
        --key /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key \
        --key-file /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.key \
        --cert /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt \
        --cert-file /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.crt \
        --cacert /etc/ssl/etcd/ca.crt \
        --trusted-ca-file /etc/ssl/etcd/metric-ca.crt \
      
    State:          Running
      Started:      Wed, 13 May 2020 12:25:27 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      ETCDCTL_API:  3
    Mounts:
      /etc/ssl/etcd/ from certs (rw)
      /run/etcd/ from discovery (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/etcd-member
    HostPathType:  
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/kubeconfig
    HostPathType:  
  discovery:
    Type:          HostPath (bare host directory volume)
    Path:          /run/etcd
    HostPathType:  
  data-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  
  conf:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/etcd
    HostPathType:  
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       
Events:            <none>


Name:                 cluster-image-registry-operator-9754995-zkdfl
Namespace:            openshift-image-registry
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:13 -0400
Labels:               name=cluster-image-registry-operator
                      pod-template-hash=9754995
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.39"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.39
IPs:
  IP:           10.241.0.39
Controlled By:  ReplicaSet/cluster-image-registry-operator-9754995
Containers:
  cluster-image-registry-operator:
    Container ID:   cri-o://79c01aa52cc3299e7835c1b4de3feb3bbde0448548f23389e053445bccc29add
    Image:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8cdebdc213cccefdd1785a73c70c704642edde81cf8266ba04094773c8d3c7ec
    Image ID:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8cdebdc213cccefdd1785a73c70c704642edde81cf8266ba04094773c8d3c7ec
    Port:           60000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 13 May 2020 12:38:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      RELEASE_VERSION:  4.3.13
      WATCH_NAMESPACE:  openshift-image-registry (v1:metadata.namespace)
      OPERATOR_NAME:    cluster-image-registry-operator
      IMAGE:            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fb9cf86efea384647eb6e77ae532246a34b9140fbaeaa87b2d587305905378dd
    Mounts:
      /etc/secrets from image-registry-operator-tls (rw)
      /var/run/configmaps/trusted-ca/ from trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-image-registry-operator-token-cdjwt (ro)
  cluster-image-registry-operator-watch:
    Container ID:  cri-o://2a904890db93ef6251aae425c0ae5e315e6e6d9ef629181be8964cb386284ac2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8cdebdc213cccefdd1785a73c70c704642edde81cf8266ba04094773c8d3c7ec
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8cdebdc213cccefdd1785a73c70c704642edde81cf8266ba04094773c8d3c7ec
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-image-registry-operator-watch
    Args:
      file-watcher-watchdog
      --namespace=$(POD_NAMESPACE)
      --process-name=cluster-image-registry-operator
      --termination-grace-period=30s
      --files=/var/run/configmaps/trusted-ca/tls-ca-bundle.pem
      --files=/etc/secrets/tls.crt
      --files=/etc/secrets/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:38:14 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      POD_NAME:       cluster-image-registry-operator-9754995-zkdfl (v1:metadata.name)
      POD_NAMESPACE:  openshift-image-registry (v1:metadata.namespace)
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-image-registry-operator-token-cdjwt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  image-registry-operator-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  image-registry-operator-tls
    Optional:    false
  trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca
    Optional:  true
  cluster-image-registry-operator-token-cdjwt:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-image-registry-operator-token-cdjwt
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 ingress-operator-7854d9fd4f-fprn2
Namespace:            openshift-ingress-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:14 -0400
Labels:               name=ingress-operator
                      pod-template-hash=7854d9fd4f
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.47"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.47
IPs:
  IP:           10.241.0.47
Controlled By:  ReplicaSet/ingress-operator-7854d9fd4f
Containers:
  ingress-operator:
    Container ID:  cri-o://9e604be4cc2ad3b124b42846958d06cf8e8c53427dd9f952fadfb84c2cc692c9
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81894a9cf702a7e32c69b057bf7011bd52fe830d5fe76ab34f42b3081449d9d2
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81894a9cf702a7e32c69b057bf7011bd52fe830d5fe76ab34f42b3081449d9d2
    Port:          <none>
    Host Port:     <none>
    Command:
      ingress-operator
      start
    State:          Running
      Started:      Wed, 13 May 2020 12:38:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  10m
    Environment:
      RELEASE_VERSION:  4.3.13
      WATCH_NAMESPACE:  openshift-ingress-operator (v1:metadata.namespace)
      IMAGE:            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Mounts:
      /etc/pki/ca-trust/extracted/pem from trusted-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from ingress-operator-token-q64cm (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://a8f35b426b82c05b9e2d06b8f24b947f3cb7f452ab6404471f96d338f345f863
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9393/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:9393
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:60000/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:38:17 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from metrics-tls (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from ingress-operator-token-q64cm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  metrics-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  metrics-tls
    Optional:    false
  trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca
    Optional:  false
  ingress-operator-token-q64cm:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  ingress-operator-token-q64cm
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 router-default-d7c84c7df-74cc8
Namespace:            openshift-ingress
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:38:58 -0400
Labels:               ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
                      pod-template-hash=d7c84c7df
Annotations:          openshift.io/scc: hostnetwork
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  ReplicaSet/router-default-d7c84c7df
Containers:
  router:
    Container ID:   cri-o://cb4524b460f7e67a33623d46b23508f9e14396074f809760e1051b509eb9ebd5
    Image:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Image ID:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Ports:          80/TCP, 443/TCP, 1936/TCP
    Host Ports:     80/TCP, 443/TCP, 1936/TCP
    State:          Running
      Started:      Wed, 13 May 2020 12:40:25 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   256Mi
    Liveness:   http-get http://localhost:1936/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://localhost:1936/healthz/ready delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DEFAULT_CERTIFICATE_DIR:       /etc/pki/tls/private
      ROUTER_CANONICAL_HOSTNAME:     apps.os4.ringen.us
      ROUTER_CIPHERS:                TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
      ROUTER_METRICS_TLS_CERT_FILE:  /etc/pki/tls/metrics-certs/tls.crt
      ROUTER_METRICS_TLS_KEY_FILE:   /etc/pki/tls/metrics-certs/tls.key
      ROUTER_METRICS_TYPE:           haproxy
      ROUTER_SERVICE_NAME:           default
      ROUTER_SERVICE_NAMESPACE:      openshift-ingress
      ROUTER_THREADS:                4
      SSL_MIN_VERSION:               TLSv1.2
      STATS_PASSWORD:                <set to the key 'statsPassword' in secret 'router-stats-default'>  Optional: false
      STATS_PORT:                    1936
      STATS_USERNAME:                <set to the key 'statsUsername' in secret 'router-stats-default'>  Optional: false
    Mounts:
      /etc/pki/tls/metrics-certs from metrics-certs (ro)
      /etc/pki/tls/private from default-certificate (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from router-token-bx58h (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-certificate:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-certs-default
    Optional:    false
  metrics-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-metrics-certs-default
    Optional:    false
  router-token-bx58h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-token-bx58h
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
                 node-role.kubernetes.io/worker=
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://localhost:1936/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://localhost:1936/healthz/ready: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://localhost:1936/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                      router-default-d7c84c7df-97fnm
Namespace:                 openshift-ingress
Priority:                  2000000000
Priority Class Name:       system-cluster-critical
Node:                      ibmworker01.os4.ringen.us/192.168.93.43
Start Time:                Wed, 13 May 2020 13:01:19 -0400
Labels:                    ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
                           pod-template-hash=d7c84c7df
Annotations:               openshift.io/scc: hostnetwork
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  ReplicaSet/router-default-d7c84c7df
Containers:
  router:
    Container ID:   cri-o://67ab4fe700008b2b9e0d7e4789821761166920df1f3baada4b7d970e85191290
    Image:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Image ID:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Ports:          80/TCP, 443/TCP, 1936/TCP
    Host Ports:     80/TCP, 443/TCP, 1936/TCP
    State:          Running
      Started:      Wed, 13 May 2020 13:01:26 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   256Mi
    Liveness:   http-get http://localhost:1936/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://localhost:1936/healthz/ready delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DEFAULT_CERTIFICATE_DIR:       /etc/pki/tls/private
      ROUTER_CANONICAL_HOSTNAME:     apps.os4.ringen.us
      ROUTER_CIPHERS:                TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
      ROUTER_METRICS_TLS_CERT_FILE:  /etc/pki/tls/metrics-certs/tls.crt
      ROUTER_METRICS_TLS_KEY_FILE:   /etc/pki/tls/metrics-certs/tls.key
      ROUTER_METRICS_TYPE:           haproxy
      ROUTER_SERVICE_NAME:           default
      ROUTER_SERVICE_NAMESPACE:      openshift-ingress
      ROUTER_THREADS:                4
      SSL_MIN_VERSION:               TLSv1.2
      STATS_PASSWORD:                <set to the key 'statsPassword' in secret 'router-stats-default'>  Optional: false
      STATS_PORT:                    1936
      STATS_USERNAME:                <set to the key 'statsUsername' in secret 'router-stats-default'>  Optional: false
    Mounts:
      /etc/pki/tls/metrics-certs from metrics-certs (ro)
      /etc/pki/tls/private from default-certificate (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from router-token-bx58h (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-certificate:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-certs-default
    Optional:    false
  metrics-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-metrics-certs-default
    Optional:    false
  router-token-bx58h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-token-bx58h
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
                 node-role.kubernetes.io/worker=
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 router-default-d7c84c7df-crj59
Namespace:            openshift-ingress
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
                      pod-template-hash=d7c84c7df
Annotations:          openshift.io/scc: hostnetwork
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/router-default-d7c84c7df
Containers:
  router:
    Image:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9ea98af9b00609e17666738516a0eb750c31318a5b08c78cc0547b63b00e00b6
    Ports:       80/TCP, 443/TCP, 1936/TCP
    Host Ports:  80/TCP, 443/TCP, 1936/TCP
    Requests:
      cpu:      100m
      memory:   256Mi
    Liveness:   http-get http://localhost:1936/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://localhost:1936/healthz/ready delay=10s timeout=1s period=10s #success=1 #failure=3
    Environment:
      DEFAULT_CERTIFICATE_DIR:       /etc/pki/tls/private
      ROUTER_CANONICAL_HOSTNAME:     apps.os4.ringen.us
      ROUTER_CIPHERS:                TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
      ROUTER_METRICS_TLS_CERT_FILE:  /etc/pki/tls/metrics-certs/tls.crt
      ROUTER_METRICS_TLS_KEY_FILE:   /etc/pki/tls/metrics-certs/tls.key
      ROUTER_METRICS_TYPE:           haproxy
      ROUTER_SERVICE_NAME:           default
      ROUTER_SERVICE_NAMESPACE:      openshift-ingress
      ROUTER_THREADS:                4
      SSL_MIN_VERSION:               TLSv1.2
      STATS_PASSWORD:                <set to the key 'statsPassword' in secret 'router-stats-default'>  Optional: false
      STATS_PORT:                    1936
      STATS_USERNAME:                <set to the key 'statsUsername' in secret 'router-stats-default'>  Optional: false
    Mounts:
      /etc/pki/tls/metrics-certs from metrics-certs (ro)
      /etc/pki/tls/private from default-certificate (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from router-token-bx58h (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  default-certificate:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-certs-default
    Optional:    false
  metrics-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-metrics-certs-default
    Optional:    false
  router-token-bx58h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  router-token-bx58h
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
                 node-role.kubernetes.io/worker=
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't have free ports for the requested pod ports.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't have free ports for the requested pod ports.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) didn't have free ports for the requested pod ports.


Name:                 insights-operator-b67db4c5d-m2jrh
Namespace:            openshift-insights
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:13 -0400
Labels:               app=insights-operator
                      pod-template-hash=b67db4c5d
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.40"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.40
IPs:
  IP:           10.241.0.40
Controlled By:  ReplicaSet/insights-operator-b67db4c5d
Containers:
  operator:
    Container ID:  cri-o://d746bb999a8676f3017422777042e7ea9f9da97ebe1a33d425d1bfb3a969cce2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4a9ca7f0c86ac4c7192fb78bf04754ad55b012bff0203e845116c1a29cc893f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4a9ca7f0c86ac4c7192fb78bf04754ad55b012bff0203e845116c1a29cc893f
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      start
      -v=4
      --config=/etc/insights-operator/server.yaml
    State:       Running
      Started:   Wed, 13 May 2020 12:44:21 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   o:63] Recording config/infrastructure with fingerprint=
I0513 16:38:39.564700       1 diskrecorder.go:63] Recording config/network with fingerprint=
I0513 16:38:40.899889       1 diskrecorder.go:63] Recording config/authentication with fingerprint=
I0513 16:38:41.003812       1 diskrecorder.go:63] Recording config/featuregate with fingerprint=
I0513 16:38:42.906392       1 diskrecorder.go:63] Recording config/oauth with fingerprint=
I0513 16:38:42.947212       1 diskrecorder.go:63] Recording config/ingress with fingerprint=
I0513 16:38:43.108993       1 diskrecorder.go:63] Recording config/proxy with fingerprint=
I0513 16:38:43.109740       1 diskrecorder.go:170] Writing 30 records to /var/lib/insights-operator/insights-2020-05-13-163843.tar.gz
I0513 16:38:43.177207       1 diskrecorder.go:134] Wrote 30 records to disk in 67ms
I0513 16:38:43.177313       1 periodic.go:151] Periodic gather config completed in 7.648s
I0513 16:38:43.177362       1 controllerstatus.go:40] name=periodic-config healthy=true reason= message=
I0513 16:40:36.074074       1 status.go:298] The operator is healthy
I0513 16:42:36.278891       1 status.go:298] The operator is healthy
I0513 16:43:35.888749       1 configobserver.go:65] Refreshing configuration from cluster pull secret
I0513 16:43:35.897754       1 configobserver.go:90] Found cloud.openshift.com token
I0513 16:43:35.897834       1 configobserver.go:107] Refreshing configuration from cluster secret
I0513 16:43:35.901984       1 configobserver.go:111] Support secret does not exist
I0513 16:43:40.506580       1 observer_polling.go:114] Observed file "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt" has been modified (old="e8d493eb481c0a5d16468af822ac0fc0310a5e210bb7597ec8f582cbd232afca", new="528e70a7e8e94e16f9844ce5f6ead7c43449d281464c6ff6e15d35309991f080")
W0513 16:43:40.506697       1 builder.go:108] Restart triggered because of file /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt was modified
F0513 16:43:40.506797       1 builder.go:217] server exited

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:38:14 -0400
      Finished:     Wed, 13 May 2020 12:43:40 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:     10m
      memory:  30Mi
    Environment:
      POD_NAME:         insights-operator-b67db4c5d-m2jrh (v1:metadata.name)
      POD_NAMESPACE:    openshift-insights (v1:metadata.namespace)
      RELEASE_VERSION:  4.3.13
    Mounts:
      /var/lib/insights-operator from snapshots (rw)
      /var/run/configmaps/trusted-ca-bundle from trusted-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from operator-token-7ztrz (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  snapshots:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      trusted-ca-bundle
    Optional:  true
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-insights-serving-cert
    Optional:    true
  operator-token-7ztrz:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  operator-token-7ztrz
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 900s
                 node.kubernetes.io/unreachable:NoExecute for 900s
Events:          <none>


Name:                 kube-apiserver-operator-5ff98d854c-n2qfq
Namespace:            openshift-kube-apiserver-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=kube-apiserver-operator
                      pod-template-hash=5ff98d854c
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.9"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.9
IPs:
  IP:           10.241.0.9
Controlled By:  ReplicaSet/kube-apiserver-operator-5ff98d854c
Containers:
  kube-apiserver-operator:
    Container ID:  cri-o://76f4ca489e3c9e1aec7b2c3c4b6511d6b1311d22312f94867228f04e9c0c529f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-kube-apiserver-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
    State:       Running
      Started:   Wed, 13 May 2020 12:38:48 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   m4s&timeoutSeconds=424&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.635275       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: Get https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?allowWatchBookmarks=true&resourceVersion=5959&timeout=5m25s&timeoutSeconds=325&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.644343       1 reflector.go:280] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Failed to watch *v1.Scheduler: Get https://172.30.0.1:443/apis/config.openshift.io/v1/schedulers?allowWatchBookmarks=true&resourceVersion=5004&timeout=6m24s&timeoutSeconds=384&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.658937       1 reflector.go:280] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Failed to watch *v1.Infrastructure: Get https://172.30.0.1:443/apis/config.openshift.io/v1/infrastructures?allowWatchBookmarks=true&resourceVersion=5009&timeout=6m57s&timeoutSeconds=417&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.664057       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?allowWatchBookmarks=true&resourceVersion=7194&timeout=6m0s&timeoutSeconds=360&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
I0513 16:38:20.737119       1 leaderelection.go:287] failed to renew lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock: failed to tryAcquireOrRenew context deadline exceeded
I0513 16:38:20.742554       1 event.go:255] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"", Name:"", UID:"", APIVersion:"v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' f2bed4f6-4467-4df6-8406-d586ce145f6c stopped leading
F0513 16:38:20.745717       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:34:40 -0400
      Finished:     Wed, 13 May 2020 12:38:20 -0400
    Ready:          True
    Restart Count:  3
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
      OPERATOR_IMAGE:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
      OPERAND_IMAGE_VERSION:   1.16.2
      OPERATOR_IMAGE_VERSION:  4.3.13
      POD_NAME:                kube-apiserver-operator-5ff98d854c-n2qfq (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-apiserver-operator-token-nf726 (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-apiserver-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-apiserver-operator-config
    Optional:  false
  kube-apiserver-operator-token-nf726:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-apiserver-operator-token-nf726
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 installer-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:30 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.20
IPs:
  IP:  10.241.0.20
Containers:
  installer:
    Container ID:  cri-o://10126a7c1499c610594ffa570e38d02fb8cebf4afdeb29c8760dc22f8c20b269
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=2
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:32:34 -0400
      Finished:     Wed, 13 May 2020 12:32:39 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-2-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:36:22 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.31"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Succeeded
IP:                   10.241.0.31
IPs:
  IP:  10.241.0.31
Containers:
  installer:
    Container ID:  cri-o://4e1e978b970cc0bae5f1c4122f43f958a677dc92c7dff6626a7ac2528edf57bd
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=3
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:36:24 -0400
      Finished:     Wed, 13 May 2020 12:36:30 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-3-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:59 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.64
IPs:
  IP:  10.241.0.64
Containers:
  installer:
    Container ID:  cri-o://9ec6ea5403d27db02370e703938934a53dd576e004c22a383e197cbf48483846
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=4
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:41:02 -0400
      Finished:     Wed, 13 May 2020 12:41:07 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-4-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-5-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:46 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.75"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Succeeded
IP:                   10.241.0.75
IPs:
  IP:  10.241.0.75
Containers:
  installer:
    Container ID:  cri-o://80b559ca800fcb169cc2a24878c8c6e8402ddce1211e4144826861dcf9e66291
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=5
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:43:03 -0400
      Finished:     Wed, 13 May 2020 12:43:10 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-5-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-6-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:00:52 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.99
IPs:
  IP:  10.241.0.99
Containers:
  installer:
    Container ID:  cri-o://04f0cba99217c50fd49d8473d9793cf9c4c6aa768df0b697c7611cba376b85ec
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=6
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:00:55 -0400
      Finished:     Thu, 14 May 2020 07:01:00 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-6-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason                       Age   From                                Message
  ----    ------                       ----  ----                                -------
  Normal  Pulled                       29m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal  Created                      29m   kubelet, dc1master01.os4.ringen.us  Created container installer
  Normal  Started                      29m   kubelet, dc1master01.os4.ringen.us  Started container installer
  Normal  StaticPodInstallerCompleted  29m   static-pod-installer                Successfully installed revision 6


Name:                 installer-7-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:09:42 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.105
IPs:
  IP:  10.241.0.105
Containers:
  installer:
    Container ID:  cri-o://ab11d0275e5520e2a9525f22008cbdddc25ecbb5bfdd3b66a79c19598230b566
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      installer
    Args:
      -v=2
      --revision=7
      --namespace=openshift-kube-apiserver
      --pod=kube-apiserver-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-apiserver-pod
      --configmaps=config
      --configmaps=kube-apiserver-cert-syncer-kubeconfig
      --optional-configmaps=oauth-metadata
      --optional-configmaps=cloud-config
      --configmaps=etcd-serving-ca
      --optional-configmaps=kube-apiserver-server-ca
      --configmaps=kubelet-serving-ca
      --configmaps=sa-token-signing-certs
      --secrets=etcd-client
      --secrets=kube-apiserver-cert-syncer-client-cert-key
      --secrets=kubelet-client
      --optional-secrets=encryption-config
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-apiserver-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=aggregator-client
      --cert-secrets=localhost-serving-cert-certkey
      --cert-secrets=service-network-serving-certkey
      --cert-secrets=external-loadbalancer-serving-certkey
      --cert-secrets=internal-loadbalancer-serving-certkey
      --cert-secrets=localhost-recovery-serving-certkey
      --optional-cert-secrets=user-serving-cert
      --optional-cert-secrets=user-serving-cert-000
      --optional-cert-secrets=user-serving-cert-001
      --optional-cert-secrets=user-serving-cert-002
      --optional-cert-secrets=user-serving-cert-003
      --optional-cert-secrets=user-serving-cert-004
      --optional-cert-secrets=user-serving-cert-005
      --optional-cert-secrets=user-serving-cert-006
      --optional-cert-secrets=user-serving-cert-007
      --optional-cert-secrets=user-serving-cert-008
      --optional-cert-secrets=user-serving-cert-009
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:09:45 -0400
      Finished:     Thu, 14 May 2020 07:09:51 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-7-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason                       Age   From                                Message
  ----    ------                       ----  ----                                -------
  Normal  Pulled                       20m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal  Created                      20m   kubelet, dc1master01.os4.ringen.us  Created container installer
  Normal  Started                      20m   kubelet, dc1master01.os4.ringen.us  Started container installer
  Normal  StaticPodInstallerCompleted  20m   static-pod-installer                Successfully installed revision 7


Name:                 kube-apiserver-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:39 -0400
Labels:               apiserver=true
                      app=openshift-kube-apiserver
                      revision=7
Annotations:          kubernetes.io/config.hash: 161ee5d1aad2404f2526197a029af979
                      kubernetes.io/config.mirror: 161ee5d1aad2404f2526197a029af979
                      kubernetes.io/config.seen: 2020-05-14T11:09:50.828257811Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:  192.168.99.41
Init Containers:
  setup:
    Container ID:  cri-o://cc869670a2bbd8ec917bcc5ff74b1896f3406ce748acbd68c691bf2deac382ec
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/timeout
      105
      /bin/bash
      -ec
    Args:
      echo -n "Fixing audit permissions."
      chmod 0700 /var/log/kube-apiserver
      echo -n "Waiting for port :6443 and :6080 to be released."
      while [ -n "$(lsof -ni :6443)$(lsof -ni :6080)" ]; do
        echo -n "."
        sleep 1
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:09:52 -0400
      Finished:     Thu, 14 May 2020 07:09:53 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/kube-apiserver from audit-dir (rw)
Containers:
  kube-apiserver-7:
    Container ID:  cri-o://b362ea38f44ef4962a1490c3366a30365225b096cf457c259c5246105349cc2f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          6443/TCP
    Host Port:     6443/TCP
    Command:
      /bin/bash
      -ec
    Args:
      if [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then
        echo "Copying system trust bundle"
        cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec hyperkube kube-apiserver --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml --advertise-address=${HOST_IP} -v=2
    State:       Running
      Started:   Thu, 14 May 2020 07:11:02 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   g   Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.)
      --service-cluster-ip-range string           A CIDR notation IP range from which to assign service cluster IPs. This must not overlap with any IP ranges assigned to nodes for pods. (default "10.0.0.0/24")
      --service-node-port-range portRange         A port range to reserve for services with NodePort visibility. Example: '30000-32767'. Inclusive at both ends of the range. (default 30000-32767)

Global flags:

      --add-dir-header                   If true, adds the file directory to the header
      --alsologtostderr                  log to standard error as well as files
  -h, --help                             help for kube-apiserver
      --log-backtrace-at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log-dir string                   If non-empty, write log files in this directory
      --log-file string                  If non-empty, use this log file
      --log-file-max-size uint           Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
      --log-flush-frequency duration     Maximum number of seconds between log flushes (default 5s)
      --logtostderr                      log to standard error instead of files (default true)
      --skip-headers                     If true, avoid header prefixes in the log messages
      --skip-log-headers                 If true, avoid headers when opening log files
      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)
  -v, --v Level                          number for the log level verbosity (default 0)
      --version version[=true]           Print version information and quit
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging


      Exit Code:    1
      Started:      Thu, 14 May 2020 07:09:58 -0400
      Finished:     Thu, 14 May 2020 07:09:58 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      150m
      memory:   1Gi
    Liveness:   http-get https://:6443/healthz delay=45s timeout=10s period=10s #success=1 #failure=3
    Readiness:  http-get https://:6443/healthz delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:
      POD_NAME:            kube-apiserver-dc1master01.os4.ringen.us (v1:metadata.name)
      POD_NAMESPACE:       openshift-kube-apiserver (v1:metadata.namespace)
      STATIC_POD_VERSION:  7
      HOST_IP:              (v1:status.hostIP)
    Mounts:
      /etc/kubernetes/static-pod-certs from cert-dir (rw)
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
      /var/log/kube-apiserver from audit-dir (rw)
  kube-apiserver-cert-syncer-7:
    Container ID:  cri-o://edb808a4776055cb3a794723111c0d8fba5a0bfbb82230b65f0809027dd7c2ee
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      cert-syncer
    Args:
      --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig
      --namespace=$(POD_NAMESPACE)
      --destination-dir=/etc/kubernetes/static-pod-certs
      --tls-server-name-override=localhost-recovery
    State:          Running
      Started:      Thu, 14 May 2020 07:09:59 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-apiserver-dc1master01.os4.ringen.us (v1:metadata.name)
      POD_NAMESPACE:  openshift-kube-apiserver (v1:metadata.namespace)
    Mounts:
      /etc/kubernetes/static-pod-certs from cert-dir (rw)
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
  kube-apiserver-insecure-readyz-7:
    Container ID:  cri-o://fe8fe479e72a530277d7d55d7688c07504e5632b3f6dd1646650804719788730
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          6080/TCP
    Host Port:     6080/TCP
    Command:
      cluster-kube-apiserver-operator
      insecure-readyz
    Args:
      --insecure-port=6080
      --delegate-url=https://localhost:6443/readyz
    State:          Running
      Started:      Thu, 14 May 2020 07:09:59 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Environment:  <none>
    Mounts:       <none>
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  resource-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/kube-apiserver-pod-7
    HostPathType:  
  cert-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/kube-apiserver-certs
    HostPathType:  
  audit-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/kube-apiserver
    HostPathType:  
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       
Events:
  Type     Reason     Age                From                                Message
  ----     ------     ----               ----                                -------
  Normal   Pulled     20m                kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine
  Normal   Created    20m                kubelet, dc1master01.os4.ringen.us  Created container setup
  Normal   Started    20m                kubelet, dc1master01.os4.ringen.us  Started container setup
  Normal   Created    20m                kubelet, dc1master01.os4.ringen.us  Created container kube-apiserver-cert-syncer-7
  Normal   Pulled     20m                kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal   Started    20m                kubelet, dc1master01.os4.ringen.us  Started container kube-apiserver-insecure-readyz-7
  Normal   Started    20m                kubelet, dc1master01.os4.ringen.us  Started container kube-apiserver-cert-syncer-7
  Normal   Pulled     20m                kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal   Created    20m                kubelet, dc1master01.os4.ringen.us  Created container kube-apiserver-insecure-readyz-7
  Normal   Created    19m (x2 over 20m)  kubelet, dc1master01.os4.ringen.us  Created container kube-apiserver-7
  Normal   Pulled     19m (x2 over 20m)  kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine
  Normal   Started    19m (x2 over 20m)  kubelet, dc1master01.os4.ringen.us  Started container kube-apiserver-7
  Warning  Unhealthy  19m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 revision-pruner-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:36:18 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.30
IPs:
  IP:  10.241.0.30
Containers:
  pruner:
    Container ID:  cri-o://bc923be0cb82876caef1b0f9be8f0423ad4d16c204c0d0caae838d84abac8e82
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=3
      --protected-revisions=1,2,3
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:36:21 -0400
      Finished:     Wed, 13 May 2020 12:36:21 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:05 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.58
IPs:
  IP:  10.241.0.58
Containers:
  pruner:
    Container ID:  cri-o://4542efb69907bbd7c500552c924f8c52567368f4bae6fa858e2c915193ed3030
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=3
      --protected-revisions=1,2,3
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:33 -0400
      Finished:     Wed, 13 May 2020 12:40:38 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:41 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.66
IPs:
  IP:  10.241.0.66
Containers:
  pruner:
    Container ID:  cri-o://1bce4680a64699b95eb85bde953ffd5e92429397c840f4c032fb80c88ad66db6
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=5
      --protected-revisions=1,2,3,4,5
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:42:45 -0400
      Finished:     Wed, 13 May 2020 12:42:45 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-5-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:44:56 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.78
IPs:
  IP:  10.241.0.78
Containers:
  pruner:
    Container ID:  cri-o://6aa80838a12a4302ac337414c5d6ffba6b44eaf84cfbf232a1ef2a5051acf256
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=5
      --protected-revisions=1,2,3,4,5
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:45:00 -0400
      Finished:     Wed, 13 May 2020 12:45:01 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-6-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:02:41 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.100
IPs:
  IP:  10.241.0.100
Containers:
  pruner:
    Container ID:  cri-o://6f7d23e0179308dee08806d647d90dc237dd07cff464828e7b190920b0b6d18c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=6
      --protected-revisions=1,2,3,4,5,6
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:02:44 -0400
      Finished:     Thu, 14 May 2020 07:02:44 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason   Age   From                                Message
  ----    ------   ----  ----                                -------
  Normal  Pulled   27m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal  Created  27m   kubelet, dc1master01.os4.ringen.us  Created container pruner
  Normal  Started  27m   kubelet, dc1master01.os4.ringen.us  Started container pruner


Name:                 revision-pruner-7-dc1master01.os4.ringen.us
Namespace:            openshift-kube-apiserver
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:11:29 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.107
IPs:
  IP:  10.241.0.107
Containers:
  pruner:
    Container ID:  cri-o://4ec79d359422caddcb9e6fe69a69879e1730f3724e1d5da4e52ca2aea24a7734
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-apiserver-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=7
      --protected-revisions=1,2,3,4,5,6,7
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-apiserver-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:11:33 -0400
      Finished:     Thu, 14 May 2020 07:11:34 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-lkdsw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-lkdsw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-lkdsw
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason   Age   From                                Message
  ----    ------   ----  ----                                -------
  Normal  Pulled   18m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68617f9e5e56a36b4b6404cc0d4973d6cea7e06729a78d910b5a6ca9fc69ca83" already present on machine
  Normal  Created  18m   kubelet, dc1master01.os4.ringen.us  Created container pruner
  Normal  Started  18m   kubelet, dc1master01.os4.ringen.us  Started container pruner


Name:                 kube-controller-manager-operator-555dccf574-dhcmm
Namespace:            openshift-kube-controller-manager-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=kube-controller-manager-operator
                      pod-template-hash=555dccf574
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.7"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.7
IPs:
  IP:           10.241.0.7
Controlled By:  ReplicaSet/kube-controller-manager-operator-555dccf574
Containers:
  kube-controller-manager-operator:
    Container ID:  cri-o://7845c570ff283128c191a9e89bf72a3d7e032572fc798b8050894f1a2fca3006
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-kube-controller-manager-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
    State:       Running
      Started:   Wed, 13 May 2020 12:38:40 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   .0.1:443: connect: connection refused
E0513 16:38:10.659786       1 reflector.go:280] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Failed to watch *v1.ClusterOperator: Get https://172.30.0.1:443/apis/config.openshift.io/v1/clusteroperators?allowWatchBookmarks=true&resourceVersion=8266&timeout=5m37s&timeoutSeconds=337&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.662171       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: Get https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/secrets?allowWatchBookmarks=true&resourceVersion=7558&timeout=5m56s&timeoutSeconds=356&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.663241       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ServiceAccount: Get https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts?allowWatchBookmarks=true&resourceVersion=8124&timeout=8m14s&timeoutSeconds=494&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:21.456894       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: Get https://172.30.0.1:443/api/v1/nodes?allowWatchBookmarks=true&resourceVersion=6504&timeout=9m19s&timeoutSeconds=559&watch=true: net/http: TLS handshake timeout
E0513 16:38:21.474239       1 reflector.go:280] github.com/openshift/client-go/config/informers/externalversions/factory.go:101: Failed to watch *v1.Infrastructure: Get https://172.30.0.1:443/apis/config.openshift.io/v1/infrastructures?allowWatchBookmarks=true&resourceVersion=5009&timeout=5m3s&timeoutSeconds=303&watch=true: net/http: TLS handshake timeout
I0513 16:38:24.104459       1 leaderelection.go:287] failed to renew lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock: failed to tryAcquireOrRenew context deadline exceeded
F0513 16:38:24.105439       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:32:43 -0400
      Finished:     Wed, 13 May 2020 12:38:24 -0400
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      IMAGE:                            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
      OPERATOR_IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
      CLUSTER_POLICY_CONTROLLER_IMAGE:  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b51a0653fb98d8729f65c27658cc9ea43dba8a7f36cc140934734495c5488fb1
      OPERATOR_IMAGE_VERSION:           4.3.13
      OPERAND_IMAGE_VERSION:            1.16.2
      POD_NAME:                         kube-controller-manager-operator-555dccf574-dhcmm (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-controller-manager-operator-token-ng98s (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-controller-manager-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-controller-manager-operator-config
    Optional:  false
  kube-controller-manager-operator-token-ng98s:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-controller-manager-operator-token-ng98s
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 installer-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:21 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.19
IPs:
  IP:  10.241.0.19
Containers:
  installer:
    Container ID:  cri-o://e579fa94133ca73246f54ac82c555458951282a9c765c278729defdff4f9dff0
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      installer
    Args:
      -v=2
      --revision=2
      --namespace=openshift-kube-controller-manager
      --pod=kube-controller-manager-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-controller-manager-pod
      --configmaps=config
      --configmaps=cluster-policy-controller-config
      --configmaps=controller-manager-kubeconfig
      --optional-configmaps=cloud-config
      --configmaps=kube-controller-cert-syncer-kubeconfig
      --configmaps=serviceaccount-ca
      --configmaps=service-ca
      --secrets=csr-signer
      --secrets=kube-controller-manager-client-cert-key
      --secrets=service-account-private-key
      --optional-secrets=serving-cert
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=csr-signer
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:32:23 -0400
      Finished:     Wed, 13 May 2020 12:32:26 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-2-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:40 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.22
IPs:
  IP:  10.241.0.22
Containers:
  installer:
    Container ID:  cri-o://b36bfbe99724d52b27c265510819c2b643824d812cbb32d3a30cb95bdbd395dc
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      installer
    Args:
      -v=2
      --revision=3
      --namespace=openshift-kube-controller-manager
      --pod=kube-controller-manager-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-controller-manager-pod
      --configmaps=config
      --configmaps=cluster-policy-controller-config
      --configmaps=controller-manager-kubeconfig
      --optional-configmaps=cloud-config
      --configmaps=kube-controller-cert-syncer-kubeconfig
      --configmaps=serviceaccount-ca
      --configmaps=service-ca
      --secrets=csr-signer
      --secrets=kube-controller-manager-client-cert-key
      --secrets=service-account-private-key
      --optional-secrets=serving-cert
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=csr-signer
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:32:45 -0400
      Finished:     Wed, 13 May 2020 12:32:47 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-3-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:14 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.59
IPs:
  IP:  10.241.0.59
Containers:
  installer:
    Container ID:  cri-o://b8d9cd7be34e4dbfea418132fa3cc53b49d4909e061cf74b37b4472092d51fb0
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      installer
    Args:
      -v=2
      --revision=4
      --namespace=openshift-kube-controller-manager
      --pod=kube-controller-manager-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-controller-manager-pod
      --configmaps=config
      --configmaps=cluster-policy-controller-config
      --configmaps=controller-manager-kubeconfig
      --optional-configmaps=cloud-config
      --configmaps=kube-controller-cert-syncer-kubeconfig
      --configmaps=serviceaccount-ca
      --configmaps=service-ca
      --secrets=csr-signer
      --secrets=kube-controller-manager-client-cert-key
      --secrets=service-account-private-key
      --optional-secrets=serving-cert
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=csr-signer
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:36 -0400
      Finished:     Wed, 13 May 2020 12:40:39 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-4-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-5-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:47 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.63
IPs:
  IP:  10.241.0.63
Containers:
  installer:
    Container ID:  cri-o://83d363d63c9225ee1a85fb500708f87b50c83aa53d8e72e14aa04ca7f6501190
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      installer
    Args:
      -v=2
      --revision=5
      --namespace=openshift-kube-controller-manager
      --pod=kube-controller-manager-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-controller-manager-pod
      --configmaps=config
      --configmaps=cluster-policy-controller-config
      --configmaps=controller-manager-kubeconfig
      --optional-configmaps=cloud-config
      --configmaps=kube-controller-cert-syncer-kubeconfig
      --configmaps=serviceaccount-ca
      --configmaps=service-ca
      --secrets=csr-signer
      --secrets=kube-controller-manager-client-cert-key
      --secrets=service-account-private-key
      --optional-secrets=serving-cert
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=csr-signer
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:51 -0400
      Finished:     Wed, 13 May 2020 12:40:53 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-5-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-6-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:09:22 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.104
IPs:
  IP:  10.241.0.104
Containers:
  installer:
    Container ID:  cri-o://cc20dd5fe148b42a2843ca08d41ecc2fa5fc245bda54768d284b88a152f34688
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      installer
    Args:
      -v=2
      --revision=6
      --namespace=openshift-kube-controller-manager
      --pod=kube-controller-manager-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-controller-manager-pod
      --configmaps=config
      --configmaps=cluster-policy-controller-config
      --configmaps=controller-manager-kubeconfig
      --optional-configmaps=cloud-config
      --configmaps=kube-controller-cert-syncer-kubeconfig
      --configmaps=serviceaccount-ca
      --configmaps=service-ca
      --secrets=csr-signer
      --secrets=kube-controller-manager-client-cert-key
      --secrets=service-account-private-key
      --optional-secrets=serving-cert
      --cert-dir=/etc/kubernetes/static-pod-resources/kube-controller-manager-certs
      --cert-configmaps=aggregator-client-ca
      --cert-configmaps=client-ca
      --optional-cert-configmaps=trusted-ca-bundle
      --cert-secrets=csr-signer
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:09:25 -0400
      Finished:     Thu, 14 May 2020 07:09:27 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-6-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason                       Age   From                                Message
  ----    ------                       ----  ----                                -------
  Normal  Pulled                       20m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119" already present on machine
  Normal  Created                      20m   kubelet, dc1master01.os4.ringen.us  Created container installer
  Normal  Started                      20m   kubelet, dc1master01.os4.ringen.us  Started container installer
  Normal  StaticPodInstallerCompleted  20m   static-pod-installer                Successfully installed revision 6


Name:                 kube-controller-manager-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:25 -0400
Labels:               app=kube-controller-manager
                      kube-controller-manager=true
                      revision=6
Annotations:          kubernetes.io/config.hash: 77882212c7c1d9b5c643ca6cd9339c28
                      kubernetes.io/config.mirror: 77882212c7c1d9b5c643ca6cd9339c28
                      kubernetes.io/config.seen: 2020-05-14T11:09:26.976678453Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:  192.168.99.41
Init Containers:
  wait-for-host-port:
    Container ID:  cri-o://6d8fc2994b7ee45ae24d59c86e433a0c81fb3a9b92ef214e82fe9666c01ade7e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/timeout
      30
      /bin/bash
      -c
    Args:
      echo -n "Waiting for port :10257 to be released."
      while [ -n "$(lsof -ni :10257)" ]; do
        echo -n "."
        sleep 1
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:09:28 -0400
      Finished:     Thu, 14 May 2020 07:09:28 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:         <none>
  wait-for-cpc-host-port:
    Container ID:  cri-o://d0107993893745aae5c575f6ee92496bda96f38d2ed5f805e8cbc184876c2187
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/timeout
      30
      /bin/bash
      -c
    Args:
      echo -n "Waiting for port :10357 to be released."
      while [ -n "$(lsof -ni :10357)" ]; do
        echo -n "."
        sleep 1
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:09:58 -0400
      Finished:     Thu, 14 May 2020 07:09:59 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:         <none>
Containers:
  kube-controller-manager-6:
    Container ID:  cri-o://35e05c195c3ae5c19fc1b24e90b5a6a010efe81e6ba2e99b2db51a0d1e5ec5f1
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          10257/TCP
    Host Port:     10257/TCP
    Command:
      /bin/bash
      -ec
    Args:
      if [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then
        echo "Copying system trust bundle"
        cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
      fi
      exec hyperkube kube-controller-manager --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml \
        --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --authentication-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --authorization-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig \
        --client-ca-file=/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt \
        --requestheader-client-ca-file=/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt -v=2 --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key
    State:          Running
      Started:      Thu, 14 May 2020 07:11:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:10257/healthz delay=45s timeout=10s period=10s #success=1 #failure=3
    Readiness:    http-get https://:10257/healthz delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/kubernetes/static-pod-certs from cert-dir (rw)
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
  cluster-policy-controller-6:
    Container ID:  cri-o://90808de2374b49e406ca0ed4d1a5fa0ab2f8c14ea852a7b338d540148558c425
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b51a0653fb98d8729f65c27658cc9ea43dba8a7f36cc140934734495c5488fb1
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b51a0653fb98d8729f65c27658cc9ea43dba8a7f36cc140934734495c5488fb1
    Port:          10357/TCP
    Host Port:     10357/TCP
    Command:
      cluster-policy-controller
      start
    Args:
      --config=/etc/kubernetes/static-pod-resources/configmaps/cluster-policy-controller-config/config.yaml
    State:          Running
      Started:      Thu, 14 May 2020 07:11:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:10357/healthz delay=45s timeout=10s period=10s #success=1 #failure=3
    Readiness:    http-get https://:10357/healthz delay=10s timeout=10s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/kubernetes/static-pod-certs from cert-dir (rw)
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
  kube-controller-manager-cert-syncer-6:
    Container ID:  cri-o://54abe617104a59c9780990ee0d0fe7486c184ff2d71a6d5ed6d51c7ffd0cbc7f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      cert-syncer
    Args:
      --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-controller-cert-syncer-kubeconfig/kubeconfig
      --namespace=$(POD_NAMESPACE)
      --destination-dir=/etc/kubernetes/static-pod-certs
    State:          Running
      Started:      Thu, 14 May 2020 07:11:03 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-controller-manager-dc1master01.os4.ringen.us (v1:metadata.name)
      POD_NAMESPACE:  openshift-kube-controller-manager (v1:metadata.namespace)
    Mounts:
      /etc/kubernetes/static-pod-certs from cert-dir (rw)
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  resource-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6
    HostPathType:  
  cert-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/kube-controller-manager-certs
    HostPathType:  
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       
Events:
  Type    Reason   Age   From                                Message
  ----    ------   ----  ----                                -------
  Normal  Pulled   20m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine
  Normal  Created  20m   kubelet, dc1master01.os4.ringen.us  Created container wait-for-host-port
  Normal  Started  20m   kubelet, dc1master01.os4.ringen.us  Started container wait-for-host-port
  Normal  Pulled   20m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine
  Normal  Created  20m   kubelet, dc1master01.os4.ringen.us  Created container wait-for-cpc-host-port
  Normal  Started  20m   kubelet, dc1master01.os4.ringen.us  Started container wait-for-cpc-host-port
  Normal  Created  19m   kubelet, dc1master01.os4.ringen.us  Created container kube-controller-manager-6
  Normal  Pulled   19m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine
  Normal  Started  19m   kubelet, dc1master01.os4.ringen.us  Started container kube-controller-manager-6
  Normal  Pulled   19m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b51a0653fb98d8729f65c27658cc9ea43dba8a7f36cc140934734495c5488fb1" already present on machine
  Normal  Created  19m   kubelet, dc1master01.os4.ringen.us  Created container cluster-policy-controller-6
  Normal  Started  19m   kubelet, dc1master01.os4.ringen.us  Started container cluster-policy-controller-6
  Normal  Pulled   19m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119" already present on machine
  Normal  Created  19m   kubelet, dc1master01.os4.ringen.us  Created container kube-controller-manager-cert-syncer-6
  Normal  Started  19m   kubelet, dc1master01.os4.ringen.us  Started container kube-controller-manager-cert-syncer-6


Name:                 revision-pruner-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:39 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.23
IPs:
  IP:  10.241.0.23
Containers:
  pruner:
    Container ID:  cri-o://4b61d241a4327ed5476e183580bdcd22d4be8e3b2f34bc49fbdcefe17f187d1e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=3
      --protected-revisions=1,2,3
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-controller-manager-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:32:46 -0400
      Finished:     Wed, 13 May 2020 12:32:46 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:34:05 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.24
IPs:
  IP:  10.241.0.24
Containers:
  pruner:
    Container ID:  cri-o://cd5462445917a193afbec571d29b8fbcde01a80f7e05e1c83f9471e0c15b9b75
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=3
      --protected-revisions=1,2,3
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-controller-manager-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:34:08 -0400
      Finished:     Wed, 13 May 2020 12:34:08 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:44 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.62"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Succeeded
IP:                   10.241.0.62
IPs:
  IP:  10.241.0.62
Containers:
  pruner:
    Container ID:  cri-o://e836b4803b9d9fe127f5ee26df23a8519c1e90cfae45345ad278bbea9bb1622b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=5
      --protected-revisions=1,2,3,4,5
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-controller-manager-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:50 -0400
      Finished:     Wed, 13 May 2020 12:40:50 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-5-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:37 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.65
IPs:
  IP:  10.241.0.65
Containers:
  pruner:
    Container ID:  cri-o://05b5521d6ee85eb185778f1f0a4e77fb7c6f56e68301031e5c1fff8113d56e44
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=5
      --protected-revisions=1,2,3,4,5
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-controller-manager-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:42:42 -0400
      Finished:     Wed, 13 May 2020 12:42:42 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-6-dc1master01.os4.ringen.us
Namespace:            openshift-kube-controller-manager
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:11:19 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.106
IPs:
  IP:  10.241.0.106
Containers:
  pruner:
    Container ID:  cri-o://6edc89c595cdd71b291ca12f4a653ef1a1e260b05c5b5067fdb2022db75f96aa
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-controller-manager-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=6
      --protected-revisions=1,2,3,4,5,6
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-controller-manager-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 14 May 2020 07:11:23 -0400
      Finished:     Thu, 14 May 2020 07:11:23 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-hbqwc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-hbqwc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-hbqwc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:
  Type    Reason   Age   From                                Message
  ----    ------   ----  ----                                -------
  Normal  Pulled   19m   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c3a1891a2c22da8c5665380f37bd7d3962432725c4ca7f480a34126a83ad119" already present on machine
  Normal  Created  19m   kubelet, dc1master01.os4.ringen.us  Created container pruner
  Normal  Started  19m   kubelet, dc1master01.os4.ringen.us  Started container pruner


Name:                 openshift-kube-scheduler-operator-f8cd66ff5-n996h
Namespace:            openshift-kube-scheduler-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=openshift-kube-scheduler-operator
                      pod-template-hash=f8cd66ff5
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.2"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.2
IPs:
  IP:           10.241.0.2
Controlled By:  ReplicaSet/openshift-kube-scheduler-operator-f8cd66ff5
Containers:
  kube-scheduler-operator-container:
    Container ID:  cri-o://3bc005fb677a9ac56caaec5611ffd92366593b0998fdd35c1c72b1fd81e102c8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=2
    State:       Running
      Started:   Wed, 13 May 2020 12:38:44 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   onnection refused
E0513 16:38:10.687786       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: Get https://172.30.0.1:443/api/v1/namespaces/openshift-config/secrets?allowWatchBookmarks=true&resourceVersion=7652&timeout=5m57s&timeoutSeconds=357&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.690226       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps?allowWatchBookmarks=true&resourceVersion=8249&timeout=7m30s&timeoutSeconds=450&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.690324       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: Get https://172.30.0.1:443/api/v1/namespaces/openshift-config/configmaps?allowWatchBookmarks=true&resourceVersion=5051&timeout=9m9s&timeoutSeconds=549&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.701712       1 reflector.go:280] k8s.io/client-go/dynamic/dynamicinformer/informer.go:90: Failed to watch *unstructured.Unstructured: Get https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers?allowWatchBookmarks=true&resourceVersion=5439&timeoutSeconds=401&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
E0513 16:38:10.701967       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RoleBinding: Get https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings?allowWatchBookmarks=true&resourceVersion=6069&timeout=8m25s&timeoutSeconds=505&watch=true: dial tcp 172.30.0.1:443: connect: connection refused
I0513 16:38:23.540652       1 leaderelection.go:287] failed to renew lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: failed to tryAcquireOrRenew context deadline exceeded
F0513 16:38:23.541060       1 leaderelection.go:66] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:33:19 -0400
      Finished:     Wed, 13 May 2020 12:38:23 -0400
    Ready:          True
    Restart Count:  2
    Requests:
      memory:  50Mi
    Environment:
      IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
      OPERATOR_IMAGE:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
      OPERATOR_IMAGE_VERSION:  4.3.13
      OPERAND_IMAGE_VERSION:   1.16.2
      POD_NAME:                openshift-kube-scheduler-operator-f8cd66ff5-n996h (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-kube-scheduler-operator-token-j2vxq (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-scheduler-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      openshift-kube-scheduler-operator-config
    Optional:  false
  openshift-kube-scheduler-operator-token-j2vxq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-kube-scheduler-operator-token-j2vxq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 installer-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:58 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.16
IPs:
  IP:  10.241.0.16
Containers:
  installer:
    Container ID:  cri-o://a7dd4873ac7d39e14fe1e70ec008e14f08cefc051ffea79807ca4d1bc51695c2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      installer
    Args:
      -v=2
      --revision=2
      --namespace=openshift-kube-scheduler
      --pod=kube-scheduler-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-scheduler-pod
      --configmaps=config
      --configmaps=scheduler-kubeconfig
      --configmaps=serviceaccount-ca
      --optional-configmaps=policy-configmap
      --secrets=kube-scheduler-client-cert-key
      --optional-secrets=serving-cert
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:32:01 -0400
      Finished:     Wed, 13 May 2020 12:32:01 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-2-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:34:53 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.26
IPs:
  IP:  10.241.0.26
Containers:
  installer:
    Container ID:  cri-o://78a142d069170e218e8515711c56bffc9d95803ef6755506f8c2c820a1f977be
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      installer
    Args:
      -v=2
      --revision=3
      --namespace=openshift-kube-scheduler
      --pod=kube-scheduler-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-scheduler-pod
      --configmaps=config
      --configmaps=scheduler-kubeconfig
      --configmaps=serviceaccount-ca
      --optional-configmaps=policy-configmap
      --secrets=kube-scheduler-client-cert-key
      --optional-secrets=serving-cert
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:34:56 -0400
      Finished:     Wed, 13 May 2020 12:34:56 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-3-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 installer-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:40:06 -0400
Labels:               app=installer
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.57
IPs:
  IP:  10.241.0.57
Containers:
  installer:
    Container ID:  cri-o://63677a8f83b47c1eedf1ec67d527985bf98cb31a1141f4de8145cab5f9097184
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      installer
    Args:
      -v=2
      --revision=4
      --namespace=openshift-kube-scheduler
      --pod=kube-scheduler-pod
      --resource-dir=/etc/kubernetes/static-pod-resources
      --pod-manifest-dir=/etc/kubernetes/manifests
      --configmaps=kube-scheduler-pod
      --configmaps=config
      --configmaps=scheduler-kubeconfig
      --configmaps=serviceaccount-ca
      --optional-configmaps=policy-configmap
      --secrets=kube-scheduler-client-cert-key
      --optional-secrets=serving-cert
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:27 -0400
      Finished:     Wed, 13 May 2020 12:40:29 -0400
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  100M
    Requests:
      memory:  100M
    Environment:
      POD_NAME:  installer-4-dc1master01.os4.ringen.us (v1:metadata.name)
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 openshift-kube-scheduler-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:01 -0400
Labels:               app=openshift-kube-scheduler
                      revision=4
                      scheduler=true
Annotations:          kubernetes.io/config.hash: 0636c4089798f78c5034ccad70d741c0
                      kubernetes.io/config.mirror: 0636c4089798f78c5034ccad70d741c0
                      kubernetes.io/config.seen: 2020-05-13T16:40:29.298493523Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:  192.168.99.41
Init Containers:
  wait-for-host-port:
    Container ID:  cri-o://eaddd0707c021faf2b31b7f92e0c95dff178832a5699cbf6f874c3d411deec67
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/timeout
      30
      /bin/bash
      -c
    Args:
      echo -n "Waiting for port :10259 and :10251 to be released."
      while [ -n "$(lsof -ni :10251)" -o -n "$(lsof -i :10259)" ]; do
        echo -n "."
        sleep 1
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:45 -0400
      Finished:     Wed, 13 May 2020 12:40:48 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:         <none>
Containers:
  scheduler:
    Container ID:  cri-o://56b8549cbf269c4851c799c17b363893d022b2f91b18e8c195e619c01f9854d8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9
    Port:          10259/TCP
    Host Port:     10259/TCP
    Command:
      hyperkube
      kube-scheduler
    Args:
      --config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml
      --cert-dir=/var/run/kubernetes
      --port=0
      --authentication-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig
      --authorization-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig
      --feature-gates=LegacyNodeRoleBehavior=false,NodeDisruptionExclusion=true,RotateKubeletServerCertificate=true,SCTPSupport=true,ServiceNodeExclusion=true,SupportPodPidsLimit=true
      -v=2
      --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt
      --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key
    State:       Running
      Started:   Thu, 14 May 2020 07:11:29 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   .Node: Get https://localhost:6443/api/v1/nodes?allowWatchBookmarks=true&resourceVersion=259023&timeout=9m33s&timeoutSeconds=573&watch=true: dial tcp [::1]:6443: connect: connection refused
E0514 11:11:02.053519       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: Get https://localhost:6443/api/v1/services?allowWatchBookmarks=true&resourceVersion=258972&timeout=8m24s&timeoutSeconds=504&watch=true: dial tcp [::1]:6443: connect: connection refused
E0514 11:11:02.053655       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: Get https://localhost:6443/apis/storage.k8s.io/v1/storageclasses?allowWatchBookmarks=true&resourceVersion=256928&timeout=9m53s&timeoutSeconds=593&watch=true: dial tcp [::1]:6443: connect: connection refused
E0514 11:11:02.055020       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: Get https://localhost:6443/api/v1/persistentvolumes?allowWatchBookmarks=true&resourceVersion=256928&timeout=5m22s&timeoutSeconds=322&watch=true: dial tcp [::1]:6443: connect: connection refused
E0514 11:11:02.055020       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: Get https://localhost:6443/api/v1/persistentvolumeclaims?allowWatchBookmarks=true&resourceVersion=256928&timeout=7m38s&timeoutSeconds=458&watch=true: dial tcp [::1]:6443: connect: connection refused
E0514 11:11:02.055132       1 reflector.go:280] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSINode: Get https://localhost:6443/apis/storage.k8s.io/v1beta1/csinodes?allowWatchBookmarks=true&resourceVersion=256928&timeout=5m20s&timeoutSeconds=320&watch=true: dial tcp [::1]:6443: connect: connection refused
I0514 11:11:10.903907       1 leaderelection.go:287] failed to renew lease openshift-kube-scheduler/kube-scheduler: failed to tryAcquireOrRenew context deadline exceeded
F0514 11:11:10.903937       1 server.go:264] leaderelection lost

      Exit Code:    255
      Started:      Thu, 14 May 2020 07:02:25 -0400
      Finished:     Thu, 14 May 2020 07:11:11 -0400
    Ready:          True
    Restart Count:  3
    Requests:
      memory:     50Mi
    Liveness:     http-get http://:10251/healthz delay=45s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get http://:10251/healthz delay=45s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/kubernetes/static-pod-resources from resource-dir (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  resource-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/kube-scheduler-pod-4
    HostPathType:  
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://192.168.99.41:10251/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://192.168.99.41:10251/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://192.168.99.41:10251/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  BackOff    19m (x2 over 19m)   kubelet, dc1master01.os4.ringen.us  Back-off restarting failed container
  Normal   Created    18m (x4 over 18h)   kubelet, dc1master01.os4.ringen.us  Created container scheduler
  Normal   Started    18m (x4 over 18h)   kubelet, dc1master01.os4.ringen.us  Started container scheduler
  Normal   Pulled     18m (x4 over 18h)   kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfe60e0c15ad29a19383452894986d4e0973bbb3da3d039d5aaad11692fbaca9" already present on machine


Name:                 revision-pruner-2-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:34:44 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.25
IPs:
  IP:  10.241.0.25
Containers:
  pruner:
    Container ID:  cri-o://fa7013a7db9ce1789507caedad684d3165b8651a63f2cd28c5e854873f27a653
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=2
      --protected-revisions=1,2
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-scheduler-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:34:46 -0400
      Finished:     Wed, 13 May 2020 12:34:46 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-3-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:35:52 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status: 
Status:               Succeeded
IP:                   10.241.0.28
IPs:
  IP:  10.241.0.28
Containers:
  pruner:
    Container ID:  cri-o://ddcfc0ee7638ade50a48e84f614710b5a92357db1ed779db1e479bf67baa3228
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=3
      --protected-revisions=1,2,3
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-scheduler-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:35:55 -0400
      Finished:     Wed, 13 May 2020 12:35:55 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 revision-pruner-4-dc1master01.os4.ringen.us
Namespace:            openshift-kube-scheduler
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:43:14 -0400
Labels:               app=pruner
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.77"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Succeeded
IP:                   10.241.0.77
IPs:
  IP:  10.241.0.77
Containers:
  pruner:
    Container ID:  cri-o://cfe17d0d0f1f6fde4f5237b050d45f161cdf5f94daa62d49e3f16ba89fd6e125
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:285e7166a5b5941cd3984c45e49a84e655bc489e99129eb1272b232584dc182f
    Port:          <none>
    Host Port:     <none>
    Command:
      cluster-kube-scheduler-operator
      prune
    Args:
      -v=4
      --max-eligible-revision=4
      --protected-revisions=1,2,3,4
      --resource-dir=/etc/kubernetes/static-pod-resources
      --static-pod-name=kube-scheduler-pod
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:43:28 -0400
      Finished:     Wed, 13 May 2020 12:43:29 -0400
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kubernetes/ from kubelet-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from installer-sa-token-jjzpc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubelet-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/
    HostPathType:  
  installer-sa-token-jjzpc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  installer-sa-token-jjzpc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
Events:          <none>


Name:                 cluster-autoscaler-operator-74b5d8858b-vjkpn
Namespace:            openshift-machine-api
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:42:47 -0400
Labels:               k8s-app=cluster-autoscaler-operator
                      pod-template-hash=74b5d8858b
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.67"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.67
IPs:
  IP:           10.241.0.67
Controlled By:  ReplicaSet/cluster-autoscaler-operator-74b5d8858b
Containers:
  kube-rbac-proxy:
    Container ID:  cri-o://ac4f0ff086b3e14320b6ac641e08d5846a5b5770dbab2a27cfbb6ac8565fb804
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9192/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:9192
      --upstream=http://127.0.0.1:9191/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --config-file=/etc/kube-rbac-proxy/config-file.yaml
      --logtostderr=true
      --v=10
    State:          Running
      Started:      Wed, 13 May 2020 12:42:57 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kube-rbac-proxy from auth-proxy-config (ro)
      /etc/tls/private from cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-autoscaler-operator-token-mjxf4 (ro)
  cluster-autoscaler-operator:
    Container ID:  cri-o://cdcc52d6fd92027c91c40f4263bbfbbc3e369b67bf34f207e11b9e4af4d68f39
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b0dd39db0d10ea3a892654ba97fa8a3a361dedb12765082b18692e592d9cf2
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b0dd39db0d10ea3a892654ba97fa8a3a361dedb12765082b18692e592d9cf2
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-autoscaler-operator
    Args:
      -alsologtostderr
    State:          Running
      Started:      Wed, 13 May 2020 12:43:25 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:               4.3.13
      WATCH_NAMESPACE:               openshift-machine-api (v1:metadata.namespace)
      CLUSTER_AUTOSCALER_NAMESPACE:  openshift-machine-api (v1:metadata.namespace)
      LEADER_ELECTION_NAMESPACE:     openshift-machine-api (v1:metadata.namespace)
      CLUSTER_AUTOSCALER_IMAGE:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:194a61a82a99550aa31c727d7607e190f6f8cefb3838a0f6afb04d5612f7cd7e
      WEBHOOKS_CERT_DIR:             /etc/cluster-autoscaler-operator/tls
      WEBHOOKS_PORT:                 8443
      METRICS_PORT:                  9191
    Mounts:
      /etc/cluster-autoscaler-operator/tls from cert (ro)
      /etc/cluster-autoscaler-operator/tls/service-ca from ca-cert (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-autoscaler-operator-token-mjxf4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-cert:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cluster-autoscaler-operator-ca
    Optional:  false
  cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-autoscaler-operator-cert
    Optional:    false
  auth-proxy-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-rbac-proxy-cluster-autoscaler-operator
    Optional:  false
  cluster-autoscaler-operator-token-mjxf4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-autoscaler-operator-token-mjxf4
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 machine-api-operator-7479d9b947-x2cx5
Namespace:            openshift-machine-api
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:21 -0400
Labels:               k8s-app=machine-api-operator
                      pod-template-hash=7479d9b947
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.15"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.15
IPs:
  IP:           10.241.0.15
Controlled By:  ReplicaSet/machine-api-operator-7479d9b947
Containers:
  kube-rbac-proxy:
    Container ID:  cri-o://2fdc4df301e7e532ba4c89dead604be8880f280b2161ab829970bd2e91cb69e6
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:8443
      --upstream=http://localhost:8080/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --config-file=/etc/kube-rbac-proxy/config-file.yaml
      --logtostderr=true
      --v=10
    State:          Running
      Started:      Wed, 13 May 2020 12:31:59 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/kube-rbac-proxy from config (rw)
      /etc/tls/private from machine-api-operator-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-api-operator-token-vs8vk (ro)
  machine-api-operator:
    Container ID:  cri-o://536ceca025b9f5d62cdaf6c94d6ceaccf3cfcaf987fc5c4a292d64c1ae292cf1
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7146e7ac9534e3429823a0c0c5bec909600ba441673de0336afc1c90e7c19dd6
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7146e7ac9534e3429823a0c0c5bec909600ba441673de0336afc1c90e7c19dd6
    Port:          <none>
    Host Port:     <none>
    Command:
      /machine-api-operator
    Args:
      start
      --images-json=/etc/machine-api-operator-config/images/images.json
      --alsologtostderr
      --v=3
    State:          Running
      Started:      Wed, 13 May 2020 12:32:07 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:      4.3.13
      COMPONENT_NAMESPACE:  openshift-machine-api (v1:metadata.namespace)
      METRICS_PORT:         8080
    Mounts:
      /etc/machine-api-operator-config/images from images (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-api-operator-token-vs8vk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-rbac-proxy
    Optional:  false
  images:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      machine-api-operator-images
    Optional:  false
  machine-api-operator-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-api-operator-tls
    Optional:    false
  machine-api-operator-token-vs8vk:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-api-operator-token-vs8vk
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 etcd-quorum-guard-c449964c-4rtkd
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=etcd-quorum-guard
                      name=etcd-quorum-guard
                      pod-template-hash=c449964c
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/etcd-quorum-guard-c449964c
Containers:
  guard:
    Image:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:       <none>
    Host Port:  <none>
    Command:
      /bin/bash
    Args:
      -c
      # properly handle TERM and exit as soon as it is signaled
      set -euo pipefail
      trap 'jobs -p | xargs -r kill; exit 0' TERM
      sleep infinity & wait
      
    Requests:
      cpu:      10m
      memory:   5Mi
    Readiness:  exec [/bin/sh -c declare -r croot=/mnt/kube
declare -r health_endpoint="https://127.0.0.1:2379/health"
declare -r cert="$(find $croot -name 'system:etcd-peer*.crt' -print -quit)"
declare -r key="${cert%.crt}.key"
declare -r cacert="$croot/ca.crt"
export NSS_SDB_USE_CACHE=no
[[ -z $cert || -z $key ]] && exit 1
curl --max-time 2 --silent --cert "${cert//:/\:}" --key "$key" --cacert "$cacert" "$health_endpoint" |grep '{ *"health" *: *"true" *}'
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /mnt/kube from kubecerts (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gq5vr (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kubecerts:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/etcd-member
    HostPathType:  
  default-token-gq5vr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-gq5vr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.


Name:                 etcd-quorum-guard-c449964c-fbng7
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=etcd-quorum-guard
                      name=etcd-quorum-guard
                      pod-template-hash=c449964c
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/etcd-quorum-guard-c449964c
Containers:
  guard:
    Image:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:       <none>
    Host Port:  <none>
    Command:
      /bin/bash
    Args:
      -c
      # properly handle TERM and exit as soon as it is signaled
      set -euo pipefail
      trap 'jobs -p | xargs -r kill; exit 0' TERM
      sleep infinity & wait
      
    Requests:
      cpu:      10m
      memory:   5Mi
    Readiness:  exec [/bin/sh -c declare -r croot=/mnt/kube
declare -r health_endpoint="https://127.0.0.1:2379/health"
declare -r cert="$(find $croot -name 'system:etcd-peer*.crt' -print -quit)"
declare -r key="${cert%.crt}.key"
declare -r cacert="$croot/ca.crt"
export NSS_SDB_USE_CACHE=no
[[ -z $cert || -z $key ]] && exit 1
curl --max-time 2 --silent --cert "${cert//:/\:}" --key "$key" --cacert "$cacert" "$health_endpoint" |grep '{ *"health" *: *"true" *}'
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /mnt/kube from kubecerts (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gq5vr (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kubecerts:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/etcd-member
    HostPathType:  
  default-token-gq5vr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-gq5vr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 2 node(s) didn't match node selector.


Name:                 etcd-quorum-guard-c449964c-wncml
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:34 -0400
Labels:               k8s-app=etcd-quorum-guard
                      name=etcd-quorum-guard
                      pod-template-hash=c449964c
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  ReplicaSet/etcd-quorum-guard-c449964c
Containers:
  guard:
    Container ID:  cri-o://ef05f1c2ab092a377206ba1b7f26f89501aba8af9aa4e22825637b3812bab90c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c10249372eac034506388608252329141c6559cd22606f8f1f4f01b511e3b051
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
    Args:
      -c
      # properly handle TERM and exit as soon as it is signaled
      set -euo pipefail
      trap 'jobs -p | xargs -r kill; exit 0' TERM
      sleep infinity & wait
      
    State:          Running
      Started:      Wed, 13 May 2020 12:32:35 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   5Mi
    Readiness:  exec [/bin/sh -c declare -r croot=/mnt/kube
declare -r health_endpoint="https://127.0.0.1:2379/health"
declare -r cert="$(find $croot -name 'system:etcd-peer*.crt' -print -quit)"
declare -r key="${cert%.crt}.key"
declare -r cacert="$croot/ca.crt"
export NSS_SDB_USE_CACHE=no
[[ -z $cert || -z $key ]] && exit 1
curl --max-time 2 --silent --cert "${cert//:/\:}" --key "$key" --cacert "$cacert" "$health_endpoint" |grep '{ *"health" *: *"true" *}'
] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /mnt/kube from kubecerts (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gq5vr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubecerts:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/static-pod-resources/etcd-member
    HostPathType:  
  default-token-gq5vr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-gq5vr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 machine-config-controller-59947d965-r4pb4
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:13 -0400
Labels:               k8s-app=machine-config-controller
                      pod-template-hash=59947d965
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.18"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.18
IPs:
  IP:           10.241.0.18
Controlled By:  ReplicaSet/machine-config-controller-59947d965
Containers:
  machine-config-controller:
    Container ID:  cri-o://27ee25d2fdf5100c73bd6690acbb915cea73f026bdcb56c06138384216c29222
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-config-controller
    Args:
      start
      --resourcelock-namespace=openshift-machine-config-operator
      --v=2
    State:          Running
      Started:      Wed, 13 May 2020 12:32:17 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        20m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-controller-token-znd49 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  machine-config-controller-token-znd49:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-controller-token-znd49
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 machine-config-daemon-jpxw5
Namespace:            openshift-machine-config-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:01:20 -0400
Labels:               controller-revision-hash=66bbffdc65
                      k8s-app=machine-config-daemon
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/machine-config-daemon
Containers:
  machine-config-daemon:
    Container ID:  cri-o://a8787770f4f0cb07bf053b9405ba10b7d47baabe87aff47da36631ad5ffcbbd5
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-config-daemon
    Args:
      start
    State:          Running
      Started:      Wed, 13 May 2020 13:01:21 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /rootfs from rootfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
  oauth-proxy:
    Container ID:  cri-o://0805e573966bb1fac18af9ee33e7f1cd8e1eae888ad7c9d6095b70fcbb263b88
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9001/TCP
    Host Port:     9001/TCP
    Args:
      --https-address=:9001
      --provider=openshift
      --openshift-service-account=machine-config-daemon
      --upstream=http://127.0.0.1:8797
      --tls-cert=/etc/tls/private/tls.crt
      --tls-key=/etc/tls/private/tls.key
      --cookie-secret-file=/etc/tls/cookie-secret/cookie-secret
      --openshift-sar={"resource": "namespaces", "verb": "get"}
      --openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
    State:          Running
      Started:      Wed, 13 May 2020 13:01:26 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        20m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /etc/tls/cookie-secret from cookie-secret (rw)
      /etc/tls/private from proxy-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  rootfs:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  proxy-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  proxy-tls
    Optional:    false
  cookie-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cookie-secret
    Optional:    false
  machine-config-daemon-token-ll42j:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-daemon-token-ll42j
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 machine-config-daemon-jx8ww
Namespace:            openshift-machine-config-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:30 -0400
Labels:               controller-revision-hash=66bbffdc65
                      k8s-app=machine-config-daemon
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/machine-config-daemon
Containers:
  machine-config-daemon:
    Container ID:  cri-o://cd84dae3ef0fbf48fff07461662c35c6a91c39ce03d29635e15bb98219b18d0a
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-config-daemon
    Args:
      start
    State:          Running
      Started:      Wed, 13 May 2020 12:32:06 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /rootfs from rootfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
  oauth-proxy:
    Container ID:  cri-o://4d2516d7191fdf4a1207abb2b65f9b2af95d1ecc45a25a3b5fd14e67a3746f9c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9001/TCP
    Host Port:     9001/TCP
    Args:
      --https-address=:9001
      --provider=openshift
      --openshift-service-account=machine-config-daemon
      --upstream=http://127.0.0.1:8797
      --tls-cert=/etc/tls/private/tls.crt
      --tls-key=/etc/tls/private/tls.key
      --cookie-secret-file=/etc/tls/cookie-secret/cookie-secret
      --openshift-sar={"resource": "namespaces", "verb": "get"}
      --openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
    State:          Running
      Started:      Wed, 13 May 2020 12:32:11 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        20m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /etc/tls/cookie-secret from cookie-secret (rw)
      /etc/tls/private from proxy-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  rootfs:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  proxy-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  proxy-tls
    Optional:    false
  cookie-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cookie-secret
    Optional:    false
  machine-config-daemon-token-ll42j:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-daemon-token-ll42j
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 machine-config-daemon-q8b69
Namespace:            openshift-machine-config-operator
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:01:30 -0400
Labels:               controller-revision-hash=66bbffdc65
                      k8s-app=machine-config-daemon
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/machine-config-daemon
Containers:
  machine-config-daemon:
    Container ID:  cri-o://c201cae796043b8c7109274b6566c1227c3b2e0140369edf3d03b8aa24a00480
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-config-daemon
    Args:
      start
    State:          Running
      Started:      Wed, 13 May 2020 13:01:30 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /rootfs from rootfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
  oauth-proxy:
    Container ID:  cri-o://106e3ba32d1301a721f2ebf70340dc35a9030904e59c7318fd35fdc1811887c9
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9001/TCP
    Host Port:     9001/TCP
    Args:
      --https-address=:9001
      --provider=openshift
      --openshift-service-account=machine-config-daemon
      --upstream=http://127.0.0.1:8797
      --tls-cert=/etc/tls/private/tls.crt
      --tls-key=/etc/tls/private/tls.key
      --cookie-secret-file=/etc/tls/cookie-secret/cookie-secret
      --openshift-sar={"resource": "namespaces", "verb": "get"}
      --openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
    State:          Running
      Started:      Wed, 13 May 2020 13:01:34 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        20m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /etc/tls/cookie-secret from cookie-secret (rw)
      /etc/tls/private from proxy-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-daemon-token-ll42j (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  rootfs:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  proxy-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  proxy-tls
    Optional:    false
  cookie-secret:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cookie-secret
    Optional:    false
  machine-config-daemon-token-ll42j:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-daemon-token-ll42j
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 machine-config-operator-86c8d6fdd6-frcpc
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               k8s-app=machine-config-operator
                      pod-template-hash=86c8d6fdd6
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.6"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.6
IPs:
  IP:           10.241.0.6
Controlled By:  ReplicaSet/machine-config-operator-86c8d6fdd6
Containers:
  machine-config-operator:
    Container ID:  cri-o://5d21b51631e7b5b18be2c1b9edd74df2779d1d291e8e7a66e28018a07a465c2e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Args:
      start
      --images-json=/etc/mco/images/images.json
    State:          Running
      Started:      Wed, 13 May 2020 12:31:24 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     20m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:  4.3.13
    Mounts:
      /etc/mco/images from images (rw)
      /etc/ssl/etcd/ca.crt from etcd-ca (rw)
      /etc/ssl/kubernetes/ca.crt from root-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gq5vr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  images:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      machine-config-operator-images
    Optional:  false
  etcd-ca:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/etcd/ca.crt
    HostPathType:  
  root-ca:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/ca.crt
    HostPathType:  
  default-token-gq5vr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-gq5vr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 machine-config-server-nnr47
Namespace:            openshift-machine-config-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:32:21 -0400
Labels:               controller-revision-hash=d4b67dcb8
                      k8s-app=machine-config-server
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/machine-config-server
Containers:
  machine-config-server:
    Container ID:  cri-o://04b130b1801108e5204537e4e350db1b1049be47959cd65c556acb4eab23a6fa
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0b798a0461129c133bb0492757680af714c12e9e2b8cbec904c13762f4ee50e4
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/machine-config-server
    Args:
      start
      --apiserver-url=https://api-int.os4.ringen.us:6443
    State:          Running
      Started:      Wed, 13 May 2020 12:32:22 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        20m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /etc/mcs/bootstrap-token from node-bootstrap-token (rw)
      /etc/ssl/mcs from certs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from machine-config-server-token-gflkq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  node-bootstrap-token:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-bootstrapper-token
    Optional:    false
  certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-server-tls
    Optional:    false
  machine-config-server-token-gflkq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  machine-config-server-token-gflkq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/etcd:NoSchedule
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:         certified-operators-d5b8986fb-bhhvr
Namespace:    openshift-marketplace
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:00 -0400
Labels:       marketplace.operatorSource=certified-operators
              pod-template-hash=d5b8986fb
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.93"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift-marketplace-update-hash: 160eb8597eca39bc
              openshift.io/scc: restricted
Status:       Running
IP:           10.241.0.93
IPs:
  IP:           10.241.0.93
Controlled By:  ReplicaSet/certified-operators-d5b8986fb
Containers:
  certified-operators:
    Container ID:  cri-o://e4c72bba253c277d6837daa8b2cde0c2e6d4854fdaccf0849a344b1faf766cca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|certified-operators
      -o
      couchbase-enterprise-certified,appranix-cps,storageos-10tb,joget-dx-operator,hazelcast-jet-enterprise-operator,joget-openshift-operator,cpx-cic-operator,traefikee-certified,gpu-operator-certified,ubix-operator,portshift-operator,ibm-helm-api-operator-app,sysdig-certified,perceptilabs-operator-package,k8s-triliovault,tidb-operator-certified,dotscience-operator,yugabyte-operator,redhat-marketplace-operator,portworx-certified,presto-operator,cortex-operator,newrelic-infrastructure,appdynamics-operator,twistlock-certified,percona-server-mongodb-operator-certified,couchdb-operator-certified,ibm-metering-operator-app,nginx-ingress-operator,citrix-cpx-istio-sidecar-injector-operator,cert-manager-operator,ibm-platform-api-operator-app,rocketchat-operator-certified,mongodb-enterprise,seldon-operator-certified,neuvector-certified-operator,alcide-kaudit-operator,cortex-hub-operator,instana-agent,open-liberty-certified,ibm-mongodb-operator-app,aqua-operator-certified,kubeturbo-marketplace-certified,anaconda-team-edition,eddi-operator-certified,insightedge-enterprise-operator2,traefikee-redhat-certified,federatorai-certified,wavefront-operator,rapidbiz-operator-certified,f5-bigip-ctlr-operator,cortex-healthcare-hub-operator,openunison-ocp-certified,kong,appsody-operator-certified,ocean-operator,redis-enterprise-operator-cert,ibm-helm-repo-operator-app,memql-certified,here-service-operator-certified,triggermesh-operator,ibm-spectrum-symphony-operator,hazelcast-enterprise-certified,kubeturbo-certified,cortex-certifai-operator,tigera-operator,storageos,aqua-certified,crunchy-postgres-operator,t8c-certified,robin-operator,nuodb-ce-certified,ivory-server-app,nxrm-operator-certified,open-enterprise-spinnaker,ibm-block-csi-operator,xcrypt-operator,datadog-operator-certified,ibm-management-ingress-operator-app,akka-cluster-operator-certified,runtime-component-operator-certified,ibm-monitoring-grafana-operator-app,zabbix-operator-certified,hpe-csi-operator,cockroachdb-certified,ibm-auditlogging-operator-app,nxiq-operator-certified,kubemq-operator-marketplace,planetscale-certified,sematext,transadv-operator,cih-operator-certified,infinibox-operator-certified,cortex-fabric-operator,atomicorp-helm-operator-certified,oneagent-certified,anchore-engine,kube-arangodb,storageos-1tb,uma-operator,timemachine-operator,orca,kubemq-operator,openshiftartifactoryha-operator,synopsys-certified,cic-operator,splunk-certified,citrix-adc-istio-ingress-gateway-operator,percona-xtradb-cluster-operator-certified,driverlessai-deployment-operator-certified,ibm-spectrum-scale-csi,cyberarmor-operator-certified,gitlab-operator
    State:          Running
      Started:      Wed, 13 May 2020 22:44:03 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      certified-operators-d5b8986fb-f2ksm
Namespace:                 openshift-marketplace
Priority:                  0
Node:                      ibmworker01.os4.ringen.us/192.168.93.43
Start Time:                Wed, 13 May 2020 18:53:33 -0400
Labels:                    marketplace.operatorSource=certified-operators
                           pod-template-hash=d5b8986fb
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.1.10"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.1.1"
                                 ]
                             }]
                           openshift-marketplace-update-hash: 160eb8597eca39bc
                           openshift.io/scc: restricted
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.1.10
IPs:
  IP:           10.241.1.10
Controlled By:  ReplicaSet/certified-operators-d5b8986fb
Containers:
  certified-operators:
    Container ID:  cri-o://e2993f2cd69c712df5e5fd948e74dacc9baf71b66eae3017c02abf0a3c4f7773
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|certified-operators
      -o
      couchbase-enterprise-certified,appranix-cps,storageos-10tb,joget-dx-operator,hazelcast-jet-enterprise-operator,joget-openshift-operator,cpx-cic-operator,traefikee-certified,gpu-operator-certified,ubix-operator,portshift-operator,ibm-helm-api-operator-app,sysdig-certified,perceptilabs-operator-package,k8s-triliovault,tidb-operator-certified,dotscience-operator,yugabyte-operator,redhat-marketplace-operator,portworx-certified,presto-operator,cortex-operator,newrelic-infrastructure,appdynamics-operator,twistlock-certified,percona-server-mongodb-operator-certified,couchdb-operator-certified,ibm-metering-operator-app,nginx-ingress-operator,citrix-cpx-istio-sidecar-injector-operator,cert-manager-operator,ibm-platform-api-operator-app,rocketchat-operator-certified,mongodb-enterprise,seldon-operator-certified,neuvector-certified-operator,alcide-kaudit-operator,cortex-hub-operator,instana-agent,open-liberty-certified,ibm-mongodb-operator-app,aqua-operator-certified,kubeturbo-marketplace-certified,anaconda-team-edition,eddi-operator-certified,insightedge-enterprise-operator2,traefikee-redhat-certified,federatorai-certified,wavefront-operator,rapidbiz-operator-certified,f5-bigip-ctlr-operator,cortex-healthcare-hub-operator,openunison-ocp-certified,kong,appsody-operator-certified,ocean-operator,redis-enterprise-operator-cert,ibm-helm-repo-operator-app,memql-certified,here-service-operator-certified,triggermesh-operator,ibm-spectrum-symphony-operator,hazelcast-enterprise-certified,kubeturbo-certified,cortex-certifai-operator,tigera-operator,storageos,aqua-certified,crunchy-postgres-operator,t8c-certified,robin-operator,nuodb-ce-certified,ivory-server-app,nxrm-operator-certified,open-enterprise-spinnaker,ibm-block-csi-operator,xcrypt-operator,datadog-operator-certified,ibm-management-ingress-operator-app,akka-cluster-operator-certified,runtime-component-operator-certified,ibm-monitoring-grafana-operator-app,zabbix-operator-certified,hpe-csi-operator,cockroachdb-certified,ibm-auditlogging-operator-app,nxiq-operator-certified,kubemq-operator-marketplace,planetscale-certified,sematext,transadv-operator,cih-operator-certified,infinibox-operator-certified,cortex-fabric-operator,atomicorp-helm-operator-certified,oneagent-certified,anchore-engine,kube-arangodb,storageos-1tb,uma-operator,timemachine-operator,orca,kubemq-operator,openshiftartifactoryha-operator,synopsys-certified,cic-operator,splunk-certified,citrix-adc-istio-ingress-gateway-operator,percona-xtradb-cluster-operator-certified,driverlessai-deployment-operator-certified,ibm-spectrum-scale-csi,cyberarmor-operator-certified,gitlab-operator
    State:          Running
      Started:      Wed, 13 May 2020 18:53:35 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      community-operators-6dd67f5d57-rb6vd
Namespace:                 openshift-marketplace
Priority:                  0
Node:                      ibmworker01.os4.ringen.us/192.168.93.43
Start Time:                Wed, 13 May 2020 17:52:35 -0400
Labels:                    marketplace.operatorSource=community-operators
                           pod-template-hash=6dd67f5d57
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.1.9"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.1.1"
                                 ]
                             }]
                           openshift-marketplace-update-hash: 160eb505d31bfaf4
                           openshift.io/scc: restricted
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.1.9
IPs:
  IP:           10.241.1.9
Controlled By:  ReplicaSet/community-operators-6dd67f5d57
Containers:
  community-operators:
    Container ID:  cri-o://dc64db73210419127bdb2454cf30efa8c91c8b3351a4e21ecc759302ab0e892b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|community-operators
      -o
      enmasse,esindex-operator,konveyor-operator,csi-operator,wso2am-operator,spark-gcp,infinispan,ibmcloud-operator,quay,api-operator,egressip-ipam-operator,strimzi-kafka-operator,microcks,3scale-community-operator,radanalytics-spark,service-binding-operator,composable-operator,traefikee-operator,splunk,camel-k,cockroachdb,cert-utils-operator,atlasmap-operator,seldon-operator,argocd-operator-helm,openebs,t8c,hyperfoil-bundle,container-security-operator,apicast-community-operator,prometheus,aqua,openshift-pipelines-operator,must-gather-operator,snyk-operator,keda,opsmx-spinnaker-operator,microsegmentation-operator,special-resource-operator,triggermesh,jenkins-operator,etcd,metering,knative-camel-operator,descheduler,apicurito,hive-operator,ember-csi-operator,neuvector-community-operator,lib-bucket-provisioner,keepalived-operator,multicluster-operators-subscription,federation,kubefed,postgresql-operator-dev4devs-com,maistraoperator,kubestone,ibm-spectrum-scale-csi-operator,horreum-operator,redis-operator,myvirtualdirectory,syndesis,event-streams-topic,nexus-operator-m88i,kogito-operator,crossplane,podium-operator-bundle,node-problem-detector,submariner,keycloak-operator,teiid,prisma-cloud-compute-console-operator,postgresql,awss3-operator-registry,kubeturbo,ripsaw,iot-simulator,hawtio-operator,skydive-operator,nsm-operator-registry,jaeger,federatorai,akka-cluster-operator,eunomia,kiali,lightbend-console-operator,cost-mgmt-operator,knative-eventing-operator,opendatahub-operator,namespace-configuration-operator,knative-kafka-operator,grafana-operator,resource-locker-operator,argocd-operator,datadog-operator,planetscale,spinnaker-operator,eclipse-che,ibm-block-csi-operator-community,kubernetes-imagepuller-operator,codeready-toolchain-operator
    State:          Running
      Started:      Wed, 13 May 2020 17:52:47 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:         community-operators-6dd67f5d57-wqqd9
Namespace:    openshift-marketplace
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:00 -0400
Labels:       marketplace.operatorSource=community-operators
              pod-template-hash=6dd67f5d57
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.95"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift-marketplace-update-hash: 160eb505d31bfaf4
              openshift.io/scc: restricted
Status:       Running
IP:           10.241.0.95
IPs:
  IP:           10.241.0.95
Controlled By:  ReplicaSet/community-operators-6dd67f5d57
Containers:
  community-operators:
    Container ID:  cri-o://bcfa90da73b841e4b2dabd5bca82d0c0d0d444f777f11d5ab61d188ffd267078
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|community-operators
      -o
      enmasse,esindex-operator,konveyor-operator,csi-operator,wso2am-operator,spark-gcp,infinispan,ibmcloud-operator,quay,api-operator,egressip-ipam-operator,strimzi-kafka-operator,microcks,3scale-community-operator,radanalytics-spark,service-binding-operator,composable-operator,traefikee-operator,splunk,camel-k,cockroachdb,cert-utils-operator,atlasmap-operator,seldon-operator,argocd-operator-helm,openebs,t8c,hyperfoil-bundle,container-security-operator,apicast-community-operator,prometheus,aqua,openshift-pipelines-operator,must-gather-operator,snyk-operator,keda,opsmx-spinnaker-operator,microsegmentation-operator,special-resource-operator,triggermesh,jenkins-operator,etcd,metering,knative-camel-operator,descheduler,apicurito,hive-operator,ember-csi-operator,neuvector-community-operator,lib-bucket-provisioner,keepalived-operator,multicluster-operators-subscription,federation,kubefed,postgresql-operator-dev4devs-com,maistraoperator,kubestone,ibm-spectrum-scale-csi-operator,horreum-operator,redis-operator,myvirtualdirectory,syndesis,event-streams-topic,nexus-operator-m88i,kogito-operator,crossplane,podium-operator-bundle,node-problem-detector,submariner,keycloak-operator,teiid,prisma-cloud-compute-console-operator,postgresql,awss3-operator-registry,kubeturbo,ripsaw,iot-simulator,hawtio-operator,skydive-operator,nsm-operator-registry,jaeger,federatorai,akka-cluster-operator,eunomia,kiali,lightbend-console-operator,cost-mgmt-operator,knative-eventing-operator,opendatahub-operator,namespace-configuration-operator,knative-kafka-operator,grafana-operator,resource-locker-operator,argocd-operator,datadog-operator,planetscale,spinnaker-operator,eclipse-che,ibm-block-csi-operator-community,kubernetes-imagepuller-operator,codeready-toolchain-operator
    State:          Running
      Started:      Wed, 13 May 2020 22:44:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 marketplace-operator-6f7cb8fd4d-7bnm2
Namespace:            openshift-marketplace
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:12 -0400
Labels:               name=marketplace-operator
                      pod-template-hash=6f7cb8fd4d
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.44"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.44
IPs:
  IP:           10.241.0.44
Controlled By:  ReplicaSet/marketplace-operator-6f7cb8fd4d
Containers:
  marketplace-operator:
    Container ID:  cri-o://e7e4a148008f58a0a4b9108e4a487ba5d230a262134ee96a7273d48fc43bca14
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b41c9e898ea152ffe243ff2eb957ac29af9b087ad10e956785d7d4af2c842961
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b41c9e898ea152ffe243ff2eb957ac29af9b087ad10e956785d7d4af2c842961
    Ports:         60000/TCP, 8080/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      marketplace-operator
    Args:
      -registryServerImage=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
      -defaultsDir=/defaults
    State:          Running
      Started:      Wed, 13 May 2020 17:52:27 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:38:14 -0400
      Finished:     Wed, 13 May 2020 17:52:26 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      10m
      memory:   50Mi
    Liveness:   http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      WATCH_NAMESPACE:  openshift-marketplace (v1:metadata.namespace)
      POD_NAME:         marketplace-operator-6f7cb8fd4d-7bnm2 (v1:metadata.name)
      OPERATOR_NAME:    marketplace-operator
      RELEASE_VERSION:  4.3.13
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from marketplace-operator-token-lvkpw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  true
  marketplace-operator-token-lvkpw:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  marketplace-operator-token-lvkpw
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.44:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x4 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.44:8080/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.44:8080/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:         redhat-operators-7db89c6466-4s7h9
Namespace:    openshift-marketplace
Priority:     0
Node:         dc1master01.os4.ringen.us/192.168.99.41
Start Time:   Wed, 13 May 2020 22:44:00 -0400
Labels:       marketplace.operatorSource=redhat-operators
              pod-template-hash=7db89c6466
Annotations:  k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "openshift-sdn",
                    "interface": "eth0",
                    "ips": [
                        "10.241.0.94"
                    ],
                    "dns": {},
                    "default-route": [
                        "10.241.0.1"
                    ]
                }]
              openshift-marketplace-update-hash: 160eb505fd434d52
              openshift.io/scc: restricted
Status:       Running
IP:           10.241.0.94
IPs:
  IP:           10.241.0.94
Controlled By:  ReplicaSet/redhat-operators-7db89c6466
Containers:
  redhat-operators:
    Container ID:  cri-o://0943db9a11062da1c69f677b792e9bea1b3a089548de424f8d4a3c2ef7a998e3
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|redhat-operators
      -o
      fuse-apicurito,serverless-operator,apicast-operator,clusterresourceoverride,jaeger-product,nfd,cam-operator,cluster-kube-descheduler-operator,amq-online,cluster-logging,fuse-online,sriov-network-operator,amq-streams,quay-operator,local-storage-operator,elasticsearch-operator,kubevirt-hyperconverged,metering-ocp,ocs-operator,ptp-operator,amq-broker-lts,openshiftansibleservicebroker,amq7-cert-manager,3scale-operator,eap,performance-addon-operator,quay-bridge-operator,codeready-workspaces,openshifttemplateservicebroker,amq7-interconnect-operator,dv-operator,businessautomation-operator,servicemeshoperator,kiali-ossm,amq-broker,container-security-operator,datagrid
    State:          Running
      Started:      Wed, 13 May 2020 22:44:03 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      redhat-operators-7db89c6466-g24xw
Namespace:                 openshift-marketplace
Priority:                  0
Node:                      ibmworker01.os4.ringen.us/192.168.93.43
Start Time:                Wed, 13 May 2020 17:52:36 -0400
Labels:                    marketplace.operatorSource=redhat-operators
                           pod-template-hash=7db89c6466
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.1.8"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.1.1"
                                 ]
                             }]
                           openshift-marketplace-update-hash: 160eb505fd434d52
                           openshift.io/scc: restricted
Status:                    Terminating (lasts 8h)
Termination Grace Period:  30s
IP:                        10.241.1.8
IPs:
  IP:           10.241.1.8
Controlled By:  ReplicaSet/redhat-operators-7db89c6466
Containers:
  redhat-operators:
    Container ID:  cri-o://34c65d1a7fa47077d6ead397b1950434fc95a1c29ada4e2b8ee73ddd611c08ca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
    Port:          50051/TCP
    Host Port:     0/TCP
    Command:
      appregistry-server
      -r
      https://quay.io/cnr|redhat-operators
      -o
      fuse-apicurito,serverless-operator,apicast-operator,clusterresourceoverride,jaeger-product,nfd,cam-operator,cluster-kube-descheduler-operator,amq-online,cluster-logging,fuse-online,sriov-network-operator,amq-streams,quay-operator,local-storage-operator,elasticsearch-operator,kubevirt-hyperconverged,metering-ocp,ocs-operator,ptp-operator,amq-broker-lts,openshiftansibleservicebroker,amq7-cert-manager,3scale-operator,eap,performance-addon-operator,quay-bridge-operator,codeready-workspaces,openshifttemplateservicebroker,amq7-interconnect-operator,dv-operator,businessautomation-operator,servicemeshoperator,kiali-ossm,amq-broker,container-security-operator,datagrid
    State:          Running
      Started:      Wed, 13 May 2020 17:52:47 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Liveness:   exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Readiness:  exec [grpc_health_probe -addr=localhost:50051] delay=5s timeout=1s period=10s #success=1 #failure=30
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from marketplace-trusted-ca (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lbxgs (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  marketplace-trusted-ca:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      marketplace-trusted-ca
    Optional:  false
  default-token-lbxgs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-lbxgs
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 alertmanager-main-0
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:49 -0400
Labels:               alertmanager=main
                      app=alertmanager
                      controller-revision-hash=alertmanager-main-59db6bd74c
                      statefulset.kubernetes.io/pod-name=alertmanager-main-0
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.90"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.90
IPs:
  IP:           10.241.0.90
Controlled By:  StatefulSet/alertmanager-main
Containers:
  alertmanager:
    Container ID:  cri-o://b79ef9e739b1b07ef5a498eda6aa2524f6777aee8c7444974a3767e21e0b8b1b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Ports:         9094/TCP, 9094/UDP
    Host Ports:    0/TCP, 0/UDP
    Args:
      --config.file=/etc/alertmanager/config/alertmanager.yaml
      --cluster.listen-address=[$(POD_IP)]:9094
      --storage.path=/alertmanager
      --data.retention=120h
      --web.listen-address=127.0.0.1:9093
      --web.external-url=https://alertmanager-main-openshift-monitoring.apps.os4.ringen.us/
      --web.route-prefix=/
      --cluster.peer=alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-1.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-2.alertmanager-operated.openshift-monitoring.svc:9094
    State:          Running
      Started:      Wed, 13 May 2020 12:45:53 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      memory:  200Mi
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from alertmanager-main-db (rw)
      /etc/alertmanager/config from config-volume (rw)
      /etc/alertmanager/secrets/alertmanager-main-proxy from secret-alertmanager-main-proxy (ro)
      /etc/alertmanager/secrets/alertmanager-main-tls from secret-alertmanager-main-tls (ro)
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  config-reloader:
    Container ID:  cri-o://fc460ce9fa09dac844deb26c65b9ba0e24bf67560ffe78f2cf7f0a9161e23e00
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      -webhook-url=http://localhost:9093/-/reload
      -volume-dir=/etc/alertmanager/config
    State:          Running
      Started:      Wed, 13 May 2020 12:45:54 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/alertmanager/config from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  alertmanager-proxy:
    Container ID:  cri-o://486b63c814f7f48080d6567f8a88e069e86e5a5862aafcc0151e1f3d7b511c67
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9095/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9095
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9093
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-service-account=alertmanager-main
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:45:55 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /etc/proxy/secrets from secret-alertmanager-main-proxy (rw)
      /etc/tls/private from secret-alertmanager-main-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main
    Optional:    false
  secret-alertmanager-main-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-tls
    Optional:    false
  secret-alertmanager-main-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-proxy
    Optional:    false
  alertmanager-main-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  alertmanager-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alertmanager-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  alertmanager-main-token-vcvz2:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-token-vcvz2
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 alertmanager-main-1
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:35 -0400
Labels:               alertmanager=main
                      app=alertmanager
                      controller-revision-hash=alertmanager-main-59db6bd74c
                      statefulset.kubernetes.io/pod-name=alertmanager-main-1
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.89"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.89
IPs:
  IP:           10.241.0.89
Controlled By:  StatefulSet/alertmanager-main
Containers:
  alertmanager:
    Container ID:  cri-o://1ee0c21c5d8fa1677acdd2d3af36457af96063e47446535fd7c849f1b81ed7d0
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Ports:         9094/TCP, 9094/UDP
    Host Ports:    0/TCP, 0/UDP
    Args:
      --config.file=/etc/alertmanager/config/alertmanager.yaml
      --cluster.listen-address=[$(POD_IP)]:9094
      --storage.path=/alertmanager
      --data.retention=120h
      --web.listen-address=127.0.0.1:9093
      --web.external-url=https://alertmanager-main-openshift-monitoring.apps.os4.ringen.us/
      --web.route-prefix=/
      --cluster.peer=alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-1.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-2.alertmanager-operated.openshift-monitoring.svc:9094
    State:          Running
      Started:      Wed, 13 May 2020 12:45:43 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      memory:  200Mi
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from alertmanager-main-db (rw)
      /etc/alertmanager/config from config-volume (rw)
      /etc/alertmanager/secrets/alertmanager-main-proxy from secret-alertmanager-main-proxy (ro)
      /etc/alertmanager/secrets/alertmanager-main-tls from secret-alertmanager-main-tls (ro)
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  config-reloader:
    Container ID:  cri-o://135a6d2574d5b20c0b56076cf8c535d34e4d1d226a2f3c9af15b8715169be58f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      -webhook-url=http://localhost:9093/-/reload
      -volume-dir=/etc/alertmanager/config
    State:          Running
      Started:      Wed, 13 May 2020 12:45:44 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/alertmanager/config from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  alertmanager-proxy:
    Container ID:  cri-o://733b73926a544328e354e84060ac163003438a8777dfbcf8df17dfc12796a9ff
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9095/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9095
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9093
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-service-account=alertmanager-main
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:45:45 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /etc/proxy/secrets from secret-alertmanager-main-proxy (rw)
      /etc/tls/private from secret-alertmanager-main-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main
    Optional:    false
  secret-alertmanager-main-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-tls
    Optional:    false
  secret-alertmanager-main-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-proxy
    Optional:    false
  alertmanager-main-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  alertmanager-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alertmanager-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  alertmanager-main-token-vcvz2:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-token-vcvz2
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 alertmanager-main-2
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:04 -0400
Labels:               alertmanager=main
                      app=alertmanager
                      controller-revision-hash=alertmanager-main-59db6bd74c
                      statefulset.kubernetes.io/pod-name=alertmanager-main-2
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.83"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.83
IPs:
  IP:           10.241.0.83
Controlled By:  StatefulSet/alertmanager-main
Containers:
  alertmanager:
    Container ID:  cri-o://e9c4c99965a7d9a8ffe2491b220f4a734b71bcbcb2f089539295432747646b0c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
    Ports:         9094/TCP, 9094/UDP
    Host Ports:    0/TCP, 0/UDP
    Args:
      --config.file=/etc/alertmanager/config/alertmanager.yaml
      --cluster.listen-address=[$(POD_IP)]:9094
      --storage.path=/alertmanager
      --data.retention=120h
      --web.listen-address=127.0.0.1:9093
      --web.external-url=https://alertmanager-main-openshift-monitoring.apps.os4.ringen.us/
      --web.route-prefix=/
      --cluster.peer=alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-1.alertmanager-operated.openshift-monitoring.svc:9094
      --cluster.peer=alertmanager-main-2.alertmanager-operated.openshift-monitoring.svc:9094
    State:          Running
      Started:      Wed, 13 May 2020 12:45:09 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      memory:  200Mi
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /alertmanager from alertmanager-main-db (rw)
      /etc/alertmanager/config from config-volume (rw)
      /etc/alertmanager/secrets/alertmanager-main-proxy from secret-alertmanager-main-proxy (ro)
      /etc/alertmanager/secrets/alertmanager-main-tls from secret-alertmanager-main-tls (ro)
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  config-reloader:
    Container ID:  cri-o://fad1a0f0d68781d6f83b59de1eb5319eb920e851b45162dc3a0cf55016af85cd
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      -webhook-url=http://localhost:9093/-/reload
      -volume-dir=/etc/alertmanager/config
    State:          Running
      Started:      Wed, 13 May 2020 12:45:12 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/alertmanager/config from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
  alertmanager-proxy:
    Container ID:  cri-o://c5278a10da13ba74384da56e92f5ec92b2cb02e753dc8e227a52a88670a9a893
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9095/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9095
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9093
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-service-account=alertmanager-main
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:45:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from alertmanager-trusted-ca-bundle (ro)
      /etc/proxy/secrets from secret-alertmanager-main-proxy (rw)
      /etc/tls/private from secret-alertmanager-main-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from alertmanager-main-token-vcvz2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main
    Optional:    false
  secret-alertmanager-main-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-tls
    Optional:    false
  secret-alertmanager-main-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-proxy
    Optional:    false
  alertmanager-main-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  alertmanager-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      alertmanager-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  alertmanager-main-token-vcvz2:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  alertmanager-main-token-vcvz2
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 cluster-monitoring-operator-7b898649bd-fj2c7
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:12 -0400
Labels:               app=cluster-monitoring-operator
                      pod-template-hash=7b898649bd
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.36"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.36
IPs:
  IP:           10.241.0.36
Controlled By:  ReplicaSet/cluster-monitoring-operator-7b898649bd
Containers:
  cluster-monitoring-operator:
    Container ID:  cri-o://dd4f0b4c267e86b68c2eb51ca75aa3cff22b64a7358f6807ef1cb25eeecf03ca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:345781fa64e6815e67a17b99ae92cc67eb8a4f6ebdbb2aaa10d2127569b79a42
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:345781fa64e6815e67a17b99ae92cc67eb8a4f6ebdbb2aaa10d2127569b79a42
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      -namespace=openshift-monitoring
      -namespace-user-workload=openshift-user-workload-monitoring
      -configmap=cluster-monitoring-config
      -release-version=$(RELEASE_VERSION)
      -logtostderr=true
      -v=3
      -images=prometheus-operator=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab76ddf988c85962887de6705d61442c6105da166466ac5c916c34d873d55b2a
      -images=prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
      -images=configmap-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
      -images=prometheus=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3179c1d9d735d1781dbed24c9e1aa48bada2a417823fed02630467fa26fe6701
      -images=alertmanager=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0768f3ebd1bd45325cd8a5b94be35f2673d5895ec12c7c52846c346859f1bfd7
      -images=grafana=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4f0077f4e02b9d45373dfa6214e7154eaccd6fba728454c03f74332b8d927c5b
      -images=oauth-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
      -images=node-exporter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
      -images=kube-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d492aeb23c3a3edbe79614aabd59d2badf8ced66e1cbc22df0904cb712a69a82
      -images=openshift-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ec7835e1157c1817539ac1cc06c69f61fcb9a1f90db6be1220225b2c74a31cac
      -images=kube-rbac-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
      -images=telemeter-client=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2ad52c7156e09479f1a931868a7576cf9dc979bd0280abfa15d5a35dcfaec7c5
      -images=prom-label-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
      -images=k8s-prometheus-adapter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980
      -images=thanos=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    State:          Running
      Started:      Wed, 13 May 2020 12:37:35 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:  4.3.13
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from cluster-monitoring-operator-token-ss9zj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cluster-monitoring-operator-token-ss9zj:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cluster-monitoring-operator-token-ss9zj
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 grafana-6db95cb6f7-p5dq4
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:10 -0400
Labels:               app=grafana
                      pod-template-hash=6db95cb6f7
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.86"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.86
IPs:
  IP:           10.241.0.86
Controlled By:  ReplicaSet/grafana-6db95cb6f7
Containers:
  grafana:
    Container ID:  cri-o://cb43cc7d814647b45b15c22a23fd5b9ab26edc603ffe0263e295fd035e833007
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4f0077f4e02b9d45373dfa6214e7154eaccd6fba728454c03f74332b8d927c5b
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4f0077f4e02b9d45373dfa6214e7154eaccd6fba728454c03f74332b8d927c5b
    Port:          3001/TCP
    Host Port:     0/TCP
    Args:
      -config=/etc/grafana/grafana.ini
    State:          Running
      Started:      Wed, 13 May 2020 12:45:19 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  <none>
    Mounts:
      /etc/grafana from grafana-config (rw)
      /etc/grafana/provisioning/dashboards from grafana-dashboards (rw)
      /etc/grafana/provisioning/datasources from grafana-datasources (rw)
      /grafana-dashboard-definitions/0/cluster-total from grafana-dashboard-cluster-total (rw)
      /grafana-dashboard-definitions/0/etcd from grafana-dashboard-etcd (rw)
      /grafana-dashboard-definitions/0/k8s-resources-cluster from grafana-dashboard-k8s-resources-cluster (rw)
      /grafana-dashboard-definitions/0/k8s-resources-namespace from grafana-dashboard-k8s-resources-namespace (rw)
      /grafana-dashboard-definitions/0/k8s-resources-node from grafana-dashboard-k8s-resources-node (rw)
      /grafana-dashboard-definitions/0/k8s-resources-pod from grafana-dashboard-k8s-resources-pod (rw)
      /grafana-dashboard-definitions/0/k8s-resources-workload from grafana-dashboard-k8s-resources-workload (rw)
      /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace from grafana-dashboard-k8s-resources-workloads-namespace (rw)
      /grafana-dashboard-definitions/0/node-cluster-rsrc-use from grafana-dashboard-node-cluster-rsrc-use (rw)
      /grafana-dashboard-definitions/0/node-rsrc-use from grafana-dashboard-node-rsrc-use (rw)
      /grafana-dashboard-definitions/0/prometheus from grafana-dashboard-prometheus (rw)
      /var/lib/grafana from grafana-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from grafana-token-dkfln (ro)
  grafana-proxy:
    Container ID:  cri-o://3d064577ffe2a8776d84d053ebd6d39918540cba2c2fff57d50ff947fd00c1db
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          3000/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:3000
      -http-address=
      -email-domain=*
      -upstream=http://localhost:3001
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-service-account=grafana
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:45:21 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   20Mi
    Readiness:  tcp-socket :https delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from grafana-trusted-ca-bundle (ro)
      /etc/proxy/secrets from secret-grafana-proxy (rw)
      /etc/tls/private from secret-grafana-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from grafana-token-dkfln (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  grafana-storage:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  grafana-datasources:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-datasources
    Optional:    false
  grafana-dashboards:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboards
    Optional:  false
  grafana-dashboard-cluster-total:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-cluster-total
    Optional:  false
  grafana-dashboard-etcd:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-etcd
    Optional:  false
  grafana-dashboard-k8s-resources-cluster:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-cluster
    Optional:  false
  grafana-dashboard-k8s-resources-namespace:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-namespace
    Optional:  false
  grafana-dashboard-k8s-resources-node:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-node
    Optional:  false
  grafana-dashboard-k8s-resources-pod:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-pod
    Optional:  false
  grafana-dashboard-k8s-resources-workload:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-workload
    Optional:  false
  grafana-dashboard-k8s-resources-workloads-namespace:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-k8s-resources-workloads-namespace
    Optional:  false
  grafana-dashboard-node-cluster-rsrc-use:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-node-cluster-rsrc-use
    Optional:  false
  grafana-dashboard-node-rsrc-use:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-node-rsrc-use
    Optional:  false
  grafana-dashboard-prometheus:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-dashboard-prometheus
    Optional:  false
  grafana-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-config
    Optional:    false
  secret-grafana-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-tls
    Optional:    false
  secret-grafana-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-proxy
    Optional:    false
  grafana-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      grafana-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  grafana-token-dkfln:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  grafana-token-dkfln
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: dial tcp 10.241.0.86:3000: i/o timeout


Name:                 kube-state-metrics-66dfc9f94f-8xjvp
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:38:57 -0400
Labels:               app=kube-state-metrics
                      pod-template-hash=66dfc9f94f
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.54"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.54
IPs:
  IP:           10.241.0.54
Controlled By:  ReplicaSet/kube-state-metrics-66dfc9f94f
Containers:
  kube-rbac-proxy-main:
    Container ID:  cri-o://778958f254a77fca942fbb2f9ddabd38234fa84ddd3b7fd897e9bedccd37c6ec
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:8443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8081/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:40:06 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from kube-state-metrics-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-9n2vx (ro)
  kube-rbac-proxy-self:
    Container ID:  cri-o://e0d292ef1833e219f05a63466effc2ab3f5b76d3dfa1d119dce87282f4139910
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9443/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:9443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8082/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:40:09 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from kube-state-metrics-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-9n2vx (ro)
  kube-state-metrics:
    Container ID:  cri-o://8cb4b44f7431e8a5eee7677f9ca46847fab78b5d20c22ca3ef1b29a70b82c2e8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d492aeb23c3a3edbe79614aabd59d2badf8ced66e1cbc22df0904cb712a69a82
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d492aeb23c3a3edbe79614aabd59d2badf8ced66e1cbc22df0904cb712a69a82
    Port:          <none>
    Host Port:     <none>
    Args:
      --host=127.0.0.1
      --port=8081
      --telemetry-host=127.0.0.1
      --telemetry-port=8082
    State:          Running
      Started:      Wed, 13 May 2020 12:40:26 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     40Mi
    Environment:  <none>
    Mounts:
      /tmp from volume-directive-shadow (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-state-metrics-token-9n2vx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  volume-directive-shadow:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-state-metrics-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-state-metrics-tls
    Optional:    false
  kube-state-metrics-token-9n2vx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-state-metrics-token-9n2vx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 node-exporter-g5fnj
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:00:39 -0400
Labels:               app=node-exporter
                      controller-revision-hash=5cb4556796
                      pod-template-generation=1
Annotations:          openshift.io/scc: node-exporter
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/node-exporter
Init Containers:
  init-textfile:
    Container ID:  cri-o://ca5ea5b49677914568f5cfdb9e27a4b358cfde427012eea16aea47c8d394a7f4
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      [[ ! -d /node_exporter/collectors/init ]] || find /node_exporter/collectors/init -perm /111 -type f -exec {} \;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:01 -0400
      Finished:     Wed, 13 May 2020 13:01:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      TMPDIR:  /tmp
    Mounts:
      /var/node_exporter/textfile from node-exporter-textfile (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Containers:
  node-exporter:
    Container ID:  cri-o://145db3bdf3f8da0b070e0cdccb897e33444b6f0ebb967f342d5720bb93549a0e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Args:
      --web.listen-address=127.0.0.1:9100
      --path.procfs=/host/proc
      --path.sysfs=/host/sys
      --path.rootfs=/host/root
      --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
      --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
      --no-collector.wifi
      --collector.mountstats
      --collector.cpu.info
      --collector.textfile.directory=/var/node_exporter/textfile
    State:          Running
      Started:      Wed, 13 May 2020 13:01:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        102m
      memory:     180Mi
    Environment:  <none>
    Mounts:
      /host/proc from proc (rw)
      /host/root from root (ro)
      /host/sys from sys (rw)
      /var/node_exporter/textfile from node-exporter-textfile (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://2453caaa065456cf5e2c25d74f26c9852f67a139232cc76ba7e0280fe0cebb6d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9100/TCP
    Host Port:     9100/TCP
    Args:
      --logtostderr
      --secure-listen-address=[$(IP)]:9100
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:9100/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 13:01:06 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      IP:   (v1:status.podIP)
    Mounts:
      /etc/tls/private from node-exporter-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  proc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  root:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  node-exporter-textfile:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  node-exporter-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-tls
    Optional:    false
  node-exporter-token-kmknn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-token-kmknn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 node-exporter-j6j4g
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:00:49 -0400
Labels:               app=node-exporter
                      controller-revision-hash=5cb4556796
                      pod-template-generation=1
Annotations:          openshift.io/scc: node-exporter
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/node-exporter
Init Containers:
  init-textfile:
    Container ID:  cri-o://e8c13056d277869368e6beeaf3c71c3fd4869867a0a40350c24a15cd308f3c91
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      [[ ! -d /node_exporter/collectors/init ]] || find /node_exporter/collectors/init -perm /111 -type f -exec {} \;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:09 -0400
      Finished:     Wed, 13 May 2020 13:01:10 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      TMPDIR:  /tmp
    Mounts:
      /var/node_exporter/textfile from node-exporter-textfile (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Containers:
  node-exporter:
    Container ID:  cri-o://ab4f3c32f96cb868c623d770e2fbcd320b2bcd2351d1cf669f4750a627c513f2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Args:
      --web.listen-address=127.0.0.1:9100
      --path.procfs=/host/proc
      --path.sysfs=/host/sys
      --path.rootfs=/host/root
      --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
      --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
      --no-collector.wifi
      --collector.mountstats
      --collector.cpu.info
      --collector.textfile.directory=/var/node_exporter/textfile
    State:          Running
      Started:      Wed, 13 May 2020 13:01:11 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        102m
      memory:     180Mi
    Environment:  <none>
    Mounts:
      /host/proc from proc (rw)
      /host/root from root (ro)
      /host/sys from sys (rw)
      /var/node_exporter/textfile from node-exporter-textfile (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://4e079ed93d70ecbe950a30a53115399d5fab46dd4f8346baaac3c28c2114c087
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9100/TCP
    Host Port:     9100/TCP
    Args:
      --logtostderr
      --secure-listen-address=[$(IP)]:9100
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:9100/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 13:01:14 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      IP:   (v1:status.podIP)
    Mounts:
      /etc/tls/private from node-exporter-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  proc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  root:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  node-exporter-textfile:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  node-exporter-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-tls
    Optional:    false
  node-exporter-token-kmknn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-token-kmknn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 node-exporter-z2b8q
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:38:57 -0400
Labels:               app=node-exporter
                      controller-revision-hash=5cb4556796
                      pod-template-generation=1
Annotations:          openshift.io/scc: node-exporter
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/node-exporter
Init Containers:
  init-textfile:
    Container ID:  cri-o://026a12856b7ec1c54b9822245822b5d304cf223640e9a45e988cdac70a4d870f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      [[ ! -d /node_exporter/collectors/init ]] || find /node_exporter/collectors/init -perm /111 -type f -exec {} \;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:40:25 -0400
      Finished:     Wed, 13 May 2020 12:40:26 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      TMPDIR:  /tmp
    Mounts:
      /var/node_exporter/textfile from node-exporter-textfile (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Containers:
  node-exporter:
    Container ID:  cri-o://c5c84c6b3c529eb8e21eeda6e1ea3d6259ba559509e8b196509b6965adf4b5de
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:02bfc8a0a0fb48a4ffbf495bbe3261f8f000aaec62c8ae29f57fb398f432a112
    Port:          <none>
    Host Port:     <none>
    Args:
      --web.listen-address=127.0.0.1:9100
      --path.procfs=/host/proc
      --path.sysfs=/host/sys
      --path.rootfs=/host/root
      --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
      --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
      --no-collector.wifi
      --collector.mountstats
      --collector.cpu.info
      --collector.textfile.directory=/var/node_exporter/textfile
    State:          Running
      Started:      Wed, 13 May 2020 12:40:28 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        102m
      memory:     180Mi
    Environment:  <none>
    Mounts:
      /host/proc from proc (rw)
      /host/root from root (ro)
      /host/sys from sys (rw)
      /var/node_exporter/textfile from node-exporter-textfile (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://8d84a1bd3b50d56dcc65eebd1d9bc2c029cbe7b1796ccbe9d67c1302dc849e65
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9100/TCP
    Host Port:     9100/TCP
    Args:
      --logtostderr
      --secure-listen-address=[$(IP)]:9100
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:9100/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:40:28 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      IP:   (v1:status.podIP)
    Mounts:
      /etc/tls/private from node-exporter-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from node-exporter-token-kmknn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  proc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  root:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  node-exporter-textfile:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  node-exporter-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-tls
    Optional:    false
  node-exporter-token-kmknn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  node-exporter-token-kmknn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 openshift-state-metrics-785dd5b54c-gx9rl
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:38:57 -0400
Labels:               k8s-app=openshift-state-metrics
                      pod-template-hash=785dd5b54c
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.55"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.55
IPs:
  IP:           10.241.0.55
Controlled By:  ReplicaSet/openshift-state-metrics-785dd5b54c
Containers:
  kube-rbac-proxy-main:
    Container ID:  cri-o://c220efd21771e2b814f59a90366a37d7ad22523890816b908e35584beb0398de
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:8443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8081/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:40:06 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from openshift-state-metrics-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-state-metrics-token-7rkxl (ro)
  kube-rbac-proxy-self:
    Container ID:  cri-o://3acc9f7da337efcc06eedd414114bd34804ffdb539123ee56bb7bcb6d312bf20
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9443/TCP
    Host Port:     0/TCP
    Args:
      --logtostderr
      --secure-listen-address=:9443
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --upstream=http://127.0.0.1:8082/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:40:08 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from openshift-state-metrics-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-state-metrics-token-7rkxl (ro)
  openshift-state-metrics:
    Container ID:  cri-o://e8f8d06d65a13ff257586c83dd284a0b2f35e7aa823952864436cce0c6321961
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ec7835e1157c1817539ac1cc06c69f61fcb9a1f90db6be1220225b2c74a31cac
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ec7835e1157c1817539ac1cc06c69f61fcb9a1f90db6be1220225b2c74a31cac
    Port:          <none>
    Host Port:     <none>
    Args:
      --host=127.0.0.1
      --port=8081
      --telemetry-host=127.0.0.1
      --telemetry-port=8082
    State:          Running
      Started:      Wed, 13 May 2020 12:40:25 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     150Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-state-metrics-token-7rkxl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  openshift-state-metrics-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-state-metrics-tls
    Optional:    false
  openshift-state-metrics-token-7rkxl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-state-metrics-token-7rkxl
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 prometheus-adapter-6c79cb8585-rv24l
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:14:56 -0400
Labels:               name=prometheus-adapter
                      pod-template-hash=6c79cb8585
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.110"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.110
IPs:
  IP:           10.241.0.110
Controlled By:  ReplicaSet/prometheus-adapter-6c79cb8585
Containers:
  prometheus-adapter:
    Container ID:  cri-o://9e4447d84674250fcbbb0a38286b4e9f28ca534d8f498f6fc09938be91b4eb61
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980
    Port:          6443/TCP
    Host Port:     0/TCP
    Args:
      --prometheus-auth-config=/etc/prometheus-config/prometheus-config.yaml
      --config=/etc/adapter/config.yaml
      --logtostderr=true
      --metrics-relist-interval=1m
      --prometheus-url=https://prometheus-k8s.openshift-monitoring.svc:9091
      --secure-port=6443
      --client-ca-file=/etc/tls/private/client-ca-file
      --requestheader-client-ca-file=/etc/tls/private/requestheader-client-ca-file
      --requestheader-allowed-names=kube-apiserver-proxy,system:kube-apiserver-proxy,system:openshift-aggregator
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Thu, 14 May 2020 07:14:58 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/adapter from config (rw)
      /etc/prometheus-config from prometheus-adapter-prometheus-config (rw)
      /etc/ssl/certs from serving-certs-ca-bundle (rw)
      /etc/tls/private from tls (ro)
      /tmp from tmpfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-adapter-token-cws5q (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmpfs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      adapter-config
    Optional:  false
  prometheus-adapter-prometheus-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-adapter-prometheus-config
    Optional:  false
  serving-certs-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      serving-certs-ca-bundle
    Optional:  false
  tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-adapter-4p4p1vou0pjm7
    Optional:    false
  prometheus-adapter-token-cws5q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-adapter-token-cws5q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                                Message
  ----    ------     ----       ----                                -------
  Normal  Scheduled  <unknown>  default-scheduler                   Successfully assigned openshift-monitoring/prometheus-adapter-6c79cb8585-rv24l to dc1master01.os4.ringen.us
  Normal  Pulled     15m        kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980" already present on machine
  Normal  Created    15m        kubelet, dc1master01.os4.ringen.us  Created container prometheus-adapter
  Normal  Started    15m        kubelet, dc1master01.os4.ringen.us  Started container prometheus-adapter


Name:                 prometheus-adapter-6c79cb8585-zgm5s
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:14:59 -0400
Labels:               name=prometheus-adapter
                      pod-template-hash=6c79cb8585
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.111"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.111
IPs:
  IP:           10.241.0.111
Controlled By:  ReplicaSet/prometheus-adapter-6c79cb8585
Containers:
  prometheus-adapter:
    Container ID:  cri-o://81f0a9eb44b089ba0d2b4631a1a7a358fea9989318aa3cf72efb409599478dcc
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980
    Port:          6443/TCP
    Host Port:     0/TCP
    Args:
      --prometheus-auth-config=/etc/prometheus-config/prometheus-config.yaml
      --config=/etc/adapter/config.yaml
      --logtostderr=true
      --metrics-relist-interval=1m
      --prometheus-url=https://prometheus-k8s.openshift-monitoring.svc:9091
      --secure-port=6443
      --client-ca-file=/etc/tls/private/client-ca-file
      --requestheader-client-ca-file=/etc/tls/private/requestheader-client-ca-file
      --requestheader-allowed-names=kube-apiserver-proxy,system:kube-apiserver-proxy,system:openshift-aggregator
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
    State:          Running
      Started:      Thu, 14 May 2020 07:15:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/adapter from config (rw)
      /etc/prometheus-config from prometheus-adapter-prometheus-config (rw)
      /etc/ssl/certs from serving-certs-ca-bundle (rw)
      /etc/tls/private from tls (ro)
      /tmp from tmpfs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-adapter-token-cws5q (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmpfs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      adapter-config
    Optional:  false
  prometheus-adapter-prometheus-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-adapter-prometheus-config
    Optional:  false
  serving-certs-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      serving-certs-ca-bundle
    Optional:  false
  tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-adapter-4p4p1vou0pjm7
    Optional:    false
  prometheus-adapter-token-cws5q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-adapter-token-cws5q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age        From                                Message
  ----    ------     ----       ----                                -------
  Normal  Scheduled  <unknown>  default-scheduler                   Successfully assigned openshift-monitoring/prometheus-adapter-6c79cb8585-zgm5s to dc1master01.os4.ringen.us
  Normal  Pulled     15m        kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0db7195fc1a3236b8f8dd7696256e8ec5b3f96e72d67a26357447394e7782980" already present on machine
  Normal  Created    15m        kubelet, dc1master01.os4.ringen.us  Created container prometheus-adapter
  Normal  Started    15m        kubelet, dc1master01.os4.ringen.us  Started container prometheus-adapter


Name:                      prometheus-k8s-0
Namespace:                 openshift-monitoring
Priority:                  2000000000
Priority Class Name:       system-cluster-critical
Node:                      ibmworker02.os4.ringen.us/192.168.93.53
Start Time:                Wed, 13 May 2020 13:02:04 -0400
Labels:                    app=prometheus
                           controller-revision-hash=prometheus-k8s-64f6b46cb4
                           prometheus=k8s
                           statefulset.kubernetes.io/pod-name=prometheus-k8s-0
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.2.3"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.2.1"
                                 ]
                             }]
                           openshift.io/scc: restricted
Status:                    Terminating (lasts 8h)
Termination Grace Period:  600s
IP:                        10.241.2.3
IPs:
  IP:           10.241.2.3
Controlled By:  StatefulSet/prometheus-k8s
Containers:
  prometheus:
    Container ID:  cri-o://39ba6e6d3e42b3a6a41ddeaadb5637fda1f0885c0bb6f5370e9e83016489de31
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3179c1d9d735d1781dbed24c9e1aa48bada2a417823fed02630467fa26fe6701
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3179c1d9d735d1781dbed24c9e1aa48bada2a417823fed02630467fa26fe6701
    Port:          <none>
    Host Port:     <none>
    Args:
      --web.console.templates=/etc/prometheus/consoles
      --web.console.libraries=/etc/prometheus/console_libraries
      --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      --storage.tsdb.path=/prometheus
      --storage.tsdb.retention.time=15d
      --web.enable-lifecycle
      --storage.tsdb.no-lockfile
      --web.external-url=https://prometheus-k8s-openshift-monitoring.apps.os4.ringen.us/
      --web.route-prefix=/
      --web.listen-address=127.0.0.1:9090
    State:       Running
      Started:   Wed, 13 May 2020 13:02:25 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   aller=main.go:657 msg="Starting TSDB ..."
level=info ts=2020-05-13T17:02:11.737Z caller=web.go:496 component=web msg="Start listening for connections" address=127.0.0.1:9090
level=info ts=2020-05-13T17:02:11.743Z caller=head.go:535 component=tsdb msg="replaying WAL, this may take awhile"
level=info ts=2020-05-13T17:02:11.743Z caller=head.go:583 component=tsdb msg="WAL segment loaded" segment=0 maxSegment=0
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:672 fs_type=EXT4_SUPER_MAGIC
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:673 msg="TSDB started"
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:743 msg="Loading configuration file" filename=/etc/prometheus/config_out/prometheus.env.yaml
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:526 msg="Stopping scrape discovery manager..."
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:540 msg="Stopping notify discovery manager..."
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:562 msg="Stopping scrape manager..."
level=info ts=2020-05-13T17:02:11.744Z caller=manager.go:814 component="rule manager" msg="Stopping rule manager..."
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:522 msg="Scrape discovery manager stopped"
level=info ts=2020-05-13T17:02:11.744Z caller=manager.go:820 component="rule manager" msg="Rule manager stopped"
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:536 msg="Notify discovery manager stopped"
level=info ts=2020-05-13T17:02:11.744Z caller=main.go:556 msg="Scrape manager stopped"
level=info ts=2020-05-13T17:02:11.746Z caller=notifier.go:602 component=notifier msg="Stopping notification manager..."
level=info ts=2020-05-13T17:02:11.746Z caller=main.go:727 msg="Notifier manager stopped"
level=error ts=2020-05-13
      Exit Code:    1
      Started:      Wed, 13 May 2020 13:02:11 -0400
      Finished:     Wed, 13 May 2020 13:02:11 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        200m
      memory:     1Gi
    Liveness:     exec [sh -c if [ -x "$(command -v curl)" ]; then curl http://localhost:9090/-/healthy; elif [ -x "$(command -v wget)" ]; then wget -q http://localhost:9090/-/healthy; else exit 1; fi] delay=0s timeout=3s period=5s #success=1 #failure=6
    Readiness:    exec [sh -c if [ -x "$(command -v curl)" ]; then curl http://localhost:9090/-/ready; elif [ -x "$(command -v wget)" ]; then wget -q http://localhost:9090/-/ready; else exit 1; fi] delay=0s timeout=3s period=5s #success=1 #failure=120
    Environment:  <none>
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from prometheus-trusted-ca-bundle (ro)
      /etc/prometheus/certs from tls-assets (ro)
      /etc/prometheus/config_out from config-out (ro)
      /etc/prometheus/configmaps/kubelet-serving-ca-bundle from configmap-kubelet-serving-ca-bundle (ro)
      /etc/prometheus/configmaps/serving-certs-ca-bundle from configmap-serving-certs-ca-bundle (ro)
      /etc/prometheus/rules/prometheus-k8s-rulefiles-0 from prometheus-k8s-rulefiles-0 (rw)
      /etc/prometheus/secrets/kube-etcd-client-certs from secret-kube-etcd-client-certs (ro)
      /etc/prometheus/secrets/kube-rbac-proxy from secret-kube-rbac-proxy (ro)
      /etc/prometheus/secrets/prometheus-k8s-htpasswd from secret-prometheus-k8s-htpasswd (ro)
      /etc/prometheus/secrets/prometheus-k8s-proxy from secret-prometheus-k8s-proxy (ro)
      /etc/prometheus/secrets/prometheus-k8s-tls from secret-prometheus-k8s-tls (ro)
      /prometheus from prometheus-k8s-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prometheus-config-reloader:
    Container ID:  cri-o://c93ddbc7866f5567a637454b1b0bff71421d31dd1f47c6f5f39318012fe32bc6
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/prometheus-config-reloader
    Args:
      --log-format=logfmt
      --reload-url=http://localhost:9090/-/reload
      --config-file=/etc/prometheus/config/prometheus.yaml.gz
      --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
    State:          Running
      Started:      Wed, 13 May 2020 13:02:14 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:     100m
      memory:  25Mi
    Environment:
      POD_NAME:  prometheus-k8s-0 (v1:metadata.name)
    Mounts:
      /etc/prometheus/config from config (rw)
      /etc/prometheus/config_out from config-out (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  rules-configmap-reloader:
    Container ID:  cri-o://3a3addcc28f1bbd2b51a978e58c304d1d9aa813b645d2c608b4162975205e755
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      --webhook-url=http://localhost:9090/-/reload
      --volume-dir=/etc/prometheus/rules/prometheus-k8s-rulefiles-0
    State:          Running
      Started:      Wed, 13 May 2020 13:02:17 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/prometheus/rules/prometheus-k8s-rulefiles-0 from prometheus-k8s-rulefiles-0 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  thanos-sidecar:
    Container ID:  cri-o://891658f08121ad50cc0b6db372c2f46e6bfac8ec3ea26e86a859c675b78730f9
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Ports:         10902/TCP, 10901/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      sidecar
      --prometheus.url=http://localhost:9090/
      --tsdb.path=/prometheus
      --grpc-address=[$(POD_IP)]:10901
      --http-address=127.0.0.1:10902
      --grpc-server-tls-cert=/etc/tls/grpc/server.crt
      --grpc-server-tls-key=/etc/tls/grpc/server.key
      --grpc-server-tls-client-ca=/etc/tls/grpc/ca.crt
    State:          Running
      Started:      Wed, 13 May 2020 13:02:21 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     50m
      memory:  100Mi
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /etc/tls/grpc from secret-grpc-tls (rw)
      /prometheus from prometheus-k8s-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prometheus-proxy:
    Container ID:  cri-o://6d7a5f54fb4fd17a73ad2acd748cb81f6a3f4eb0ca1ff985f806b3726769d0f2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9091/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9091
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9090
      -htpasswd-file=/etc/proxy/htpasswd/auth
      -openshift-service-account=prometheus-k8s
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 13:02:21 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from prometheus-trusted-ca-bundle (ro)
      /etc/proxy/htpasswd from secret-prometheus-k8s-htpasswd (rw)
      /etc/proxy/secrets from secret-prometheus-k8s-proxy (rw)
      /etc/tls/private from secret-prometheus-k8s-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://1acd617b9b65e21b394f2140d8dcb98f1bf91f696b56aded7d4fb275b54f525e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9092/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:9092
      --upstream=http://127.0.0.1:9095
      --config-file=/etc/kube-rbac-proxy/config.yaml
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --logtostderr=true
      --v=10
    State:          Running
      Started:      Wed, 13 May 2020 13:02:21 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/kube-rbac-proxy from secret-kube-rbac-proxy (rw)
      /etc/tls/private from secret-prometheus-k8s-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prom-label-proxy:
    Container ID:  cri-o://74b381e0cbbb9004b8f24f28cc7b3ddf68c7fd37f0d64875b3267a5ab760ea4a
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Port:          <none>
    Host Port:     <none>
    Args:
      --insecure-listen-address=127.0.0.1:9095
      --upstream=http://127.0.0.1:9090
      --label=namespace
    State:          Running
      Started:      Wed, 13 May 2020 13:02:24 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s
    Optional:    false
  tls-assets:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-tls-assets
    Optional:    false
  config-out:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  prometheus-k8s-rulefiles-0:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-k8s-rulefiles-0
    Optional:  false
  secret-kube-etcd-client-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-etcd-client-certs
    Optional:    false
  secret-prometheus-k8s-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-tls
    Optional:    false
  secret-prometheus-k8s-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-proxy
    Optional:    false
  secret-prometheus-k8s-htpasswd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-htpasswd
    Optional:    false
  secret-kube-rbac-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-rbac-proxy
    Optional:    false
  configmap-serving-certs-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      serving-certs-ca-bundle
    Optional:  false
  configmap-kubelet-serving-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kubelet-serving-ca-bundle
    Optional:  false
  prometheus-k8s-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  secret-grpc-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-grpc-tls-3nnbpcbn3l222
    Optional:    false
  prometheus-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  prometheus-k8s-token-s7l5l:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-token-s7l5l
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                      prometheus-k8s-1
Namespace:                 openshift-monitoring
Priority:                  2000000000
Priority Class Name:       system-cluster-critical
Node:                      ibmworker01.os4.ringen.us/192.168.93.43
Start Time:                Wed, 13 May 2020 13:01:19 -0400
Labels:                    app=prometheus
                           controller-revision-hash=prometheus-k8s-64f6b46cb4
                           prometheus=k8s
                           statefulset.kubernetes.io/pod-name=prometheus-k8s-1
Annotations:               k8s.v1.cni.cncf.io/networks-status:
                             [{
                                 "name": "openshift-sdn",
                                 "interface": "eth0",
                                 "ips": [
                                     "10.241.1.2"
                                 ],
                                 "dns": {},
                                 "default-route": [
                                     "10.241.1.1"
                                 ]
                             }]
                           openshift.io/scc: restricted
Status:                    Terminating (lasts 8h)
Termination Grace Period:  600s
IP:                        10.241.1.2
IPs:
  IP:           10.241.1.2
Controlled By:  StatefulSet/prometheus-k8s
Containers:
  prometheus:
    Container ID:  cri-o://cf71e8458c17756a2c6d6c6ce0948af4f79f9424041b59da684c4d94f07c8537
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3179c1d9d735d1781dbed24c9e1aa48bada2a417823fed02630467fa26fe6701
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3179c1d9d735d1781dbed24c9e1aa48bada2a417823fed02630467fa26fe6701
    Port:          <none>
    Host Port:     <none>
    Args:
      --web.console.templates=/etc/prometheus/consoles
      --web.console.libraries=/etc/prometheus/console_libraries
      --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      --storage.tsdb.path=/prometheus
      --storage.tsdb.retention.time=15d
      --web.enable-lifecycle
      --storage.tsdb.no-lockfile
      --web.external-url=https://prometheus-k8s-openshift-monitoring.apps.os4.ringen.us/
      --web.route-prefix=/
      --web.listen-address=127.0.0.1:9090
    State:       Running
      Started:   Wed, 13 May 2020 13:01:51 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   aller=main.go:657 msg="Starting TSDB ..."
level=info ts=2020-05-13T17:01:30.529Z caller=web.go:496 component=web msg="Start listening for connections" address=127.0.0.1:9090
level=info ts=2020-05-13T17:01:30.536Z caller=head.go:535 component=tsdb msg="replaying WAL, this may take awhile"
level=info ts=2020-05-13T17:01:30.536Z caller=head.go:583 component=tsdb msg="WAL segment loaded" segment=0 maxSegment=0
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:672 fs_type=EXT4_SUPER_MAGIC
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:673 msg="TSDB started"
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:743 msg="Loading configuration file" filename=/etc/prometheus/config_out/prometheus.env.yaml
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:526 msg="Stopping scrape discovery manager..."
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:540 msg="Stopping notify discovery manager..."
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:562 msg="Stopping scrape manager..."
level=info ts=2020-05-13T17:01:30.538Z caller=manager.go:814 component="rule manager" msg="Stopping rule manager..."
level=info ts=2020-05-13T17:01:30.538Z caller=manager.go:820 component="rule manager" msg="Rule manager stopped"
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:522 msg="Scrape discovery manager stopped"
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:556 msg="Scrape manager stopped"
level=info ts=2020-05-13T17:01:30.538Z caller=main.go:536 msg="Notify discovery manager stopped"
level=info ts=2020-05-13T17:01:30.541Z caller=notifier.go:602 component=notifier msg="Stopping notification manager..."
level=info ts=2020-05-13T17:01:30.541Z caller=main.go:727 msg="Notifier manager stopped"
level=error ts=2020-05-13
      Exit Code:    1
      Started:      Wed, 13 May 2020 13:01:30 -0400
      Finished:     Wed, 13 May 2020 13:01:30 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        200m
      memory:     1Gi
    Liveness:     exec [sh -c if [ -x "$(command -v curl)" ]; then curl http://localhost:9090/-/healthy; elif [ -x "$(command -v wget)" ]; then wget -q http://localhost:9090/-/healthy; else exit 1; fi] delay=0s timeout=3s period=5s #success=1 #failure=6
    Readiness:    exec [sh -c if [ -x "$(command -v curl)" ]; then curl http://localhost:9090/-/ready; elif [ -x "$(command -v wget)" ]; then wget -q http://localhost:9090/-/ready; else exit 1; fi] delay=0s timeout=3s period=5s #success=1 #failure=120
    Environment:  <none>
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from prometheus-trusted-ca-bundle (ro)
      /etc/prometheus/certs from tls-assets (ro)
      /etc/prometheus/config_out from config-out (ro)
      /etc/prometheus/configmaps/kubelet-serving-ca-bundle from configmap-kubelet-serving-ca-bundle (ro)
      /etc/prometheus/configmaps/serving-certs-ca-bundle from configmap-serving-certs-ca-bundle (ro)
      /etc/prometheus/rules/prometheus-k8s-rulefiles-0 from prometheus-k8s-rulefiles-0 (rw)
      /etc/prometheus/secrets/kube-etcd-client-certs from secret-kube-etcd-client-certs (ro)
      /etc/prometheus/secrets/kube-rbac-proxy from secret-kube-rbac-proxy (ro)
      /etc/prometheus/secrets/prometheus-k8s-htpasswd from secret-prometheus-k8s-htpasswd (ro)
      /etc/prometheus/secrets/prometheus-k8s-proxy from secret-prometheus-k8s-proxy (ro)
      /etc/prometheus/secrets/prometheus-k8s-tls from secret-prometheus-k8s-tls (ro)
      /prometheus from prometheus-k8s-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prometheus-config-reloader:
    Container ID:  cri-o://e091076aea910f0a9a1d0703ed5607da757a3bf1ddc57e69b91bef80c19aea42
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/prometheus-config-reloader
    Args:
      --log-format=logfmt
      --reload-url=http://localhost:9090/-/reload
      --config-file=/etc/prometheus/config/prometheus.yaml.gz
      --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
    State:          Running
      Started:      Wed, 13 May 2020 13:01:33 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:     100m
      memory:  25Mi
    Environment:
      POD_NAME:  prometheus-k8s-1 (v1:metadata.name)
    Mounts:
      /etc/prometheus/config from config (rw)
      /etc/prometheus/config_out from config-out (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  rules-configmap-reloader:
    Container ID:  cri-o://57c5dc6e7551c64f0c9e5c1ab1a5f4e5eb9219d7e80bdf13d04c0b0a01ee59a8
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      --webhook-url=http://localhost:9090/-/reload
      --volume-dir=/etc/prometheus/rules/prometheus-k8s-rulefiles-0
    State:          Running
      Started:      Wed, 13 May 2020 13:01:37 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/prometheus/rules/prometheus-k8s-rulefiles-0 from prometheus-k8s-rulefiles-0 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  thanos-sidecar:
    Container ID:  cri-o://adf86544afca1fefe3e35999ed1783f34fe1312a5f31a1cba8bdfce8a8653a72
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Ports:         10902/TCP, 10901/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      sidecar
      --prometheus.url=http://localhost:9090/
      --tsdb.path=/prometheus
      --grpc-address=[$(POD_IP)]:10901
      --http-address=127.0.0.1:10902
      --grpc-server-tls-cert=/etc/tls/grpc/server.crt
      --grpc-server-tls-key=/etc/tls/grpc/server.key
      --grpc-server-tls-client-ca=/etc/tls/grpc/ca.crt
    State:          Running
      Started:      Wed, 13 May 2020 13:01:41 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     50m
      memory:  100Mi
    Environment:
      POD_IP:   (v1:status.podIP)
    Mounts:
      /etc/tls/grpc from secret-grpc-tls (rw)
      /prometheus from prometheus-k8s-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prometheus-proxy:
    Container ID:  cri-o://747b1e4baaefe911ca48603837e6e389486d88d51c600ebaee518a1cf33cbaab
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9091/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9091
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9090
      -htpasswd-file=/etc/proxy/htpasswd/auth
      -openshift-service-account=prometheus-k8s
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 13:01:42 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from prometheus-trusted-ca-bundle (ro)
      /etc/proxy/htpasswd from secret-prometheus-k8s-htpasswd (rw)
      /etc/proxy/secrets from secret-prometheus-k8s-proxy (rw)
      /etc/tls/private from secret-prometheus-k8s-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://01b06bae39436747b24db6d91f4b43da559ac89e1fb284c332080cf3352eabca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9092/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:9092
      --upstream=http://127.0.0.1:9095
      --config-file=/etc/kube-rbac-proxy/config.yaml
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --logtostderr=true
      --v=10
    State:          Running
      Started:      Wed, 13 May 2020 13:01:42 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/kube-rbac-proxy from secret-kube-rbac-proxy (rw)
      /etc/tls/private from secret-prometheus-k8s-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
  prom-label-proxy:
    Container ID:  cri-o://437397b587fbd3c8887b79d952eebbe10ba56fc0437d2a9bfe35ca66012e530c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Port:          <none>
    Host Port:     <none>
    Args:
      --insecure-listen-address=127.0.0.1:9095
      --upstream=http://127.0.0.1:9090
      --label=namespace
    State:          Running
      Started:      Wed, 13 May 2020 13:01:50 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-k8s-token-s7l5l (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s
    Optional:    false
  tls-assets:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-tls-assets
    Optional:    false
  config-out:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  prometheus-k8s-rulefiles-0:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-k8s-rulefiles-0
    Optional:  false
  secret-kube-etcd-client-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-etcd-client-certs
    Optional:    false
  secret-prometheus-k8s-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-tls
    Optional:    false
  secret-prometheus-k8s-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-proxy
    Optional:    false
  secret-prometheus-k8s-htpasswd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-htpasswd
    Optional:    false
  secret-kube-rbac-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-rbac-proxy
    Optional:    false
  configmap-serving-certs-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      serving-certs-ca-bundle
    Optional:  false
  configmap-kubelet-serving-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kubelet-serving-ca-bundle
    Optional:  false
  prometheus-k8s-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  secret-grpc-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-grpc-tls-3nnbpcbn3l222
    Optional:    false
  prometheus-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  prometheus-k8s-token-s7l5l:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-k8s-token-s7l5l
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 prometheus-operator-84cc8585dc-9tlzd
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:02 -0400
Labels:               app.kubernetes.io/component=controller
                      app.kubernetes.io/name=prometheus-operator
                      app.kubernetes.io/version=v0.34.1
                      pod-template-hash=84cc8585dc
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.79"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.79
IPs:
  IP:           10.241.0.79
Controlled By:  ReplicaSet/prometheus-operator-84cc8585dc
Containers:
  prometheus-operator:
    Container ID:  cri-o://c3bedaf339adbedfff4878fd992deb46bd052276b5c5ef7a06cd04d930ea8071
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab76ddf988c85962887de6705d61442c6105da166466ac5c916c34d873d55b2a
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ab76ddf988c85962887de6705d61442c6105da166466ac5c916c34d873d55b2a
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-service=kube-system/kubelet
      --logtostderr=true
      --config-reloader-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
      --prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0f9634fd0beda0488cffb7139654df7dff6cff66f85c5f940568ee076887b97e
      --namespaces=openshift-apiserver,openshift-apiserver-operator,openshift-authentication,openshift-authentication-operator,openshift-cloud-credential-operator,openshift-cluster-machine-approver,openshift-cluster-samples-operator,openshift-cluster-version,openshift-console-operator,openshift-controller-manager,openshift-controller-manager-operator,openshift-dns,openshift-dns-operator,openshift-image-registry,openshift-ingress,openshift-ingress-operator,openshift-insights,openshift-kube-apiserver,openshift-kube-apiserver-operator,openshift-kube-controller-manager,openshift-kube-controller-manager-operator,openshift-kube-scheduler,openshift-kube-scheduler-operator,openshift-machine-api,openshift-machine-config-operator,openshift-marketplace,openshift-monitoring,openshift-multus,openshift-operator-lifecycle-manager,openshift-sdn,openshift-service-catalog-apiserver-operator,openshift-service-catalog-controller-manager-operator,openshift-user-workload-monitoring
      --prometheus-instance-namespaces=openshift-monitoring
      --alertmanager-instance-namespaces=openshift-monitoring
    State:          Running
      Started:      Wed, 13 May 2020 12:45:07 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     60Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from prometheus-operator-token-rwwln (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  prometheus-operator-token-rwwln:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-operator-token-rwwln
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 telemeter-client-67bcd9659b-t2fx5
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:02 -0400
Labels:               k8s-app=telemeter-client
                      pod-template-hash=67bcd9659b
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.80"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.80
IPs:
  IP:           10.241.0.80
Controlled By:  ReplicaSet/telemeter-client-67bcd9659b
Containers:
  telemeter-client:
    Container ID:  cri-o://d78d958a667363a7aa456379bdd0e5106f84114ceff237afab3319b3c8539a48
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2ad52c7156e09479f1a931868a7576cf9dc979bd0280abfa15d5a35dcfaec7c5
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2ad52c7156e09479f1a931868a7576cf9dc979bd0280abfa15d5a35dcfaec7c5
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /usr/bin/telemeter-client
      --id=$(ID)
      --from=$(FROM)
      --from-ca-file=/etc/serving-certs-ca-bundle/service-ca.crt
      --from-token-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      --to=$(TO)
      --to-token-file=/etc/telemeter/token
      --listen=localhost:8080
      --anonymize-salt-file=/etc/telemeter/salt
      --anonymize-labels=$(ANONYMIZE_LABELS)
      --match={__name__=~"cluster:usage:.*"}
      --match={__name__="count:up0"}
      --match={__name__="count:up1"}
      --match={__name__="cluster_version"}
      --match={__name__="cluster_version_available_updates"}
      --match={__name__="cluster_operator_up"}
      --match={__name__="cluster_operator_conditions"}
      --match={__name__="cluster_version_payload"}
      --match={__name__="cluster_installer"}
      --match={__name__="cluster_infrastructure_provider"}
      --match={__name__="cluster_feature_set"}
      --match={__name__="instance:etcd_object_counts:sum"}
      --match={__name__="ALERTS",alertstate="firing"}
      --match={__name__="code:apiserver_request_count:rate:sum"}
      --match={__name__="cluster:capacity_cpu_cores:sum"}
      --match={__name__="cluster:capacity_memory_bytes:sum"}
      --match={__name__="cluster:cpu_usage_cores:sum"}
      --match={__name__="cluster:memory_usage_bytes:sum"}
      --match={__name__="openshift:cpu_usage_cores:sum"}
      --match={__name__="openshift:memory_usage_bytes:sum"}
      --match={__name__="workload:cpu_usage_cores:sum"}
      --match={__name__="workload:memory_usage_bytes:sum"}
      --match={__name__="cluster:virt_platform_nodes:sum"}
      --match={__name__="cluster:node_instance_type_count:sum"}
      --match={__name__="cnv:vmi_status_running:count"}
      --match={__name__="node_role_os_version_machine:cpu_capacity_cores:sum"}
      --match={__name__="node_role_os_version_machine:cpu_capacity_sockets:sum"}
      --match={__name__="subscription_sync_total"}
      --match={__name__="csv_succeeded"}
      --match={__name__="csv_abnormal"}
      --match={__name__="ceph_cluster_total_bytes"}
      --match={__name__="ceph_cluster_total_used_raw_bytes"}
      --match={__name__="ceph_health_status"}
      --match={__name__="job:ceph_osd_metadata:count"}
      --match={__name__="job:kube_pv:count"}
      --match={__name__="job:ceph_pools_iops:total"}
      --match={__name__="job:ceph_pools_iops_bytes:total"}
      --match={__name__="job:ceph_versions_running:count"}
      --match={__name__="job:noobaa_total_unhealthy_buckets:sum"}
      --match={__name__="job:noobaa_bucket_count:sum"}
      --match={__name__="job:noobaa_total_object_count:sum"}
      --match={__name__="noobaa_accounts_num"}
      --match={__name__="noobaa_total_usage"}
      --match={__name__="console_url"}
      --match={__name__="cluster:network_attachment_definition_instances:max"}
      --match={__name__="cluster:network_attachment_definition_enabled_instance_up:max"}
      --limit-bytes=5242880
    State:          Running
      Started:      Wed, 13 May 2020 12:45:08 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      ANONYMIZE_LABELS:  
      FROM:              https://prometheus-k8s.openshift-monitoring.svc:9091
      ID:                63bab5a5-6d44-49fb-aaae-d858e55d3eab
      TO:                https://infogw.api.openshift.com
      HTTP_PROXY:        
      HTTPS_PROXY:       
      NO_PROXY:          
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from telemeter-trusted-ca-bundle (ro)
      /etc/serving-certs-ca-bundle from serving-certs-ca-bundle (rw)
      /etc/telemeter from secret-telemeter-client (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from telemeter-client-token-rzrbp (ro)
  reload:
    Container ID:  cri-o://291d82548bd6e6a6b719a782ff04a81f1397068a58cac20d61485ed8691a8fc6
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:20018d0ee15dffead505711e5ca6261822c9b3554a051b6e6cd2365136cd5330
    Port:          <none>
    Host Port:     <none>
    Args:
      --webhook-url=http://localhost:8080/-/reload
      --volume-dir=/etc/serving-certs-ca-bundle
    State:          Running
      Started:      Wed, 13 May 2020 12:45:11 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/serving-certs-ca-bundle from serving-certs-ca-bundle (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from telemeter-client-token-rzrbp (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://387a0dd665df03cea2c0df322d572ac80bef9ad5413a38c7d5708edb8a96220b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=:8443
      --upstream=http://127.0.0.1:8080/
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
    State:          Running
      Started:      Wed, 13 May 2020 12:45:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/tls/private from telemeter-client-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from telemeter-client-token-rzrbp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-certs-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      telemeter-client-serving-certs-ca-bundle
    Optional:  false
  secret-telemeter-client:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  telemeter-client
    Optional:    false
  telemeter-client-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  telemeter-client-tls
    Optional:    false
  telemeter-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      telemeter-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  telemeter-client-token-rzrbp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  telemeter-client-token-rzrbp
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 thanos-querier-78bc465589-dtxss
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:46:09 -0400
Labels:               app.kubernetes.io/name=thanos-querier
                      pod-template-hash=78bc465589
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.92"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.92
IPs:
  IP:           10.241.0.92
Controlled By:  ReplicaSet/thanos-querier-78bc465589
Containers:
  thanos-querier:
    Container ID:  cri-o://efef51ae4a9836d922ccc2a3ce5c0a3fc85cf10bc9208710a476ae2d6957d261
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Port:          <none>
    Host Port:     <none>
    Args:
      query
      --query.replica-label=prometheus_replica
      --grpc-address=127.0.0.1:10901
      --http-address=127.0.0.1:9090
      --grpc-client-tls-secure
      --grpc-client-tls-cert=/etc/tls/grpc/client.crt
      --grpc-client-tls-key=/etc/tls/grpc/client.key
      --grpc-client-tls-ca=/etc/tls/grpc/ca.crt
      --grpc-client-server-name=prometheus-grpc
      --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
    State:          Running
      Started:      Wed, 13 May 2020 12:46:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     12Mi
    Liveness:     exec [sh -c curl http://localhost:9090/-/healthy] delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    exec [sh -c curl http://localhost:9090/-/healthy] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/tls/grpc from secret-grpc-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  oauth-proxy:
    Container ID:  cri-o://213205a64b8868373663991625fa29694bb1a5701b5c5d03be44f8e372cc5e3f
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9091/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9091
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9090
      -htpasswd-file=/etc/proxy/htpasswd/auth
      -openshift-service-account=thanos-querier
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:46:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from thanos-querier-trusted-ca-bundle (ro)
      /etc/proxy/htpasswd from secret-thanos-querier-oauth-htpasswd (rw)
      /etc/proxy/secrets from secret-thanos-querier-oauth-cookie (rw)
      /etc/tls/private from secret-thanos-querier-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://24033fa6b8547a7e7acef84bde9aa6634c713e5a34bb0a9b7a934090b2de2eff
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9092/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:9092
      --upstream=http://127.0.0.1:9095
      --config-file=/etc/kube-rbac-proxy/config.yaml
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --logtostderr=true
    State:          Running
      Started:      Wed, 13 May 2020 12:46:14 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/kube-rbac-proxy from secret-thanos-querier-kube-rbac-proxy (rw)
      /etc/tls/private from secret-thanos-querier-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  prom-label-proxy:
    Container ID:  cri-o://207b5f523132ca891cec87a49433c3b33e8173726f3ad9dd656e75c91dc3fa5d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Port:          <none>
    Host Port:     <none>
    Args:
      --insecure-listen-address=127.0.0.1:9095
      --upstream=http://127.0.0.1:9090
      --label=namespace
    State:          Running
      Started:      Wed, 13 May 2020 12:46:14 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  secret-thanos-querier-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-tls
    Optional:    false
  secret-thanos-querier-oauth-cookie:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-oauth-cookie
    Optional:    false
  secret-thanos-querier-oauth-htpasswd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-oauth-htpasswd
    Optional:    false
  secret-thanos-querier-kube-rbac-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-kube-rbac-proxy
    Optional:    false
  secret-grpc-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-grpc-tls-a41ms1mik7ilm
    Optional:    false
  thanos-querier-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      thanos-querier-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  thanos-querier-token-wq5sg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-token-wq5sg
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 thanos-querier-78bc465589-kntjz
Namespace:            openshift-monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:45:57 -0400
Labels:               app.kubernetes.io/name=thanos-querier
                      pod-template-hash=78bc465589
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.91"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: restricted
Status:               Running
IP:                   10.241.0.91
IPs:
  IP:           10.241.0.91
Controlled By:  ReplicaSet/thanos-querier-78bc465589
Containers:
  thanos-querier:
    Container ID:  cri-o://8e2669c146e4cd4415d95f4925dbb8e123e43c95e3a752b000a87b05d72dbbbf
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b6937f9a576a40b5cf717a92cadd91ca56f39ed7241478ae096589f703afe61d
    Port:          <none>
    Host Port:     <none>
    Args:
      query
      --query.replica-label=prometheus_replica
      --grpc-address=127.0.0.1:10901
      --http-address=127.0.0.1:9090
      --grpc-client-tls-secure
      --grpc-client-tls-cert=/etc/tls/grpc/client.crt
      --grpc-client-tls-key=/etc/tls/grpc/client.key
      --grpc-client-tls-ca=/etc/tls/grpc/ca.crt
      --grpc-client-server-name=prometheus-grpc
      --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
    State:          Running
      Started:      Wed, 13 May 2020 12:46:01 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     12Mi
    Liveness:     exec [sh -c curl http://localhost:9090/-/healthy] delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    exec [sh -c curl http://localhost:9090/-/healthy] delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/tls/grpc from secret-grpc-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  oauth-proxy:
    Container ID:  cri-o://29cd386d7b9f88d7282927ac4b7256dec776139173b6078369df7b9b55183691
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:308dd6d12e25b8f1c91c4dcaaa4795804f0d9ceb9dfac8a890f3a5e73cfb903d
    Port:          9091/TCP
    Host Port:     0/TCP
    Args:
      -provider=openshift
      -https-address=:9091
      -http-address=
      -email-domain=*
      -upstream=http://localhost:9090
      -htpasswd-file=/etc/proxy/htpasswd/auth
      -openshift-service-account=thanos-querier
      -openshift-sar={"resource": "namespaces", "verb": "get"}
      -openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}
      -tls-cert=/etc/tls/private/tls.crt
      -tls-key=/etc/tls/private/tls.key
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret-file=/etc/proxy/secrets/session_secret
      -openshift-ca=/etc/pki/tls/cert.pem
      -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      -skip-auth-regex=^/metrics
    State:          Running
      Started:      Wed, 13 May 2020 12:46:01 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  20Mi
    Environment:
      HTTP_PROXY:   
      HTTPS_PROXY:  
      NO_PROXY:     
    Mounts:
      /etc/pki/ca-trust/extracted/pem/ from thanos-querier-trusted-ca-bundle (ro)
      /etc/proxy/htpasswd from secret-thanos-querier-oauth-htpasswd (rw)
      /etc/proxy/secrets from secret-thanos-querier-oauth-cookie (rw)
      /etc/tls/private from secret-thanos-querier-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  kube-rbac-proxy:
    Container ID:  cri-o://20b6d90135ecf1e537ea39317be932a467974d8b4aeea36e166d2f31181227ca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b4657fc9efc597c2651fb0934f7e27c09940cec5540d48b65165667ed2d78242
    Port:          9092/TCP
    Host Port:     0/TCP
    Args:
      --secure-listen-address=0.0.0.0:9092
      --upstream=http://127.0.0.1:9095
      --config-file=/etc/kube-rbac-proxy/config.yaml
      --tls-cert-file=/etc/tls/private/tls.crt
      --tls-private-key-file=/etc/tls/private/tls.key
      --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      --logtostderr=true
    State:          Running
      Started:      Wed, 13 May 2020 12:46:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /etc/kube-rbac-proxy from secret-thanos-querier-kube-rbac-proxy (rw)
      /etc/tls/private from secret-thanos-querier-tls (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
  prom-label-proxy:
    Container ID:  cri-o://690d263dec4cd24e51bc6b6a5d456408b47f22d776a31119c6ef2f36b5283ef7
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a45355006cafaa7591faff4f022f7bad3806bccff502bc2161b6fb5c49779354
    Port:          <none>
    Host Port:     <none>
    Args:
      --insecure-listen-address=127.0.0.1:9095
      --upstream=http://127.0.0.1:9090
      --label=namespace
    State:          Running
      Started:      Wed, 13 May 2020 12:46:03 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     20Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from thanos-querier-token-wq5sg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  secret-thanos-querier-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-tls
    Optional:    false
  secret-thanos-querier-oauth-cookie:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-oauth-cookie
    Optional:    false
  secret-thanos-querier-oauth-htpasswd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-oauth-htpasswd
    Optional:    false
  secret-thanos-querier-kube-rbac-proxy:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-kube-rbac-proxy
    Optional:    false
  secret-grpc-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-grpc-tls-a41ms1mik7ilm
    Optional:    false
  thanos-querier-trusted-ca-bundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      thanos-querier-trusted-ca-bundle-39man1pbaa8jq
    Optional:  true
  thanos-querier-token-wq5sg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  thanos-querier-token-wq5sg
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 multus-4f5zw
Namespace:            openshift-multus
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:00:39 -0400
Labels:               app=multus
                      component=network
                      controller-revision-hash=8c59f758b
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/multus
Init Containers:
  multus-binary-copy:
    Container ID:  cri-o://1fa3b197b5c9dc6c1bd9c5cc0eb144d9e2ca38ecdb9eef62082d885353edfacc
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:01 -0400
      Finished:     Wed, 13 May 2020 13:01:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/multus-cni/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  cni-plugins:
    Container ID:  cri-o://3b77c50d9a34bd8229ade832e5ba860085c6efc83fe9e36ddd8ce37aa7f015ab
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:02 -0400
      Finished:     Wed, 13 May 2020 13:01:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/plugins/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/plugins/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/plugins/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  routeoverride-cni:
    Container ID:  cri-o://9e435ed4607ed8c0bda9ac11571db7898122e8889ca0cc2e16132b1f2c75e1f4
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:06 -0400
      Finished:     Wed, 13 May 2020 13:01:06 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/route-override/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/route-override/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni-bincopy:
    Container ID:  cri-o://db0a6437237286c446b1db22a09ca73d90b8063a0dc682feabd578fe05840098
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:13 -0400
      Finished:     Wed, 13 May 2020 13:01:13 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/whereabouts/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni:
    Container ID:  cri-o://b84229a24990f3c23f8191e8f93453d1f533823418620daaa8a3cceec588d305
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      #!/bin/sh
      
      set -u -e
      
      CNI_BIN_DIR=${CNI_BIN_DIR:-"/host/opt/cni/bin/"}
      WHEREABOUTS_KUBECONFIG_FILE_HOST=${WHEREABOUTS_KUBECONFIG_FILE_HOST:-"/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig"}
      CNI_CONF_DIR=${CNI_CONF_DIR:-"/host/etc/cni/net.d"}
      
      # Make a whereabouts.d directory (for our kubeconfig)
      
      mkdir -p $CNI_CONF_DIR/whereabouts.d
      WHEREABOUTS_KUBECONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.kubeconfig
      WHEREABOUTS_GLOBALCONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.conf
      
      # ------------------------------- Generate a "kube-config"
      SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
      KUBE_CA_FILE=${KUBE_CA_FILE:-$SERVICE_ACCOUNT_PATH/ca.crt}
      SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
      SKIP_TLS_VERIFY=${SKIP_TLS_VERIFY:-false}
      
      
      # Check if we're running as a k8s pod.
      if [ -f "$SERVICE_ACCOUNT_PATH/token" ]; then
        # We're running as a k8d pod - expect some variables.
        if [ -z ${KUBERNETES_SERVICE_HOST} ]; then
          error "KUBERNETES_SERVICE_HOST not set"; exit 1;
        fi
        if [ -z ${KUBERNETES_SERVICE_PORT} ]; then
          error "KUBERNETES_SERVICE_PORT not set"; exit 1;
        fi
      
        if [ "$SKIP_TLS_VERIFY" == "true" ]; then
          TLS_CFG="insecure-skip-tls-verify: true"
        elif [ -f "$KUBE_CA_FILE" ]; then
          TLS_CFG="certificate-authority-data: $(cat $KUBE_CA_FILE | base64 | tr -d '\n')"
        fi
      
        # Write a kubeconfig file for the CNI plugin.  Do this
        # to skip TLS verification for now.  We should eventually support
        # writing more complete kubeconfig files. This is only used
        # if the provided CNI network config references it.
        touch $WHEREABOUTS_KUBECONFIG
        chmod ${KUBECONFIG_MODE:-600} $WHEREABOUTS_KUBECONFIG
        cat > $WHEREABOUTS_KUBECONFIG <<EOF
      # Kubeconfig file for Multus CNI plugin.
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: ${KUBERNETES_SERVICE_PROTOCOL:-https}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}
          $TLS_CFG
      users:
      - name: whereabouts
        user:
          token: "${SERVICEACCOUNT_TOKEN}"
      contexts:
      - name: whereabouts-context
        context:
          cluster: local
          user: whereabouts
          namespace: ${WHEREABOUTS_NAMESPACE}
      current-context: whereabouts-context
      EOF
      
      # Kubeconfig file for Multus CNI plugin.
      cat > $WHEREABOUTS_GLOBALCONFIG <<EOF
      {
        "datastore": "kubernetes",
        "kubernetes": {
          "kubeconfig": "/etc/kubernetes/cni/net.d/whereabouts.d/whereabouts.kubeconfig"
        },
        "log_level": "debug"
      }
      EOF
      
      else
        warn "Doesn't look like we're running in a kubernetes environment (no serviceaccount token)"
      fi
      
      # copy whereabouts to the cni bin dir
      # SKIPPED DUE TO FIPS COPY.
      # cp -f /whereabouts $CNI_BIN_DIR
      
      # ---------------------- end Generate a "kube-config".
      
      # Unless told otherwise, sleep forever.
      # This prevents Kubernetes from restarting the pod repeatedly.
      should_sleep=${SLEEP:-"true"}
      echo "Done configuring CNI.  Sleep=$should_sleep"
      while [ "$should_sleep" == "true"  ]; do
          sleep 1000000000000
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:14 -0400
      Finished:     Wed, 13 May 2020 13:01:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      CNI_BIN_DIR:              /host/opt/cni/bin/
      CNI_CONF_DIR:             /host/etc/cni/net.d
      SLEEP:                    false
      WHEREABOUTS_NAMESPACE:    openshift-multus
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Containers:
  kube-multus:
    Container ID:  cri-o://0369549f0555405ac0309c086986c6d8461d80a93aec1d65a826453cf0fdbeca
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint.sh
    Args:
      --multus-conf-file=auto
      --multus-autoconfig-dir=/host/var/run/multus/cni/net.d
      --multus-kubeconfig-file-host=/etc/kubernetes/cni/net.d/multus.d/multus.kubeconfig
      --readiness-indicator-file=/var/run/multus/cni/net.d/80-openshift-network.conf
      --cleanup-config-on-exit=true
      --namespace-isolation=true
      --multus-log-level=verbose
      --cni-version=0.3.1
      --additional-bin-dir=/opt/multus/bin
      --skip-multus-binary-copy=true
    State:          Running
      Started:      Wed, 13 May 2020 13:01:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  150Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /host/var/run/multus/cni/net.d from multus-cni-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  system-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/cni/net.d
    HostPathType:  
  multus-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  cnibin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  os-release:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/os-release
    HostPathType:  File
  cni-binary-copy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cni-binary-copy-script
    Optional:  false
  multus-token-kwjpq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  multus-token-kwjpq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 multus-97k8h
Namespace:            openshift-multus
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:27:31 -0400
Labels:               app=multus
                      component=network
                      controller-revision-hash=8c59f758b
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/multus
Init Containers:
  multus-binary-copy:
    Container ID:  cri-o://0074c9dfc508155c4d73c50d41dc43ce8642a7e47077143fc77e0a9ea1c777f4
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:27:39 -0400
      Finished:     Wed, 13 May 2020 12:27:39 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/multus-cni/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  cni-plugins:
    Container ID:  cri-o://f6b6d94cfe5875f9fc886959e117579893c6c105fd7cd8bcaaf7f0ea89436b15
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:27:55 -0400
      Finished:     Wed, 13 May 2020 12:27:55 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/plugins/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/plugins/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/plugins/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  routeoverride-cni:
    Container ID:  cri-o://b1a9fb36f27cab0e73df4cbeb7e8d9fbf473b2bca2f4f5c01961e05647f8729d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:27:58 -0400
      Finished:     Wed, 13 May 2020 12:27:58 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/route-override/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/route-override/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni-bincopy:
    Container ID:  cri-o://37f409119af460a5e0bc642ee574dfa0be3a30f12e070dafdf865ecc594c01f7
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:28:03 -0400
      Finished:     Wed, 13 May 2020 12:28:04 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/whereabouts/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni:
    Container ID:  cri-o://e99a25382d38e74c99b702bdeeb42632060af9a532ad3d0974a3f817f646fbd5
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      #!/bin/sh
      
      set -u -e
      
      CNI_BIN_DIR=${CNI_BIN_DIR:-"/host/opt/cni/bin/"}
      WHEREABOUTS_KUBECONFIG_FILE_HOST=${WHEREABOUTS_KUBECONFIG_FILE_HOST:-"/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig"}
      CNI_CONF_DIR=${CNI_CONF_DIR:-"/host/etc/cni/net.d"}
      
      # Make a whereabouts.d directory (for our kubeconfig)
      
      mkdir -p $CNI_CONF_DIR/whereabouts.d
      WHEREABOUTS_KUBECONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.kubeconfig
      WHEREABOUTS_GLOBALCONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.conf
      
      # ------------------------------- Generate a "kube-config"
      SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
      KUBE_CA_FILE=${KUBE_CA_FILE:-$SERVICE_ACCOUNT_PATH/ca.crt}
      SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
      SKIP_TLS_VERIFY=${SKIP_TLS_VERIFY:-false}
      
      
      # Check if we're running as a k8s pod.
      if [ -f "$SERVICE_ACCOUNT_PATH/token" ]; then
        # We're running as a k8d pod - expect some variables.
        if [ -z ${KUBERNETES_SERVICE_HOST} ]; then
          error "KUBERNETES_SERVICE_HOST not set"; exit 1;
        fi
        if [ -z ${KUBERNETES_SERVICE_PORT} ]; then
          error "KUBERNETES_SERVICE_PORT not set"; exit 1;
        fi
      
        if [ "$SKIP_TLS_VERIFY" == "true" ]; then
          TLS_CFG="insecure-skip-tls-verify: true"
        elif [ -f "$KUBE_CA_FILE" ]; then
          TLS_CFG="certificate-authority-data: $(cat $KUBE_CA_FILE | base64 | tr -d '\n')"
        fi
      
        # Write a kubeconfig file for the CNI plugin.  Do this
        # to skip TLS verification for now.  We should eventually support
        # writing more complete kubeconfig files. This is only used
        # if the provided CNI network config references it.
        touch $WHEREABOUTS_KUBECONFIG
        chmod ${KUBECONFIG_MODE:-600} $WHEREABOUTS_KUBECONFIG
        cat > $WHEREABOUTS_KUBECONFIG <<EOF
      # Kubeconfig file for Multus CNI plugin.
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: ${KUBERNETES_SERVICE_PROTOCOL:-https}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}
          $TLS_CFG
      users:
      - name: whereabouts
        user:
          token: "${SERVICEACCOUNT_TOKEN}"
      contexts:
      - name: whereabouts-context
        context:
          cluster: local
          user: whereabouts
          namespace: ${WHEREABOUTS_NAMESPACE}
      current-context: whereabouts-context
      EOF
      
      # Kubeconfig file for Multus CNI plugin.
      cat > $WHEREABOUTS_GLOBALCONFIG <<EOF
      {
        "datastore": "kubernetes",
        "kubernetes": {
          "kubeconfig": "/etc/kubernetes/cni/net.d/whereabouts.d/whereabouts.kubeconfig"
        },
        "log_level": "debug"
      }
      EOF
      
      else
        warn "Doesn't look like we're running in a kubernetes environment (no serviceaccount token)"
      fi
      
      # copy whereabouts to the cni bin dir
      # SKIPPED DUE TO FIPS COPY.
      # cp -f /whereabouts $CNI_BIN_DIR
      
      # ---------------------- end Generate a "kube-config".
      
      # Unless told otherwise, sleep forever.
      # This prevents Kubernetes from restarting the pod repeatedly.
      should_sleep=${SLEEP:-"true"}
      echo "Done configuring CNI.  Sleep=$should_sleep"
      while [ "$should_sleep" == "true"  ]; do
          sleep 1000000000000
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:28:05 -0400
      Finished:     Wed, 13 May 2020 12:28:05 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      CNI_BIN_DIR:              /host/opt/cni/bin/
      CNI_CONF_DIR:             /host/etc/cni/net.d
      SLEEP:                    false
      WHEREABOUTS_NAMESPACE:    openshift-multus
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Containers:
  kube-multus:
    Container ID:  cri-o://91a62e359ebe618d2fe2c28f56c15c10968ddb37cffe11b3b80a8ed4370c83a5
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint.sh
    Args:
      --multus-conf-file=auto
      --multus-autoconfig-dir=/host/var/run/multus/cni/net.d
      --multus-kubeconfig-file-host=/etc/kubernetes/cni/net.d/multus.d/multus.kubeconfig
      --readiness-indicator-file=/var/run/multus/cni/net.d/80-openshift-network.conf
      --cleanup-config-on-exit=true
      --namespace-isolation=true
      --multus-log-level=verbose
      --cni-version=0.3.1
      --additional-bin-dir=/opt/multus/bin
      --skip-multus-binary-copy=true
    State:          Running
      Started:      Wed, 13 May 2020 12:28:05 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  150Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /host/var/run/multus/cni/net.d from multus-cni-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  system-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/cni/net.d
    HostPathType:  
  multus-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  cnibin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  os-release:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/os-release
    HostPathType:  File
  cni-binary-copy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cni-binary-copy-script
    Optional:  false
  multus-token-kwjpq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  multus-token-kwjpq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 multus-admission-controller-pgw48
Namespace:            openshift-multus
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=multus-admission-controller
                      component=network
                      controller-revision-hash=67477587f9
                      namespace=openshift-multus
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.14"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.14
IPs:
  IP:           10.241.0.14
Controlled By:  DaemonSet/multus-admission-controller
Containers:
  multus-admission-controller:
    Container ID:  cri-o://03c36e4d4acf0b6efc7395eab3a47c9b74714c0eae5c2653d7b3d585ad3c971e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5d7b9aa5360c759797d3f0c12885c414fdf8cfc64147e2dc9b5c88f1c4e8b56c
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5d7b9aa5360c759797d3f0c12885c414fdf8cfc64147e2dc9b5c88f1c4e8b56c
    Port:          9091/TCP
    Host Port:     0/TCP
    Command:
      /usr/bin/webhook
    Args:
      -bind-address=0.0.0.0
      -port=6443
      -tls-private-key-file=/etc/webhook/tls.key
      -tls-cert-file=/etc/webhook/tls.crt
      -alsologtostderr=true
      -metrics-listen-address=0.0.0.0:9091
    State:          Running
      Started:      Wed, 13 May 2020 12:31:58 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
    Environment:  <none>
    Mounts:
      /etc/webhook from webhook-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  webhook-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  multus-admission-controller-secret
    Optional:    false
  multus-token-kwjpq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  multus-token-kwjpq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 multus-cppl7
Namespace:            openshift-multus
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:00:49 -0400
Labels:               app=multus
                      component=network
                      controller-revision-hash=8c59f758b
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/multus
Init Containers:
  multus-binary-copy:
    Container ID:  cri-o://aa92dcb0f13f7eedbf2cc2e23062a701a537da5cdde4da9cd3f7e54cbf57c45c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:09 -0400
      Finished:     Wed, 13 May 2020 13:01:10 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/multus-cni/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/multus-cni/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  cni-plugins:
    Container ID:  cri-o://2153f307be6a421f0c0446c7efb8e3033968b809253eb6793038d027fbac7eae
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:11 -0400
      Finished:     Wed, 13 May 2020 13:01:11 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/plugins/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/plugins/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/plugins/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  routeoverride-cni:
    Container ID:  cri-o://7198ca2c5a402a7f901db5e30526bb746c4c7c8f16f60b1691287ad606b3e66c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:14 -0400
      Finished:     Wed, 13 May 2020 13:01:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/route-override/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/route-override/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni-bincopy:
    Container ID:  cri-o://d79c2aee74d6dd8171b20c4fa092af121d64cfeba38bc3db338babf73c15ddea
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint/cnibincopy.sh
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:21 -0400
      Finished:     Wed, 13 May 2020 13:01:21 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      RHEL7_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel7/bin/
      RHEL8_SOURCE_DIRECTORY:    /usr/src/whereabouts/rhel8/bin/
      DEFAULT_SOURCE_DIRECTORY:  /usr/src/whereabouts/bin/
    Mounts:
      /entrypoint from cni-binary-copy (rw)
      /host/etc/os-release from os-release (ro)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
  whereabouts-cni:
    Container ID:  cri-o://ec1e91e095bd5cac6c7c4580c65d1034d6413677facf1a38e32e545c1849e693
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      #!/bin/sh
      
      set -u -e
      
      CNI_BIN_DIR=${CNI_BIN_DIR:-"/host/opt/cni/bin/"}
      WHEREABOUTS_KUBECONFIG_FILE_HOST=${WHEREABOUTS_KUBECONFIG_FILE_HOST:-"/etc/cni/net.d/whereabouts.d/whereabouts.kubeconfig"}
      CNI_CONF_DIR=${CNI_CONF_DIR:-"/host/etc/cni/net.d"}
      
      # Make a whereabouts.d directory (for our kubeconfig)
      
      mkdir -p $CNI_CONF_DIR/whereabouts.d
      WHEREABOUTS_KUBECONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.kubeconfig
      WHEREABOUTS_GLOBALCONFIG=$CNI_CONF_DIR/whereabouts.d/whereabouts.conf
      
      # ------------------------------- Generate a "kube-config"
      SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
      KUBE_CA_FILE=${KUBE_CA_FILE:-$SERVICE_ACCOUNT_PATH/ca.crt}
      SERVICEACCOUNT_TOKEN=$(cat $SERVICE_ACCOUNT_PATH/token)
      SKIP_TLS_VERIFY=${SKIP_TLS_VERIFY:-false}
      
      
      # Check if we're running as a k8s pod.
      if [ -f "$SERVICE_ACCOUNT_PATH/token" ]; then
        # We're running as a k8d pod - expect some variables.
        if [ -z ${KUBERNETES_SERVICE_HOST} ]; then
          error "KUBERNETES_SERVICE_HOST not set"; exit 1;
        fi
        if [ -z ${KUBERNETES_SERVICE_PORT} ]; then
          error "KUBERNETES_SERVICE_PORT not set"; exit 1;
        fi
      
        if [ "$SKIP_TLS_VERIFY" == "true" ]; then
          TLS_CFG="insecure-skip-tls-verify: true"
        elif [ -f "$KUBE_CA_FILE" ]; then
          TLS_CFG="certificate-authority-data: $(cat $KUBE_CA_FILE | base64 | tr -d '\n')"
        fi
      
        # Write a kubeconfig file for the CNI plugin.  Do this
        # to skip TLS verification for now.  We should eventually support
        # writing more complete kubeconfig files. This is only used
        # if the provided CNI network config references it.
        touch $WHEREABOUTS_KUBECONFIG
        chmod ${KUBECONFIG_MODE:-600} $WHEREABOUTS_KUBECONFIG
        cat > $WHEREABOUTS_KUBECONFIG <<EOF
      # Kubeconfig file for Multus CNI plugin.
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: ${KUBERNETES_SERVICE_PROTOCOL:-https}://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}
          $TLS_CFG
      users:
      - name: whereabouts
        user:
          token: "${SERVICEACCOUNT_TOKEN}"
      contexts:
      - name: whereabouts-context
        context:
          cluster: local
          user: whereabouts
          namespace: ${WHEREABOUTS_NAMESPACE}
      current-context: whereabouts-context
      EOF
      
      # Kubeconfig file for Multus CNI plugin.
      cat > $WHEREABOUTS_GLOBALCONFIG <<EOF
      {
        "datastore": "kubernetes",
        "kubernetes": {
          "kubeconfig": "/etc/kubernetes/cni/net.d/whereabouts.d/whereabouts.kubeconfig"
        },
        "log_level": "debug"
      }
      EOF
      
      else
        warn "Doesn't look like we're running in a kubernetes environment (no serviceaccount token)"
      fi
      
      # copy whereabouts to the cni bin dir
      # SKIPPED DUE TO FIPS COPY.
      # cp -f /whereabouts $CNI_BIN_DIR
      
      # ---------------------- end Generate a "kube-config".
      
      # Unless told otherwise, sleep forever.
      # This prevents Kubernetes from restarting the pod repeatedly.
      should_sleep=${SLEEP:-"true"}
      echo "Done configuring CNI.  Sleep=$should_sleep"
      while [ "$should_sleep" == "true"  ]; do
          sleep 1000000000000
      done
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:22 -0400
      Finished:     Wed, 13 May 2020 13:01:22 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      CNI_BIN_DIR:              /host/opt/cni/bin/
      CNI_CONF_DIR:             /host/etc/cni/net.d
      SLEEP:                    false
      WHEREABOUTS_NAMESPACE:    openshift-multus
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Containers:
  kube-multus:
    Container ID:  cri-o://f42605969072b7328ac6475d0a3b8b43696d205c107a5fcabe752f1cfa4c03d1
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
    Port:          <none>
    Host Port:     <none>
    Command:
      /entrypoint.sh
    Args:
      --multus-conf-file=auto
      --multus-autoconfig-dir=/host/var/run/multus/cni/net.d
      --multus-kubeconfig-file-host=/etc/kubernetes/cni/net.d/multus.d/multus.kubeconfig
      --readiness-indicator-file=/var/run/multus/cni/net.d/80-openshift-network.conf
      --cleanup-config-on-exit=true
      --namespace-isolation=true
      --multus-log-level=verbose
      --cni-version=0.3.1
      --additional-bin-dir=/opt/multus/bin
      --skip-multus-binary-copy=true
    State:          Running
      Started:      Wed, 13 May 2020 13:01:23 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  150Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
    Mounts:
      /host/etc/cni/net.d from system-cni-dir (rw)
      /host/opt/cni/bin from cnibin (rw)
      /host/var/run/multus/cni/net.d from multus-cni-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from multus-token-kwjpq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  system-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/cni/net.d
    HostPathType:  
  multus-cni-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  cnibin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  os-release:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/os-release
    HostPathType:  File
  cni-binary-copy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cni-binary-copy-script
    Optional:  false
  multus-token-kwjpq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  multus-token-kwjpq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 network-operator-5b9f69556-wkp5p
Namespace:            openshift-network-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:27:10 -0400
Labels:               name=network-operator
                      pod-template-hash=5b9f69556
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  ReplicaSet/network-operator-5b9f69556
Containers:
  network-operator:
    Container ID:  cri-o://19546d49dfdc2b15ad7c65361e8a7e233be7fb182a62ea69f09cfc2091283e25
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4e498aab93a775b7c27f488b41a1b6033c674597a9809dc406f169ba74ad97bd
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4e498aab93a775b7c27f488b41a1b6033c674597a9809dc406f169ba74ad97bd
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/bin/cluster-network-operator
      --url-only-kubeconfig=/etc/kubernetes/kubeconfig
    State:          Running
      Started:      Wed, 13 May 2020 12:27:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:                    4.3.13
      SDN_IMAGE:                          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
      KUBE_PROXY_IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:be1bd05915437c24f47c2b49b80df26e0edaa853231d1acddc2e0128882efe5d
      MULTUS_IMAGE:                       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9992c0a9e96c8e2bb80c8807d205dfa762588a5690a6720bd56ce7168971cea9
      MULTUS_ADMISSION_CONTROLLER_IMAGE:  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5d7b9aa5360c759797d3f0c12885c414fdf8cfc64147e2dc9b5c88f1c4e8b56c
      CNI_PLUGINS_IMAGE:                  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
      WHEREABOUTS_CNI_IMAGE:              quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cba959055a03e7d3ac29790eb5aa4a68508fd4e5f2b3a95d83c28965ca35fac4
      ROUTE_OVERRRIDE_CNI_IMAGE:          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:137b1ce1a4c275eee519b77b7f58b4b93cbf839018e6bd2971405b37206320c0
      OVN_IMAGE:                          quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae9dbadf8b848df5ceb07a52cac6ac538a2231f7c2b6b696e71ccadf8b3db4af
      KURYR_DAEMON_IMAGE:                 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b060bc428e33c559128d7eef2ebe3396e9bf96d9b6f68af4f1c0c3d3ec3dcce5
      KURYR_CONTROLLER_IMAGE:             quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6490325fc796373258ed76af822b7e83d6eb83639961b77ff9dd67ae840f8122
      POD_NAME:                           network-operator-5b9f69556-wkp5p (v1:metadata.name)
    Mounts:
      /etc/kubernetes/kubeconfig from host-kubeconfig (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-545hq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  host-kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/kubeconfig
    HostPathType:  
  default-token-545hq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-545hq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:                 catalog-operator-868fd6ddb5-vmmgh
Namespace:            openshift-operator-lifecycle-manager
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:14 -0400
Labels:               app=catalog-operator
                      pod-template-hash=868fd6ddb5
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.46"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.46
IPs:
  IP:           10.241.0.46
Controlled By:  ReplicaSet/catalog-operator-868fd6ddb5
Containers:
  catalog-operator:
    Container ID:  cri-o://587583403eb6e3519414f70c94ff6d4220c056d5bbdfa37840d8fb605466a349
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Ports:         8080/TCP, 8081/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /bin/catalog
    Args:
      -namespace
      openshift-marketplace
      -configmapServerImage=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13ef9100ac654ab5a486452fd73806588fa90f320a540e16d204be97ef2e8a62
      -writeStatusName
      operator-lifecycle-manager-catalog
      -tls-cert
      /var/run/secrets/serving-cert/tls.crt
      -tls-key
      /var/run/secrets/serving-cert/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 12:38:13 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   80Mi
    Liveness:   http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      RELEASE_VERSION:  4.3.13
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from olm-operator-serviceaccount-token-zpzdq (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  catalog-operator-serving-cert
    Optional:    false
  olm-operator-serviceaccount-token-zpzdq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  olm-operator-serviceaccount-token-zpzdq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                  From                                Message
  ----     ------     ----                 ----                                -------
  Warning  Unhealthy  137m (x2 over 137m)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.46:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x3 over 13h)   kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.46:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)


Name:                 olm-operator-76f5488559-82ff6
Namespace:            openshift-operator-lifecycle-manager
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:13 -0400
Labels:               app=olm-operator
                      pod-template-hash=76f5488559
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.41"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.41
IPs:
  IP:           10.241.0.41
Controlled By:  ReplicaSet/olm-operator-76f5488559
Containers:
  olm-operator:
    Container ID:  cri-o://23c40a5e3ac5bc3dc28adb7bef7c98f8e6e2df51d883fc47faee76667cceb810
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Ports:         8080/TCP, 8081/TCP
    Host Ports:    0/TCP, 0/TCP
    Command:
      /bin/olm
    Args:
      -namespace
      $(OPERATOR_NAMESPACE)
      -writeStatusName
      operator-lifecycle-manager
      -writePackageServerStatusName
      operator-lifecycle-manager-packageserver
      -tls-cert
      /var/run/secrets/serving-cert/tls.crt
      -tls-key
      /var/run/secrets/serving-cert/tls.key
    State:          Running
      Started:      Wed, 13 May 2020 17:52:27 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:38:13 -0400
      Finished:     Wed, 13 May 2020 17:52:26 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      10m
      memory:   160Mi
    Liveness:   http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:8080/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      RELEASE_VERSION:     4.3.13
      OPERATOR_NAMESPACE:  openshift-operator-lifecycle-manager (v1:metadata.namespace)
      OPERATOR_NAME:       olm-operator
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from olm-operator-serviceaccount-token-zpzdq (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  olm-operator-serving-cert
    Optional:    false
  olm-operator-serviceaccount-token-zpzdq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  olm-operator-serviceaccount-token-zpzdq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x3 over 13h)  kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.41:8080/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m                kubelet, dc1master01.os4.ringen.us  Readiness probe failed: Get http://10.241.0.41:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get http://10.241.0.41:8080/healthz: net/http: request canceled (Client.Timeout exceeded while awaiting headers)


Name:                 packageserver-5658d5bb89-rsb64
Namespace:            openshift-operator-lifecycle-manager
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:12:00 -0400
Labels:               app=packageserver
                      pod-template-hash=5658d5bb89
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.109"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      olm.operatorGroup: olm-operators
                      olm.operatorNamespace: openshift-operator-lifecycle-manager
                      olm.targetNamespaces: openshift-operator-lifecycle-manager
                      olmcahash: d7aa81242109970f48a2734a7cf649d49b0e9fc13b9b5d98481682d76f4e2911
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.109
IPs:
  IP:           10.241.0.109
Controlled By:  ReplicaSet/packageserver-5658d5bb89
Containers:
  packageserver:
    Container ID:  cri-o://a22f76c7d35e3e54a01a25b7a92c30f530c156a5254610ae9aabd470d41e8840
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Port:          5443/TCP
    Host Port:     0/TCP
    Command:
      /bin/package-server
      -v=4
      --secure-port
      5443
      --global-namespace
      openshift-marketplace
    State:          Running
      Started:      Thu, 14 May 2020 07:12:03 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get https://:5443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:5443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /apiserver.local.config/certificates from apiservice-cert (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from olm-operator-serviceaccount-token-zpzdq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  apiservice-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v1.packages.operators.coreos.com-cert
    Optional:    false
  olm-operator-serviceaccount-token-zpzdq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  olm-operator-serviceaccount-token-zpzdq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type     Reason     Age        From                                Message
  ----     ------     ----       ----                                -------
  Normal   Scheduled  <unknown>  default-scheduler                   Successfully assigned openshift-operator-lifecycle-manager/packageserver-5658d5bb89-rsb64 to dc1master01.os4.ringen.us
  Normal   Pulled     18m        kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4" already present on machine
  Normal   Created    18m        kubelet, dc1master01.os4.ringen.us  Created container packageserver
  Normal   Started    18m        kubelet, dc1master01.os4.ringen.us  Started container packageserver
  Warning  Unhealthy  18m        kubelet, dc1master01.os4.ringen.us  Liveness probe failed: Get https://10.241.0.109:5443/healthz: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)


Name:                 packageserver-5658d5bb89-tq86g
Namespace:            openshift-operator-lifecycle-manager
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Thu, 14 May 2020 07:11:47 -0400
Labels:               app=packageserver
                      pod-template-hash=5658d5bb89
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.108"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      olm.operatorGroup: olm-operators
                      olm.operatorNamespace: openshift-operator-lifecycle-manager
                      olm.targetNamespaces: openshift-operator-lifecycle-manager
                      olmcahash: d7aa81242109970f48a2734a7cf649d49b0e9fc13b9b5d98481682d76f4e2911
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.108
IPs:
  IP:           10.241.0.108
Controlled By:  ReplicaSet/packageserver-5658d5bb89
Containers:
  packageserver:
    Container ID:  cri-o://9de36fce68883f568a2671f46b120afba13e6dd49a2f2305de014cdb23c3ad3c
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4
    Port:          5443/TCP
    Host Port:     0/TCP
    Command:
      /bin/package-server
      -v=4
      --secure-port
      5443
      --global-namespace
      openshift-marketplace
    State:          Running
      Started:      Thu, 14 May 2020 07:11:51 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        10m
      memory:     50Mi
    Liveness:     http-get https://:5443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:5443/healthz delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /apiserver.local.config/certificates from apiservice-cert (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from olm-operator-serviceaccount-token-zpzdq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  apiservice-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  v1.packages.operators.coreos.com-cert
    Optional:    false
  olm-operator-serviceaccount-token-zpzdq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  olm-operator-serviceaccount-token-zpzdq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:
  Type    Reason     Age        From                                Message
  ----    ------     ----       ----                                -------
  Normal  Scheduled  <unknown>  default-scheduler                   Successfully assigned openshift-operator-lifecycle-manager/packageserver-5658d5bb89-tq86g to dc1master01.os4.ringen.us
  Normal  Pulled     18m        kubelet, dc1master01.os4.ringen.us  Container image "quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6d678859d3e67f523e1b1153fb6810c362ef85302c4a49fd08ecc0770d274fe4" already present on machine
  Normal  Created    18m        kubelet, dc1master01.os4.ringen.us  Created container packageserver
  Normal  Started    18m        kubelet, dc1master01.os4.ringen.us  Started container packageserver


Name:                 ovs-jzckw
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:00:49 -0400
Labels:               app=ovs
                      component=network
                      controller-revision-hash=c4755b6b8
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/ovs
Containers:
  openvswitch:
    Container ID:  cri-o://e7fe061aeacfdd21347ea506177a466cdd749cbf0906f6c0060a6dbe5ccb6e56
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      chown -R openvswitch:openvswitch /var/run/openvswitch
      chown -R openvswitch:openvswitch /etc/openvswitch
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); exit 0' TERM
      retries=0
      while true; do
        if /usr/share/openvswitch/scripts/ovs-ctl status &>/dev/null; then
          echo "warning: Another process is currently managing OVS, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently managing OVS, exiting" 2>&1
          exit 1
        fi
      done
      
      # launch OVS
      function quit {
          /usr/share/openvswitch/scripts/ovs-ctl stop
          exit 0
      }
      trap quit SIGTERM
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovs-vswitchd --system-id=random
      
      # Restrict the number of pthreads ovs-vswitchd creates to reduce the
      # amount of RSS it uses on hosts with many cores
      # https://bugzilla.redhat.com/show_bug.cgi?id=1571379
      # https://bugzilla.redhat.com/show_bug.cgi?id=1572797
      if [[ `nproc` -gt 12 ]]; then
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-revalidator-threads=4
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-handler-threads=10
      fi
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovsdb-server --system-id=random
      
      tail -F --pid=$(cat /var/run/openvswitch/ovs-vswitchd.pid) /var/log/openvswitch/ovs-vswitchd.log &
      tail -F --pid=$(cat /var/run/openvswitch/ovsdb-server.pid) /var/log/openvswitch/ovsdb-server.log &
      wait
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:09 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  400Mi
    Liveness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null &&
if /usr/bin/ovs-vsctl -t 5 br-exists br0; then /usr/bin/ovs-ofctl -t 5 -O OpenFlow13 probe br0; else true; fi
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/openvswitch from host-config-openvswitch (rw)
      /lib/modules from host-modules (ro)
      /run/openvswitch from host-run-ovs (rw)
      /sys from host-sys (ro)
      /var/run/openvswitch from host-run-ovs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /run/openvswitch
    HostPathType:  
  host-sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  host-config-openvswitch:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/openvswitch
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 ovs-vz5qz
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:27:46 -0400
Labels:               app=ovs
                      component=network
                      controller-revision-hash=c4755b6b8
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/ovs
Containers:
  openvswitch:
    Container ID:  cri-o://c224680f3f106634c211aae1fd408a0222ac63829ab8608011c6ad81b07b12ce
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      chown -R openvswitch:openvswitch /var/run/openvswitch
      chown -R openvswitch:openvswitch /etc/openvswitch
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); exit 0' TERM
      retries=0
      while true; do
        if /usr/share/openvswitch/scripts/ovs-ctl status &>/dev/null; then
          echo "warning: Another process is currently managing OVS, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently managing OVS, exiting" 2>&1
          exit 1
        fi
      done
      
      # launch OVS
      function quit {
          /usr/share/openvswitch/scripts/ovs-ctl stop
          exit 0
      }
      trap quit SIGTERM
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovs-vswitchd --system-id=random
      
      # Restrict the number of pthreads ovs-vswitchd creates to reduce the
      # amount of RSS it uses on hosts with many cores
      # https://bugzilla.redhat.com/show_bug.cgi?id=1571379
      # https://bugzilla.redhat.com/show_bug.cgi?id=1572797
      if [[ `nproc` -gt 12 ]]; then
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-revalidator-threads=4
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-handler-threads=10
      fi
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovsdb-server --system-id=random
      
      tail -F --pid=$(cat /var/run/openvswitch/ovs-vswitchd.pid) /var/log/openvswitch/ovs-vswitchd.log &
      tail -F --pid=$(cat /var/run/openvswitch/ovsdb-server.pid) /var/log/openvswitch/ovsdb-server.log &
      wait
      
    State:          Running
      Started:      Wed, 13 May 2020 12:27:55 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  400Mi
    Liveness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null &&
if /usr/bin/ovs-vsctl -t 5 br-exists br0; then /usr/bin/ovs-ofctl -t 5 -O OpenFlow13 probe br0; else true; fi
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/openvswitch from host-config-openvswitch (rw)
      /lib/modules from host-modules (ro)
      /run/openvswitch from host-run-ovs (rw)
      /sys from host-sys (ro)
      /var/run/openvswitch from host-run-ovs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /run/openvswitch
    HostPathType:  
  host-sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  host-config-openvswitch:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/openvswitch
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:
  Type     Reason     Age                 From                                Message
  ----     ------     ----                ----                                -------
  Warning  Unhealthy  137m (x2 over 13h)  kubelet, dc1master01.os4.ringen.us  Liveness probe failed: command timed out


Name:                 ovs-w89tq
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:00:39 -0400
Labels:               app=ovs
                      component=network
                      controller-revision-hash=c4755b6b8
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/ovs
Containers:
  openvswitch:
    Container ID:  cri-o://8dd8564ffc6eaa88d71621319c514aafa241b73ed9d715a5c375b73e2f5eb0cc
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      chown -R openvswitch:openvswitch /var/run/openvswitch
      chown -R openvswitch:openvswitch /etc/openvswitch
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); exit 0' TERM
      retries=0
      while true; do
        if /usr/share/openvswitch/scripts/ovs-ctl status &>/dev/null; then
          echo "warning: Another process is currently managing OVS, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently managing OVS, exiting" 2>&1
          exit 1
        fi
      done
      
      # launch OVS
      function quit {
          /usr/share/openvswitch/scripts/ovs-ctl stop
          exit 0
      }
      trap quit SIGTERM
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovs-vswitchd --system-id=random
      
      # Restrict the number of pthreads ovs-vswitchd creates to reduce the
      # amount of RSS it uses on hosts with many cores
      # https://bugzilla.redhat.com/show_bug.cgi?id=1571379
      # https://bugzilla.redhat.com/show_bug.cgi?id=1572797
      if [[ `nproc` -gt 12 ]]; then
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-revalidator-threads=4
          ovs-vsctl --no-wait set Open_vSwitch . other_config:n-handler-threads=10
      fi
      /usr/share/openvswitch/scripts/ovs-ctl start --ovs-user=openvswitch:openvswitch --no-ovsdb-server --system-id=random
      
      tail -F --pid=$(cat /var/run/openvswitch/ovs-vswitchd.pid) /var/log/openvswitch/ovs-vswitchd.log &
      tail -F --pid=$(cat /var/run/openvswitch/ovsdb-server.pid) /var/log/openvswitch/ovsdb-server.log &
      wait
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:01 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     200m
      memory:  400Mi
    Liveness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null &&
if /usr/bin/ovs-vsctl -t 5 br-exists br0; then /usr/bin/ovs-ofctl -t 5 -O OpenFlow13 probe br0; else true; fi
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Readiness:  exec [/bin/bash -c #!/bin/bash
/usr/share/openvswitch/scripts/ovs-ctl status > /dev/null &&
/usr/bin/ovs-appctl -T 5 ofproto/list > /dev/null &&
/usr/bin/ovs-vsctl -t 5 show > /dev/null
] delay=15s timeout=1s period=5s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/openvswitch from host-config-openvswitch (rw)
      /lib/modules from host-modules (ro)
      /run/openvswitch from host-run-ovs (rw)
      /sys from host-sys (ro)
      /var/run/openvswitch from host-run-ovs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /run/openvswitch
    HostPathType:  
  host-sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:  
  host-config-openvswitch:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/openvswitch
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 sdn-69fn2
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker01.os4.ringen.us/192.168.93.43
Start Time:           Wed, 13 May 2020 13:00:39 -0400
Labels:               app=sdn
                      component=network
                      controller-revision-hash=5bd55f5696
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.43
IPs:
  IP:           192.168.93.43
Controlled By:  DaemonSet/sdn
Init Containers:
  install-cni-plugins:
    Container ID:  cri-o://906e0d217d449358eb0e2d3d0b782461d3c9f0b40574341fa0bd3608dd738588
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -ex
      cp -f /usr/src/plugins/bin/{loopback,host-local} /host/opt/cni/bin
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:01 -0400
      Finished:     Wed, 13 May 2020 13:01:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/opt/cni/bin from host-cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Containers:
  sdn:
    Container ID:  cri-o://448521e51cb0795258204161cc57595fa435e08acbb3246eccdf9ac751efcd69
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          10256/TCP
    Host Port:     10256/TCP
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); rm -f /etc/cni/net.d/80-openshift-network.conf ; exit 0' TERM
      retries=0
      while true; do
        if echo 'test' | socat - UNIX-CONNECT:/var/run/openshift-sdn/cni-server.sock &>/dev/null; then
          echo "warning: Another process is currently listening on the CNI socket, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently listening on the CNI socket, exiting" 2>&1
          exit 1
        fi
      done
      
      # local environment overrides
      if [[ -f /etc/sysconfig/openshift-sdn ]]; then
        set -o allexport
        source /etc/sysconfig/openshift-sdn
        set +o allexport
      fi
      #BUG: cdc accidentally mounted /etc/sysconfig/openshift-sdn as DirectoryOrCreate; clean it up so we can ultimately mount /etc/sysconfig/openshift-sdn as FileOrCreate
      # Once this is released, then we can mount it properly
      if [[ -d /etc/sysconfig/openshift-sdn ]]; then
        rmdir /etc/sysconfig/openshift-sdn || true
      fi
      
      # Take over network functions on the node
      rm -f /etc/cni/net.d/80-openshift-network.conf
      cp -f /opt/cni/bin/openshift-sdn /host/opt/cni/bin/
      
      # Launch the network process
      exec /usr/bin/openshift-sdn-node --proxy-config=/config/kube-proxy-config.yaml --v=${DEBUG_LOGLEVEL:-2}
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:02 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   200Mi
    Readiness:  exec [test -f /etc/cni/net.d/80-openshift-network.conf] delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      OPENSHIFT_DNS_DOMAIN:     cluster.local
      K8S_NODE_NAME:             (v1:spec.nodeName)
    Mounts:
      /config from config (ro)
      /etc/cni/net.d from host-cni-conf (rw)
      /etc/sysconfig from etc-sysconfig (ro)
      /host from host-slash (ro)
      /host/opt/cni/bin from host-cni-bin (rw)
      /lib/modules from host-modules (ro)
      /var/lib/cni/networks/openshift-sdn from host-var-lib-cni-networks-openshift-sdn (rw)
      /var/run from host-var-run (rw)
      /var/run/dbus/ from host-var-run-dbus (ro)
      /var/run/kubernetes/ from host-var-run-kubernetes (ro)
      /var/run/openshift-sdn from host-var-run-openshift-sdn (rw)
      /var/run/openvswitch/ from host-var-run-ovs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      sdn-config
    Optional:  false
  etc-sysconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/sysconfig
    HostPathType:  
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-var-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run
    HostPathType:  
  host-var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  host-var-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openvswitch
    HostPathType:  
  host-var-run-kubernetes:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/kubernetes
    HostPathType:  
  host-var-run-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openshift-sdn
    HostPathType:  
  host-slash:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  host-cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  host-cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  host-var-lib-cni-networks-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks/openshift-sdn
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 sdn-9zkdt
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:27:46 -0400
Labels:               app=sdn
                      component=network
                      controller-revision-hash=5bd55f5696
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/sdn
Init Containers:
  install-cni-plugins:
    Container ID:  cri-o://62e1c5b7f407644f68edcd90edef326ebcb9cc86d9e38b6e17ff4f8478dafd6e
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -ex
      cp -f /usr/src/plugins/bin/{loopback,host-local} /host/opt/cni/bin
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 12:27:55 -0400
      Finished:     Wed, 13 May 2020 12:27:55 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/opt/cni/bin from host-cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Containers:
  sdn:
    Container ID:  cri-o://957dc29eedc3e8f9d265ebd5162a9ea827af00cc5cc9cffe0cf6392b1b10b0ce
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          10256/TCP
    Host Port:     10256/TCP
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); rm -f /etc/cni/net.d/80-openshift-network.conf ; exit 0' TERM
      retries=0
      while true; do
        if echo 'test' | socat - UNIX-CONNECT:/var/run/openshift-sdn/cni-server.sock &>/dev/null; then
          echo "warning: Another process is currently listening on the CNI socket, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently listening on the CNI socket, exiting" 2>&1
          exit 1
        fi
      done
      
      # local environment overrides
      if [[ -f /etc/sysconfig/openshift-sdn ]]; then
        set -o allexport
        source /etc/sysconfig/openshift-sdn
        set +o allexport
      fi
      #BUG: cdc accidentally mounted /etc/sysconfig/openshift-sdn as DirectoryOrCreate; clean it up so we can ultimately mount /etc/sysconfig/openshift-sdn as FileOrCreate
      # Once this is released, then we can mount it properly
      if [[ -d /etc/sysconfig/openshift-sdn ]]; then
        rmdir /etc/sysconfig/openshift-sdn || true
      fi
      
      # Take over network functions on the node
      rm -f /etc/cni/net.d/80-openshift-network.conf
      cp -f /opt/cni/bin/openshift-sdn /host/opt/cni/bin/
      
      # Launch the network process
      exec /usr/bin/openshift-sdn-node --proxy-config=/config/kube-proxy-config.yaml --v=${DEBUG_LOGLEVEL:-2}
      
    State:          Running
      Started:      Wed, 13 May 2020 12:27:56 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   200Mi
    Readiness:  exec [test -f /etc/cni/net.d/80-openshift-network.conf] delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      OPENSHIFT_DNS_DOMAIN:     cluster.local
      K8S_NODE_NAME:             (v1:spec.nodeName)
    Mounts:
      /config from config (ro)
      /etc/cni/net.d from host-cni-conf (rw)
      /etc/sysconfig from etc-sysconfig (ro)
      /host from host-slash (ro)
      /host/opt/cni/bin from host-cni-bin (rw)
      /lib/modules from host-modules (ro)
      /var/lib/cni/networks/openshift-sdn from host-var-lib-cni-networks-openshift-sdn (rw)
      /var/run from host-var-run (rw)
      /var/run/dbus/ from host-var-run-dbus (ro)
      /var/run/kubernetes/ from host-var-run-kubernetes (ro)
      /var/run/openshift-sdn from host-var-run-openshift-sdn (rw)
      /var/run/openvswitch/ from host-var-run-ovs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      sdn-config
    Optional:  false
  etc-sysconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/sysconfig
    HostPathType:  
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-var-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run
    HostPathType:  
  host-var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  host-var-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openvswitch
    HostPathType:  
  host-var-run-kubernetes:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/kubernetes
    HostPathType:  
  host-var-run-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openshift-sdn
    HostPathType:  
  host-slash:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  host-cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  host-cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  host-var-lib-cni-networks-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks/openshift-sdn
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 sdn-controller-b46xx
Namespace:            openshift-sdn
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:27:43 -0400
Labels:               app=sdn-controller
                      controller-revision-hash=5d9fb5d7d5
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.99.41
IPs:
  IP:           192.168.99.41
Controlled By:  DaemonSet/sdn-controller
Containers:
  sdn-controller:
    Container ID:  cri-o://f4462bd33d340be0009cac11e2cd664a1243d08df90e588692c5a1002ed9b95d
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          <none>
    Host Port:     <none>
    Command:
      openshift-sdn-controller
    State:       Running
      Started:   Wed, 13 May 2020 12:38:27 -0400
    Last State:  Terminated
      Reason:    Error
      Message:   k8s.io/client-go/informers/factory.go:134: Failed to list *v1.Node: Get https://api-int.os4.ringen.us:6443/api/v1/nodes?limit=500&resourceVersion=0: EOF
E0513 16:38:21.732419       1 reflector.go:123] github.com/openshift/client-go/network/informers/externalversions/factory.go:101: Failed to list *v1.HostSubnet: Get https://api-int.os4.ringen.us:6443/apis/network.openshift.io/v1/hostsubnets?limit=500&resourceVersion=0: EOF
E0513 16:38:21.732878       1 reflector.go:123] k8s.io/client-go/informers/factory.go:134: Failed to list *v1.Namespace: Get https://api-int.os4.ringen.us:6443/api/v1/namespaces?limit=500&resourceVersion=0: EOF
E0513 16:38:21.839654       1 reflector.go:123] github.com/openshift/client-go/network/informers/externalversions/factory.go:101: Failed to list *v1.NetNamespace: Get https://api-int.os4.ringen.us:6443/apis/network.openshift.io/v1/netnamespaces?limit=500&resourceVersion=0: EOF
E0513 16:38:21.961880       1 event.go:293] Could not construct reference to: '&v1.ConfigMap{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Data:map[string]string(nil), BinaryData:map[string][]uint8(nil)}' due to: 'no kind is registered for the type v1.ConfigMap in scheme "k8s.io/kubernetes/pkg/api/legacyscheme/scheme.go:30"'. Will not report event: 'Normal' 'LeaderElection' 'dc1master01.os4.ringen.us stopped leading'
I0513 16:38:21.961954       1 leaderelection.go:287] failed to renew lease openshift-sdn/openshift-network-controller: timed out waiting for the condition
F0513 16:38:21.961966       1 network_controller.go:96] leaderelection lost

      Exit Code:    255
      Started:      Wed, 13 May 2020 12:27:55 -0400
      Finished:     Wed, 13 May 2020 12:38:22 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-controller-token-j2cml (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  sdn-controller-token-j2cml:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-controller-token-j2cml
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:                 sdn-dnrtk
Namespace:            openshift-sdn
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ibmworker02.os4.ringen.us/192.168.93.53
Start Time:           Wed, 13 May 2020 13:00:49 -0400
Labels:               app=sdn
                      component=network
                      controller-revision-hash=5bd55f5696
                      openshift.io/component=network
                      pod-template-generation=1
                      type=infra
Annotations:          <none>
Status:               Running
IP:                   192.168.93.53
IPs:
  IP:           192.168.93.53
Controlled By:  DaemonSet/sdn
Init Containers:
  install-cni-plugins:
    Container ID:  cri-o://a49da508bd626277d907cb49d5a8d4f38965a4a24e845f98446a751db627c735
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74cb5e225940de9cb44e43d63645d8a25dc6cac71caf74dc002403f7c005dc1e
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -ex
      cp -f /usr/src/plugins/bin/{loopback,host-local} /host/opt/cni/bin
      
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 13 May 2020 13:01:09 -0400
      Finished:     Wed, 13 May 2020 13:01:10 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /host/opt/cni/bin from host-cni-bin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Containers:
  sdn:
    Container ID:  cri-o://724d44cd416f2015664e4f538f2264db61a0834130ea6a05c539a0e3dd4f621b
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93afc82709fc19846d2ba87db244537eb4ac84743372d053f9b1ebc042956735
    Port:          10256/TCP
    Host Port:     10256/TCP
    Command:
      /bin/bash
      -c
      #!/bin/bash
      set -euo pipefail
      
      # if another process is listening on the cni-server socket, wait until it exits
      trap 'kill $(jobs -p); rm -f /etc/cni/net.d/80-openshift-network.conf ; exit 0' TERM
      retries=0
      while true; do
        if echo 'test' | socat - UNIX-CONNECT:/var/run/openshift-sdn/cni-server.sock &>/dev/null; then
          echo "warning: Another process is currently listening on the CNI socket, waiting 15s ..." 2>&1
          sleep 15 & wait
          (( retries += 1 ))
        else
          break
        fi
        if [[ "${retries}" -gt 40 ]]; then
          echo "error: Another process is currently listening on the CNI socket, exiting" 2>&1
          exit 1
        fi
      done
      
      # local environment overrides
      if [[ -f /etc/sysconfig/openshift-sdn ]]; then
        set -o allexport
        source /etc/sysconfig/openshift-sdn
        set +o allexport
      fi
      #BUG: cdc accidentally mounted /etc/sysconfig/openshift-sdn as DirectoryOrCreate; clean it up so we can ultimately mount /etc/sysconfig/openshift-sdn as FileOrCreate
      # Once this is released, then we can mount it properly
      if [[ -d /etc/sysconfig/openshift-sdn ]]; then
        rmdir /etc/sysconfig/openshift-sdn || true
      fi
      
      # Take over network functions on the node
      rm -f /etc/cni/net.d/80-openshift-network.conf
      cp -f /opt/cni/bin/openshift-sdn /host/opt/cni/bin/
      
      # Launch the network process
      exec /usr/bin/openshift-sdn-node --proxy-config=/config/kube-proxy-config.yaml --v=${DEBUG_LOGLEVEL:-2}
      
    State:          Running
      Started:      Wed, 13 May 2020 13:01:11 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      100m
      memory:   200Mi
    Readiness:  exec [test -f /etc/cni/net.d/80-openshift-network.conf] delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:
      KUBERNETES_SERVICE_PORT:  6443
      KUBERNETES_SERVICE_HOST:  api-int.os4.ringen.us
      OPENSHIFT_DNS_DOMAIN:     cluster.local
      K8S_NODE_NAME:             (v1:spec.nodeName)
    Mounts:
      /config from config (ro)
      /etc/cni/net.d from host-cni-conf (rw)
      /etc/sysconfig from etc-sysconfig (ro)
      /host from host-slash (ro)
      /host/opt/cni/bin from host-cni-bin (rw)
      /lib/modules from host-modules (ro)
      /var/lib/cni/networks/openshift-sdn from host-var-lib-cni-networks-openshift-sdn (rw)
      /var/run from host-var-run (rw)
      /var/run/dbus/ from host-var-run-dbus (ro)
      /var/run/kubernetes/ from host-var-run-kubernetes (ro)
      /var/run/openshift-sdn from host-var-run-openshift-sdn (rw)
      /var/run/openvswitch/ from host-var-run-ovs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from sdn-token-4g878 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      sdn-config
    Optional:  false
  etc-sysconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/sysconfig
    HostPathType:  
  host-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  host-var-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run
    HostPathType:  
  host-var-run-dbus:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/dbus
    HostPathType:  
  host-var-run-ovs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openvswitch
    HostPathType:  
  host-var-run-kubernetes:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/kubernetes
    HostPathType:  
  host-var-run-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/openshift-sdn
    HostPathType:  
  host-slash:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:  
  host-cni-bin:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/bin
    HostPathType:  
  host-cni-conf:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/multus/cni/net.d
    HostPathType:  
  host-var-lib-cni-networks-openshift-sdn:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/cni/networks/openshift-sdn
    HostPathType:  
  sdn-token-4g878:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  sdn-token-4g878
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  kubernetes.io/os=linux
Tolerations:     
Events:          <none>


Name:                 service-ca-operator-84d789565c-kbp8t
Namespace:            openshift-service-ca-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:19 -0400
Labels:               app=service-ca-operator
                      pod-template-hash=84d789565c
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.3"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.3
IPs:
  IP:           10.241.0.3
Controlled By:  ReplicaSet/service-ca-operator-84d789565c
Containers:
  operator:
    Container ID:  cri-o://18d982a3b55fec920b97a347de62dee14e3861b0510db02a4463e168630dd957
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Port:          <none>
    Host Port:     <none>
    Command:
      service-ca-operator
      operator
    Args:
      --config=/var/run/configmaps/config/operator-config.yaml
      -v=4
    State:          Running
      Started:      Wed, 13 May 2020 12:38:28 -0400
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Wed, 13 May 2020 12:31:36 -0400
      Finished:     Wed, 13 May 2020 12:38:24 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:     10m
      memory:  80Mi
    Environment:
      CONTROLLER_IMAGE:        quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
      OPERATOR_IMAGE_VERSION:  4.3.13
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from service-ca-operator-token-x67sq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      service-ca-operator-config
    Optional:  false
  service-ca-operator-token-x67sq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  service-ca-operator-token-x67sq
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 apiservice-cabundle-injector-54ff756f6d-rv2cr
Namespace:            openshift-service-ca
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:45 -0400
Labels:               apiservice-cabundle-injector=true
                      app=apiservice-cabundle-injector
                      pod-template-hash=54ff756f6d
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.11"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.11
IPs:
  IP:           10.241.0.11
Controlled By:  ReplicaSet/apiservice-cabundle-injector-54ff756f6d
Containers:
  apiservice-cabundle-injector-controller:
    Container ID:  cri-o://69f8099960f8c35cfb899224dd687a2743eb830038d2fd0d2c606acbc9fa7f67
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      service-ca-operator
      apiservice-cabundle-injector
    Args:
      --config=/var/run/configmaps/config/controller-config.yaml
      -v=2
    State:          Running
      Started:      Wed, 13 May 2020 12:38:18 -0400
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Wed, 13 May 2020 12:31:48 -0400
      Finished:     Wed, 13 May 2020 12:38:16 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        10m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/configmaps/signing-cabundle from signing-cabundle (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from apiservice-cabundle-injector-sa-token-xq6l4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  signing-cabundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      signing-cabundle
    Optional:  false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      apiservice-cabundle-injector-config
    Optional:  false
  apiservice-cabundle-injector-sa-token-xq6l4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  apiservice-cabundle-injector-sa-token-xq6l4
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 configmap-cabundle-injector-55c6486c88-p5qw9
Namespace:            openshift-service-ca
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:46 -0400
Labels:               app=configmap-cabundle-injector
                      configmap-cabundle-injector=true
                      pod-template-hash=55c6486c88
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.12"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.12
IPs:
  IP:           10.241.0.12
Controlled By:  ReplicaSet/configmap-cabundle-injector-55c6486c88
Containers:
  configmap-cabundle-injector-controller:
    Container ID:  cri-o://078144b2f770ec251537255255959a6f42d4688a54aefd24ffe0b7f1db8633d2
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      service-ca-operator
      configmap-cabundle-injector
    Args:
      --config=/var/run/configmaps/config/controller-config.yaml
      -v=2
    State:          Running
      Started:      Wed, 13 May 2020 12:38:19 -0400
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Wed, 13 May 2020 12:31:48 -0400
      Finished:     Wed, 13 May 2020 12:38:17 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        10m
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/configmaps/signing-cabundle from signing-cabundle (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from configmap-cabundle-injector-sa-token-2glvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  signing-cabundle:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      signing-cabundle
    Optional:  false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      configmap-cabundle-injector-config
    Optional:  false
  configmap-cabundle-injector-sa-token-2glvx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  configmap-cabundle-injector-sa-token-2glvx
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 service-serving-cert-signer-675ffc9d54-9nm7h
Namespace:            openshift-service-ca
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:31:45 -0400
Labels:               app=service-serving-cert-signer
                      pod-template-hash=675ffc9d54
                      service-serving-cert-signer=true
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.10"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
Status:               Running
IP:                   10.241.0.10
IPs:
  IP:           10.241.0.10
Controlled By:  ReplicaSet/service-serving-cert-signer-675ffc9d54
Containers:
  service-serving-cert-signer-controller:
    Container ID:  cri-o://06d3e33c02b4b0ef849b25f1de2581125a77ba14048645fd1a72970ea07b1afe
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a9f44039de4fbabb9db03d1346fcb608331dc83f17a1d8d1eb9c0fef74d6436f
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      service-ca-operator
      serving-cert-signer
    Args:
      --config=/var/run/configmaps/config/controller-config.yaml
      -v=2
    State:          Running
      Started:      Wed, 13 May 2020 12:38:28 -0400
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Wed, 13 May 2020 12:31:47 -0400
      Finished:     Wed, 13 May 2020 12:38:24 -0400
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:        10m
      memory:     120Mi
    Environment:  <none>
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from service-serving-cert-signer-sa-token-b4rl7 (ro)
      /var/run/secrets/signing-key from signing-key (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  signing-key:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  signing-key
    Optional:    false
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      service-serving-cert-signer-config
    Optional:  false
  service-serving-cert-signer-sa-token-b4rl7:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  service-serving-cert-signer-sa-token-b4rl7
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 openshift-service-catalog-apiserver-operator-74c4446f5d-jf9bs
Namespace:            openshift-service-catalog-apiserver-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:14 -0400
Labels:               app=openshift-service-catalog-apiserver-operator
                      pod-template-hash=74c4446f5d
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.42"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.42
IPs:
  IP:           10.241.0.42
Controlled By:  ReplicaSet/openshift-service-catalog-apiserver-operator-74c4446f5d
Containers:
  operator:
    Container ID:  cri-o://333d3f43523e317a757be91be2163d28b3dde5aeaf88f2818dff5c07f7be35df
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:48862eed3f38e5af62eebc3e13ba14bbea0494b3f11ddedbefb38447794e09df
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:48862eed3f38e5af62eebc3e13ba14bbea0494b3f11ddedbefb38447794e09df
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-svcat-apiserver-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=5
    State:          Running
      Started:      Wed, 13 May 2020 12:38:15 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      memory:  50Mi
    Environment:
      IMAGE:                   quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cbd6bf10356e6a813f9ce1e51a0017746dfd96daf02772cf99c5db1ef9d5e84e
      OPERATOR_IMAGE_VERSION:  4.3.13
      OPERAND_IMAGE_VERSION:   4.3.13
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-service-catalog-apiserver-operator-token-zm7cl (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-service-catalog-apiserver-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      openshift-service-catalog-apiserver-operator-config
    Optional:  false
  openshift-service-catalog-apiserver-operator-token-zm7cl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-service-catalog-apiserver-operator-token-zm7cl
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>


Name:                 openshift-service-catalog-controller-manager-operator-dbf4bf86c
Namespace:            openshift-service-catalog-controller-manager-operator
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 dc1master01.os4.ringen.us/192.168.99.41
Start Time:           Wed, 13 May 2020 12:37:13 -0400
Labels:               app=openshift-service-catalog-controller-manager-operator
                      pod-template-hash=dbf46d9bc
Annotations:          k8s.v1.cni.cncf.io/networks-status:
                        [{
                            "name": "openshift-sdn",
                            "interface": "eth0",
                            "ips": [
                                "10.241.0.37"
                            ],
                            "dns": {},
                            "default-route": [
                                "10.241.0.1"
                            ]
                        }]
                      openshift.io/scc: anyuid
Status:               Running
IP:                   10.241.0.37
IPs:
  IP:           10.241.0.37
Controlled By:  ReplicaSet/openshift-service-catalog-controller-manager-operator-dbf46d9bc
Containers:
  operator:
    Container ID:  cri-o://d6f8d8d3a9ad8c28e253e895e0bcc190616ee396557c586e8f2a2384ec26c0e9
    Image:         quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6991eb20f8c56e4f41566a9a87229e4333d76a88b249ccf8f7adab1258b1776f
    Image ID:      quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6991eb20f8c56e4f41566a9a87229e4333d76a88b249ccf8f7adab1258b1776f
    Port:          8443/TCP
    Host Port:     0/TCP
    Command:
      cluster-svcat-controller-manager-operator
      operator
    Args:
      --config=/var/run/configmaps/config/config.yaml
      -v=4
    State:          Running
      Started:      Wed, 13 May 2020 12:38:11 -0400
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     10m
      memory:  50Mi
    Environment:
      RELEASE_VERSION:  4.3.13
      IMAGE:            quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cbd6bf10356e6a813f9ce1e51a0017746dfd96daf02772cf99c5db1ef9d5e84e
      POD_NAME:         openshift-service-catalog-controller-manager-operator-dbf4bf86c (v1:metadata.name)
    Mounts:
      /var/run/configmaps/config from config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from openshift-service-catalog-controller-manager-operator-tokesr4kc (ro)
      /var/run/secrets/serving-cert from serving-cert (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  serving-cert:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-service-catalog-controller-manager-operator-serving-cert
    Optional:    true
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      openshift-service-catalog-controller-manager-operator-config
    Optional:  false
  openshift-service-catalog-controller-manager-operator-tokesr4kc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  openshift-service-catalog-controller-manager-operator-tokesr4kc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 120s
                 node.kubernetes.io/unreachable:NoExecute for 120s
Events:          <none>
